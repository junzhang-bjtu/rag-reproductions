# Medical Imaging Technology

**Author(s):** Mark A. Haidekker

# Context

Mark A. HaidekkerSpringerBriefs in PhysicsMedical Imaging Technology201310.1007/978-1-4614-7073-1(C) The Author(s) 2013

SpringerBriefs in Physics

For further volumes: http://www.springer.com/series/8902

Mark A. Haidekker

Medical Imaging Technology

Mark A. Haidekker

College of Engineering, University of Georgia, Athens, GA, USA

ISSN 2191-5423e-ISSN 2191-5431

ISBN 978-1-4614-7072-4e-ISBN 978-1-4614-7073-1

Springer New York Heidelberg Dordrecht London

Library of Congress Control Number: 2013934273

(C) The Author(s) 2013

This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the material is concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction on microfilms or in any other physical way, and transmission or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed. Exempted from this legal reservation are brief excerpts in connection with reviews or scholarly analysis or material supplied specifically for the purpose of being entered and executed on a computer system, for exclusive use by the purchaser of the work. Duplication of this publication or parts thereof is permitted only under the provisions of the Copyright Law of the Publisher's location, in its current version, and permission for use must always be obtained from Springer. Permissions for use may be obtained through RightsLink at the Copyright Clearance Center. Violations are liable to prosecution under the respective Copyright Law.

The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication does not imply, even in the absence of a specific statement, that such names are exempt from the relevant protective laws and regulations and therefore free for general use.

While the advice and information in this book are believed to be true and accurate at the date of publication, neither the authors nor the editors nor the publisher can accept any legal responsibility for any errors or omissions that may be made. The publisher makes no warranty, express or implied, with respect to the material contained herein.

Printed on acid-free paper

Springer is part of Springer Science+Business Media (www.springer.com)

Preface

Among many major developments in the medical field over the past two centuries, I personally consider three of them of particularly outstanding importance: The discovery of antibiotics in the late nineteenth century, the discovery of anesthesia during a period from the late eighteenth century to the mid-1800s, and the discovery of methods to look inside the human body without surgery. Out of these three, biomedical imaging is the youngest development, and its beginnings can be pinpointed to a very precise point in history--the discovery of the X-ray by C. W. Rontgen in 1895.

The history of medical imaging is a fascinating topic in itself, and it is briefly covered in Chap. 1 . Most notably, the history of medical imaging is closely linked to the evolution of digital data processing and computer science, and to the evolution of digital electronics and the microprocessor. Medical imaging is truly interdisciplinary as it relies on physics, mathematics, biology, computer science, and engineering. This book tries to provide a solid foundation of the principles that lead to image formation. Specifically, many books on the same subject are intended for a medical or more general audience and treat the image formation process to some extent as a black box.

In this book, the image formation process can be followed from start to end, beginning with the question of how contrast is achieved. To this end, the source/detector systems that probe the tissue and provide the data necessary for image formation are explained. For each modality, this book explains the type of data collected, and how it is converted into an image. In addition, engineering aspects of the imaging devices, and a discussion of strengths and limitations of the modality can be found. In the first chapter, the basic concepts of contrast and resolution are introduced, and some concepts that are common to all modalities are explained. Subsequent chapters cover specific modalities.

We can broadly divide the imaging modalities into two groups, those with and those without the use of ionizing radiation. Unlike visible light, high-energy photons undergo only moderate scattering in tissue and can be used to penetrate the entire body. Most of the high-energy photons (X-ray and gamma radiation) follow a straight path, and certain geometrical assumptions are allowed that give rise to projection imaging ( Chap. 2 ) and X-ray-based tomography ( Chap. 3 ). Emission tomography, based on radioactive compounds that emit radiation from inside the body ( Chap. 4 ) also belong to this category, since gamma radiation is used for the actual image formation. Imaging modalities with ionizing radiation share many common principles, and Chaps. 2 through  partly build on each other.

Magnetic resonance imaging and ultrasound imaging both use fundamentally different physical phenomena that are covered in Chaps. 5 and  , respectively. Finally, Chap. 7 deals with recent developments both in the traditional modalities and in new modalities that are not yet widely used in clinical practice.

Since this book places considerable emphasis on the mathematical description of image formation, the reader should be familiar with calculus and have a basic understanding of differential equations. Although the Fourier transform is introduced in Sect. 1.4 , some familiarity with the Fourier transform is helpful. Prior understanding of digital signal processing is helpful, too, although not a prerequisite for the topics covered in this book.

The chapters in this book can serve as an entry point for the in-depth study of individual modalities by providing the essential basics of each modality in a comprehensive and easy-to-understand manner. As such, this book is equally suitable as a textbook for undergraduate or graduate biomedical imaging classes and as a reference and self-study guide that prepare the reader for more specialized in-depth studies.

However, any one imaging modality could fill a whole book on its own, and for advanced study of a specific modality, more specialized books are also available. In-depth coverage of all modalities with an emphasis on the clinical application (but less emphasis on the mathematics) is provided in the book by Bushberg et al. [1]. A practice-oriented, user-friendly, yet highly detailed view of computed tomography can be found in [2], although it does not provide the mathematical details of CT image reconstruction. For readers who are interested in image reconstruction algorithms and their mathematical foundation, the books by Kak and Slaney [3] or by Herman [4] are recommended. MRI is a comprehensive subject, and in-depth coverage is provided in the work by Haake et al. [5]. A reference for ultrasound imaging can be found in the book by Hedrick et al. [6].

Among many individuals and colleagues who helped to shape this book with comments and discussions, I would particularly like to thank Erin E. Roberts for the help I received with some figures, Richard Speir and Dr. Adnan Mustafic for detailed revisions of the manuscript and helpful ideas how to improve the text, and to Prof. Qun Zhao and Prof. Geoff Dougherty for reviewing the book. Furthermore, I would like to express my gratitude toward the team at Springer: Christopher Coughlin and HoYing Fan, as well as the production team at SPS who ensured a smooth path from concept to publication.

Mark A. Haidekker

Athens

January 2013

Contents

1 Introduction 1

1.1 A Brief Historical Overview 2

1.2 Image Resolution and Contrast 3

1.3 Systems and Signals: A Short Introduction 6

1.4 The Fourier Transform 9

2 X-Ray Projection Imaging 13

2.1 X-Ray Generation 13

2.1.1 The X-Ray Tube 15

2.1.2 A Focus on Geometry 19

2.2 X-Ray Attenuation 20

2.2.1 Photon-Matter Interaction 20

2.2.2 Macroscopic Attenuation and Lambert-Beer's Law 22

2.2.3 Lambert-Beer's Law in Inhomogeneous Materials 25

2.2.4 Dual-Energy X-Ray Absorptiometry 26

2.3 X-Ray Detectors 28

2.3.1 Film-Based Imaging 28

2.3.2 Fluoroscopes 30

2.3.3 Semiconductor Detectors 32

2.3.4 Photomultiplier Tubes 33

2.4 Factors that Determine X-Ray Image Quality 34

3 Computed Tomography 37

3.1 CT Image Formation Principles 37

3.1.1 The Radon Transform and the Fourier Slice Theorem 39

3.1.2 Practical Image Reconstruction 42

3.2 Engineering Aspects of CT Scanners 49

3.3 Quantitative CT 51

3.4 Image Quality and Artifacts 52

4 Nuclear Imaging 55

4.1 Radiopharmaceuticals 55

4.2 Production of Short-Lived Radioactive Tracers 56

4.3 Detector Systems and the Anger Camera 57

4.4 Single Photon Emission Computed Tomography 59

4.5 Positron Emission Tomography 64

4.6 Multi-Modality Imaging 66

5 Magnetic Resonance Imaging 67

5.1 Proton Spins in an External Magnetic Field 67

5.2 The Spin-Echo Experiment 70

5.3 The Spin-Echo Pulse Sequence 76

 5.3.1 Measurement of T 2  77

 5.3.2 Measurement of T 1 Through Incomplete Recovery  77

5.3.3 Measurement of Proton Density 78

 5.3.4 The Significance of T E and T R  78

5.4 From NMR to MRI: The Gradient Fields 79

5.4.1 The Slice Encode Gradient 81

5.4.2 Fourier-Encoding with the Gradient 84

5.4.3 The Frequency Encode Gradient 85

5.4.4 The Phase Encode Gradient 86

5.5 Putting Everything Together: Spatially-Resolved Spin-Echo Acquisition 87

5.6 Other Imaging Sequences 88

5.6.1 Gradient-Recalled Echo Sequences 89

5.6.2 Inversion Recovery Sequence 90

5.6.3 Echo Planar Imaging 92

5.7 Technical Realization 93

 5.7.1 B 0 Magnet  93

5.7.2 Gradient Subsystem 94

5.7.3 RF Subsystem 95

6 Ultrasound Imaging 97

6.1 Sound Propagation in Biological Tissue 97

6.2 Ultrasound Image Formation 101

6.2.1 Ultrasound Generation and Echo Detection 101

6.2.2 A-Mode Scans 103

6.2.3 B-Mode Scans 105

6.2.4 M-Mode Scans 107

6.2.5 Volumetric Scans and 3D Ultrasound 108

6.3 Doppler Ultrasound 108

7 Trends in Medical Imaging Technology 111

7.1 Progress in Established Imaging Modalities 112

7.1.1 X-ray and CT 112

7.1.2 Magnetic Resonance Imaging 113

7.1.3 Ultrasound Imaging 114

7.1.4 PET and Multi-Modality Imaging 114

7.1.5 Molecular Imaging 115

7.2 Optical Tomography 115

7.3 Advanced Image Processing 118

References121

Index127
Mark A HaidekkerSpringerBriefs in PhysicsMedical Imaging Technology201310.1007/978-1-4614-7073-1_1(C) The Author(s) 2013

# 1. Introduction

Mark A. Haidekker1

(1)

College of Engineering, University of Georgia, 597 D.W. Brooks Drive, Driftmier Engineering Center, Athens, 30602, USA

Mark A. Haidekker

Email: mhaidekk@uga.edu

Abstract

"Medical imaging refers to several different technologies that are used to view the human body in order to diagnose, monitor, or treat medical conditions". All imaging modalities have in common that the medical condition becomes visible by some form of contrast, meaning that the feature of interest (such as a tumor) can be recognized in the image and examined by a trained radiologist. The image can be seen as a model of the imaged tissue. Images in the context of this book are digital. This implies a finite resolution with the pixel as the smallest element. Furthermore, all imaging modalities lead to some degradation of the image when compared to the original object. Primarily, the degradation consists of blur (loss of detail) and noise (unwanted contrast). Some underlying principles are common to all imaging modalities, such as the interpretation as a system and its mathematical treatment. The image itself can be seen as a multidimensional signal. In many cases, the steps in image formation can be seen as linear systems, which allow simplified mathematical treatment.

> "Medical imaging refers to several different technologies that are used to view the human body in order to diagnose, monitor, or treat medical conditions. Each type of technology gives different information about the area of the body being studied or treated, related to possible disease, injury, or the effectiveness of medical treatment".

This concise definition by the US Food and Drug Administration illuminates the goal of medical imaging: To make a specific condition or disease visible. In this context, visible implies that the area of interest is distinguishable in some fashion (for example, by a different shade or color) from the surrounding tissue and, ideally, from healthy, normal tissue. The difference in shade or color can be generalized with the term contrast.

The process of gathering data to create a visible model (i.e., the image) is common to all medical imaging technologies and can be explained with the simple example of a visible-light camera. The sample is probed with incident light, and reflected light carries the desired information. For example, a melanoma of the skin would reflect less light than the surrounding healthy skin. The camera lens collects some of the reflected light and--most importantly--focuses the light onto the film or image sensor in such a way that a spatial relationship exists between the origin of the light ray and its location on the image sensor. The ability to spatially resolve a signal (in this example, light intensity) is fundamental to every imaging method. The ability to spatially resolve a signal can be fairly straightforward (for example, following an X-ray beam along a straight path) or fairly complex (for example in magnetic resonance imaging, where a radiofrequency signal is encoded spatially by its frequency and its phase).

In the next step of the process, the spatially resolved data are accumulated. Once again, the camera analogy is helpful. At the start of the exposure, the sensor array is reset. Over the duration of the exposure, incoming light creates a number of electrical charges that depends on the light intensity. At the end of the exposure, the charges are transferred from the sensor to a storage medium. From here, the image would typically be displayed in such a fashion that higher charge read-outs correspond to higher screen intensity. In the camera example, the relationship between reflected light intensity and displayed intensity is straightforward. In other cases, intensity relates to different physical properties. Examples include X-ray absorption (which gives X-ray images the characteristic negative appearance with bones appearing bright and air dark), concentration of a radioactively labeled compound, or the time it takes for a proton to regain its equilibrium orientation in a magnetic field.

The physical interpretation of image intensity is key to interpreting the image, and the underlying physical process is fundamental to achieving the desired contrast. As a consequence, the information encoded in the image varies fundamentally between image modalities and, in some cases (such as MRI), even within the same modality.

The image is evaluated by an experienced professional, usually a radiologist. Even in today's age of automated image analysis and computerized image understanding, the radiologist combines the information encoded in the image with knowledge of the patient's symptoms and history and with knowledge of anatomy and pathology to finally form a diagnosis. Traditional viewing of film over a light box is still prominent, even with purely digital imaging modalities, although more and more radiologists make use of on-the-fly capabilities of the digital imaging workstation to view and enhance images. Furthermore, computerized image processing can help enhance the image, for example, by noise reduction, emphasizing edges, improving contrast, or taking measurements.

## 1.1 A Brief Historical Overview

X-rays were discovered in 1895. Within less than a decade, which is an astonishingly short time, X-ray imaging became a main-stream diagnostic procedure and was adopted by most major hospitals in Europe and the USA. At that time, sensitivity was low, and exposure times for a single image were very long. The biological effects of X-rays were poorly explored, and radiation burns were common in the early years of diagnostic--and recreational--X-ray use. As the pernicious effects of ionizing radiation became better understood, efforts were made to shield operators from radiation and to reduce patient exposure. However, for half a century, X-ray imaging did not change in any fundamental fashion, and X-ray imaging remained the only way to provide images from inside the body.

The development of sonar (sound navigation and ranging) eventually led to the next major discovery in biomedical imaging: ultrasound imaging. After World War II, efforts were made, in part with surplus military equipment, to use sound wave transmission and sound echoes to probe organs inside the human body. Ultrasound imaging is unique in that image formation can take place with purely analog circuits. As such, ultrasound imaging was feasible with state-of-the-art electronics in the 1940s and 1950s (meaning: analog signal processing with vacuum tubes). Progress in medical imaging modalities accelerated dramatically with the advent of digital electronics and, most notably, digital computers for data processing. In fact, with the exception of film-based radiography, all modern modalities rely on computers for image formation. Even ultrasound imaging now involves digital filtering and computer-based image enhancement.

In 1972, Geoffrey Hounsfield introduced a revolutionary new device that was capable of providing cross-sectional, rather than planar, images with X-rays. He called the method tomography, from the Greek words to cut and to write [7]. The imaging modality is known as computed tomography (CT) or computer-aided tomography (CAT), and it was the first imaging modality that required the use of digital computers for image formation. CT technology aided the development of emission tomography, and the first CT scanner was soon followed by the first positron emission tomography scanner.

The next milestone, magnetic resonance imaging (MRI), was introduced in the late 1970s. MRI, too, relies on digital data processing, in part because it uses the Fourier transform to provide the cross-sectional image. Since then, progress became more incremental, with substantial advances in image quality and acquisition speed. The resolution and tissue discrimination of both CT and MRI, for example, that today's devices are capable of, was literally unthinkable at the time these devices were introduced. In parallel, digital image processing and the digital imaging workstation provided the radiologist with new tools to examine images and provide a diagnosis. Three-dimensional image display, multi-modality image matching, and preoperative surgery planning were made possible by computerized image processing and display.

A present trend exists toward the development of imaging modalities based on visible or infrared light. Optical coherence tomography (OCT) became widely known in the 1990s and has evolved into a mainstream method to provide cross-sectional scans of the retina and skin. Other evolving optical modalities, such as diffuse optical tomography, have not reached the maturity level that would allow its use in medical practice.

## 1.2 Image Resolution and Contrast

Digital images are discretely sampled on a rectangular grid. A digital camera again illustrates the nature of a digital image: the camera sensor is composed of millions of light-sensitive cells. A sketch of a few cells, strongly magnified, is shown in Fig. 1.1. Each single sensor cell is composed of a light-sensitive semiconductor element (photodiode) and its associated amplifier and drive circuitry. The cells are spaced   apart in the horizontal direction, and   in the vertical direction. The actual light-sensitive area is smaller,   by  . To illustrate these dimensions, let us assume a 12-megapixel sensor with 4,000 cells in the horizontal and 3,000 cells in the vertical direction. When the overall dimensions of the sensor chip are 24 by 18 mm, we know  m. Depending on the chip design, the photodiode occupies most of the space, for example,  m. Irrespective of the amount of detail in the image projected onto the sensor, detail information smaller than the size of a sensor cell is lost, because the photodiode averages the intensity over its surface, and the surrounding driver is not sensitive to light. Each cell (i.e., each pixel), therefore, provides one single intensity value that is representative of the area it occupies.

Fig. 1.1

Sketch of a magnified part of a digital camera image sensor. Each sensor cell consists of a light-sensitive photodiode (gray-shaded area) and associated amplifier and driver circuitry (hatched region). Each sensor cell averages the light across its sensitive surface and provides one single intensity value

The spatial resolution of the most important medical imaging modalities spans a wide range. Planar X-ray imaging can achieve a spatial resolution of up to 10  m, in part limited by the film grain. Digital X-ray sensors can achieve a similarly high resolution, although 20-50  m pixel size is more common. With CT, in-plane pixel sizes between 0.1 and 0.5 mm are common in whole-body scanners. MRI scanners have typical in-plane pixels of 0.5-1 mm. Due to the different detector system, radionuclide imaging modalities (SPECT and PET) have pixel sizes in the centimeter range. Ultrasound resolution lies between CT and MRI.

The sensor is not the only limiting factor for the spatial resolution. An ideally focused light source is spread out by the camera lens, primarily as a consequence of lens shape approximations and light diffraction. The image of a point source is called the point-spread function. The importance of the point-spread function is demonstrated in Fig. 1.2. The image shows photos of tightly focused laser beams taken with a digital SLR camera from 2 m distance. It can be seen that the image of a single beam shows a Gaussian profile (Fig. 1.2a). An ideal imaging apparatus would provide a delta function (i.e., a cylinder of one pixel width). The point-spread function can be quantified by its full width at half-maximum (FWHM), that is, the width of the point image where it drops to one half of its peak value (Fig. 1.2a). In this example, we observe a FWHM of 6 pixels. As long as two closely spaced point sources are further apart than the FWHM, they can be distinguished as two separate peaks (Fig. 1.2b), which is no longer possible when the point sources are closer than the FWHM (Fig. 1.2c).

Clearly, the point-spread function poses a limit on the spatial resolution, often more so than the detector size. In X-ray imaging, for example, one factor that determines the point-spread function is the active area of the X-ray tube. In ultrasound imaging, factors are the length of the initial ultrasound pulse and the diameter of the ultrasound beam. Furthermore, the wavelength of the sound wave itself is a limiting factor.

Fig. 1.2

Point-spread function of a digital camera, shown in grayscale representation and as an elevation map where intensity translates into height. The pixel size   and   is indicated. a The image of a highly focused laser beam has a Gaussian shape in the image. The full width at half-maximum (FWHM) spread is 6 pixels. b Two closely spaced sources can be distinguished if their distance is larger than the FWHM. c Two sources that are more closely spaced than the FWHM become indistinguishable from a single source

The image values are stored digitally. A certain number of bits is set aside for each cell (each pixel). Since each bit can hold two values (one and zero), a  -bit pixel can hold   discrete intensity levels. Color photos are commonly stored with 24 bits per pixel, with 8 bits each for the three fundamental colors, red, green, and blue. For each color, 256 intensity levels are possible. Most magnetic resonance and ultrasound images are also stored with 8 bits depth, whereas computed tomography normally provides 12 bits.

The pixel size determines the absolute limit for the spatial resolution, and the bit depth determines the contrast limit. Consider an 8-bit image: the intensity increase from one discrete image value to the next is 0.39 % of the maximum value. Any smaller intensity variations cannot be represented. The error that is associated with rounding of a continuous signal to the next possible image value is referred to as digitization noise .

Noise is introduced in several steps of the acquisition and processing chain. Both the sensors and the amplifiers introduce noise components, particularly when weak signals need to be amplified by a large gain factor. Examples are the RF echo signal in MRI and the ultrasound echo in ultrasound imaging. To some extent noise can be suppressed with suitable filters, but the side-effect is a broadening of the point-spread function and the associated loss of detail. Conversely, any filter that tries to counteract the point-spread function increases the noise component. The noise component is critical for the overall image quality, because noise can "bury" detail information from small objects or objects with low contrast.

The ability to provide a specific, desired contrast depends strongly on the modality. X-ray imaging, for example, provides very strong contrast between bone and soft tissue, and between soft tissue and air (e.g., in images of the lung or the chest). Magnetic resonance imaging shows high contrast between different types of soft tissue (e.g., gray and white matter of the brain), but bone and air are dark due to the absence of water. Ultrasound generally provides good tissue contrast, but suffers from a high noise component, visible as characteristic ultrasound speckles.

## 1.3 Systems and Signals: A Short Introduction

System is a broad term that encompasses any assembly of interconnected and interacting components that have measurable behavior and a defined response to a defined manipulation of its parts. Any device that provides a medical image is a system in this definition, and it consists in turn of several components that can be seen as systems themselves. Systems have inputs and outputs. One example for a system is an X-ray detector. The number of X-ray photons hitting the conversion layer can be interpreted as the system input. The detector provides a voltage that is proportional to the incident photon flux, and this voltage is the output. Similarly, a computer algorithm for image reconstruction is a system. In a computed tomography scanner, for example, the input to the image reconstruction algorithm is the X-ray intensity as a function of the scan angle and position, and the output is a two-dimensional map (i.e., cross-section) of X-ray absorption coefficients.

The input and output to a system can be interpreted as signals. Often, a signal is understood as a function of time, but in imaging devices, signals are functions of a spatial coordinate. In the most general form, imaging devices process signals of the form  , that is, a quantity that depends on a location in (three-dimensional) space and on time. Often, simplifications can be made when a signal is approximately constant over the image acquisition time, or when a signal is obtained only within one plane. An example is shown in Fig. 1.3, where components of the scanning and image reconstruction process are shown as blocks with signals represented by arrows.

Fig. 1.3

Systems interpretation of a computed tomography scanner. Components of the system (in itself systems) are represented by blocks, and signals represented by arrows. The original object has some property  , for example, X-ray absorption that varies within the  -plane. The detector collects X-ray intensity   as a function of scan direction   and scan angle   in the  -plane, and provides a proportional voltage  . In the image formation stage, these data are transformed into a cross-sectional map of apparent X-ray opaqueness,  . Finally, the display outputs a light intensity   that is proportional to   and approximates

In any system, the output signal can be described mathematically for a given input signal. The X-ray detector, for example, converts X-ray photon flux   into a proportional voltage  :

(1.1)

where   is the gain of the X-ray detector. Similarly, the image reconstruction stage approximates the inverse Radon transform   (see Sect. 3.1.1):

(1.2)

A special group of systems are linear, time-invariant systems. These systems are characterized by three properties,

  * Linearity: If   is the output for a given input  , then a change of the magnitude of the input signal by a constant factor   (i.e., we input  ) leads to a proportional output signal  .

  * Superposition: If a system responds to an input signal   with the output signal   and to a different input signal   with  , then the sum of the input signals   will elicit the response  .

  * Time-invariance: If   is the time-dependent output signal for a given input signal  , then the application of the delayed signal   causes an identical, but equally delayed response  . In images, time-invariance translates into shift-invariance. This means that an operator that produces an image   from an input image produces the same image, but shifted by  , when the input image is shifted by the same distance.

Figure 1.3 provides a different view of the point-spread function: we can see that an object (the original tissue property  ) is probed by some physical means. The image formation process leads to the display of an image  , which differs from  . Referring back to Fig. 1.2, we can see that the image functions of the laser dots are superimposed (i.e., added together). With the superposition principle, we can examine each individual pixel separately and subject it to the point-spread function, then add the results. Very often, the point-spread function has Gaussian character, and we can model the peak seen in Fig. 1.2a as

(1.3)

where   is the Euclidean distance from the center pixel of the point source  . If we know the signal of the idealized point source  , we can now predict the measured (i.e., blurred with the PSF) intensity for each pixel  :

(1.4)

where   should be seen as a generalized point-spread function whose center is shifted to the center of the point source. Consequently, we can express the image formed by two point sources of strength   and   and centered on   and  , respectively, as the superposition of the image functions

(1.5)

Fig. 1.4

Illustration of the effects of a Gaussian point-spread function. a Idealized image (note the two added white dots in the top left and top right corners indicated by arrows). b Image obtained after a process with a Gaussian point-spread function. c Intensity profiles along the horizontal dashed line in a. It can be seen that sharp transitions are softened, because higher image values also influence their neighbors. Point sources assume a Gaussian-shaped profile

This concept can be further generalized. Assume that we have an idealized (but inaccessible) source image   and we measure the image   with an imaging device that makes it subject to the point-spread function  . In this case, we can subject each individual pixel of   to the point-spread function and recombine them by addition:

(1.6)

The sum in Eq. 1.6 needs to be evaluated for all pixels   of the target image  . Equation 1.6 describes the two-dimensional discrete convolution of the source image   with a convolution function (often called convolution kernel)  . Since any bright pixel spreads out and influences its neighbors (thus, point spread function), sharp transitions are softened, and detail is lost. The effect is demonstrated in Fig. 1.4, where the idealized image   (Fig. 1.4a) has been subjected to a simulated point-spread function, in this case, a Gaussian function with   pixels to reveal the actual image   (Fig. 1.4b). The line profiles (Fig. 1.4c) help illustrate how sharp transitions are blurred and how isolated points assume a Gaussian shape.

## 1.4 The Fourier Transform

The Fourier transform is one of the most important linear operations in image processing, and it is fundamental to most imaging modalities. Intuitively, a transform reveals a different aspect of the data. In the case of the Fourier transform, it shows the distribution of harmonic content--how the signal is composed of periodic oscillations of different frequency and amplitude. For example, a time-dependent oscillation   could be described by its amplitude   and its frequency  . In a diagram   over  , we obtain an oscillation. In a diagram of amplitude over frequency, the same signal is defined by a single point at  . Superimposed sine waves would be represented by multiple points in the diagram of amplitude over frequency. In this simplified explanation, a phase shift   cannot be considered, because a third dimension becomes necessary to include  ,  , and  . The Fourier transform uses sine and cosine functions to include the phase shift, and each harmonic oscillation becomes  .

Fourier's theorem states that any periodic signal   can be represented as an infinite sum of harmonic oscillations, and the Fourier synthesis of the signal   can be written as

(1.7)

where   and   are the Fourier coefficients that determine the contribution of the  th harmonic to the signal  . For any given signal  , the Fourier coefficients can be obtained by Fourier analysis,

(1.8)

Equation 1.7 describes the synthesis of a signal from harmonics with integer multiples of its fundamental frequency. The spectrum (i.e., the sequence of   and  ) is discrete. The continuous Fourier transform is better derived from a different form of Fourier synthesis that uses a continuous spectrum   and  :

(1.9)

The integration takes place over all possible frequencies  . Since the basis functions (sin and cos) in Eq. 1.9 are orthogonal, we can express the Fourier synthesis in terms of a complex harmonic oscillation  . Fourier synthesis restores a signal from its spectrum and corresponds to the inverse Fourier transform  , whereas the Fourier analysis, which provides the spectrum of a signal, is referred to as the actual Fourier transform  :

(1.10)

Equation 1.10 defines the Fourier transform in terms of the angular frequency  . In some cases, it is more convenient to express the spectrum   as a function of the linear frequency  , for which the Fourier transform becomes

(1.11)

To explain the significance of the Fourier transform, let us consider two examples. First, in magnetic resonance imaging, we deal with signals that are caused by protons spinning at different speeds (cf. Sect. 5.4.3). The angular frequency of the protons increases along one spatial axis (let us call it the  -axis), and the protons emit a signal whose strength is determined, among other factors, by the number of protons at any point along the  -axis. The signal can be collected by an antenna, but the antenna only provides the additive mix of all signals. We can, however, obtain the local proton density by using the relationship  , where   is the rate of change of the frequency along the  -axis. The antenna provides a signal  , which we subject to the Fourier transform. The resulting harmonic content   is directly related to the signal strength at any point along the  -axis and therefore to the proton density.

Second, it is sometimes desirable to have a signal that contains all frequencies in a limited range (i.e., a broadband signal). We can ask the question, how would a broadband signal   look like for which the spectral component is unity for all frequencies between   and  ?1 To answer this question, we use the inverse Fourier transform (Eq. 1.11) with the description of the broadband signal,

(1.12)

which leads to the following integral where the limits of the bandwidth determine the integration bounds:

(1.13)

Fortunately, Euler's relationship allows us to simplify the expression in square brackets to  , and the imaginary unit   cancels out. We therefore obtain our broadband signal as

(1.14)

For  , Eq. 1.14 describes the well-known sinc function, and it can be shown that the boxcar function (Eq. 1.12) and the sinc-function are a Fourier transform pair, meaning, a square pulse in the time domain has a sinc-like spectrum, and a sinc-like function has a boxcar-type spectrum.

Since digital signals and digital images are discretely sampled, we need to take a look at the discrete Fourier transform. In the one-dimensional case, the signal   exists as a set of   discretely sampled values  , obtained at  . Here,   is the sampling period. In the discrete world, the integral corresponds to a summation, and the discrete Fourier transform becomes

(1.15)

where   is the discrete frequency variable, and the sum needs to be evaluated for  . Equation1.15 does not consider the sampling rate, and   needs to be known to relate   to any real-world units. Any spectral component   has the corresponding frequency  ,

(1.16)

Note that Eq. 1.16 is not limited to sampling in time. When   is a time interval,   has units of frequency (i.e., inverse seconds). However, a signal can be sampled with discrete detectors along a spatial axis (see, for example, Fig. 1.1). In this case, the sampling interval has units of distance, and   has units of inverse distance. This is referred to as spatial frequency. An example to illustrate spatial frequency is a diffraction grating, which causes interference patterns with a certain spatial distance. For example, if an interference maximum occurs every 0.2 mm, the corresponding spatial frequency is 5 mm  (or 5 maxima per mm).

We can see from Eq. 1.15 that choosing   yields the same result as  . For increasing  , therefore, the spectrum repeats itself. Even more, the symmetry of the complex exponential in Eq. 1.15 provides us with  , where   indicates the conjugate-complex of  . For this reason, we gain no new information from computing the discrete Fourier transform for  . By looking at Eq. 1.16, we can see that the frequency at   is exactly one half of the sampling frequency. This is the maximum frequency that can be unambiguously reconstructed in a discretely-sampled signal (known as the Shannon sampling theorem). The frequency   is known as the Nyquist frequency. In the context of Fig. 1.1, we briefly touched on the loss of detail smaller than the sensor area. Here, we have approached the same phenomenon from the mathematical perspective. The situation becomes worse if the signal actually contains frequency components higher than the Nyquist frequency, because those spectral components are reflected into the frequency band below  , a phenomenon known asaliasing. Aliasing is not limited to signals sampled in time. Spatial discretization of components with a higher spatial frequency than   leads to Moire patterns .

For images (i.e., discretely-sampled functions in two dimensions), the Fourier transform can be extended into two dimensions as well. Because of the linearity of the Fourier transform, we can perform the row-by-row Fourier transform in one dimension and subject the result to a column-by-column Fourier transform in the orthogonal dimension:

(1.17)

The Fourier transform now has two orthogonal frequency axes,   and  . The inverse Fourier transform is

(1.18)

The two-dimensional inverse Fourier transform finds its application in the reconstruction process in computed tomography and magnetic resonance. In both cases, one-dimensional Fourier-encoded information is gathered and used to fill a 2D Fourier-domain spaceholder. Once the spaceholder is completely filled, the inverse Fourier transform yields the cross-sectional reconstructed image.

Footnotes

1

Examining negative frequencies is not unreasonable. Equation 1.9 holds for  , and the Fourier transform shows some symmetry. The Fourier transform has a number of very interesting properties, but they go beyond the scope of this book.
Mark A HaidekkerSpringerBriefs in PhysicsMedical Imaging Technology201310.1007/978-1-4614-7073-1_2(C) The Author(s) 2013

# 2. X-Ray Projection Imaging

Mark A. Haidekker1

(1)

College of Engineering, University of Georgia, 597 D.W. Brooks Drive, Driftmier Engineering Center, Athens, 30602, USA

Mark A. Haidekker

Email: mhaidekk@uga.edu

Abstract

X-ray imaging is the oldest medical imaging modality, which found its way into medical practice shortly after the discovery of the X-rays in 1895. X-ray imaging is a projection technique, and image formation takes place traditionally on photosensitive film, although direct digital X-ray imaging is becoming more and more common. In its most common form, X-ray imaging is a qualitative modality. X-rays are high-energy photons, and atomic interaction with inner shell electrons is fundamental to both X-ray production and generation of X-ray contrast. Soft-tissue contrast is comparatively low, but bone and air provide excellent contrast. In some cases, contrast can be enhanced with contrast agents. An undesirable (but unavoidable) side-effect of the photon-atom interaction is the ionization of tissue along the beam path, which can lead to radiation damage. X-ray images can reveal very subtle features, and its popularity is further enhanced by the relatively inexpensive equipment and the straightforward imaging procedure.

## 2.1 X-Ray Generation

X-rays are generated when kinetic electrons interact with a solid target and undergo sudden deceleration ("braking radiation" or "bremsstrahlung"). As electrons lose some of their kinetic energy, this energy is released as an X-ray photon. In an X-ray tube, electrons are emitted into the vacuum at the cathode and accelerated toward the anode in a high-voltage electrostatic field. In the dense metallic anode material, the electrons lose part of their energy when they are deflected in the Coulombic field of the nucleus (Fig. 2.1). The extreme case is a direct collision of a high-energy electron with a nucleus, whereby the electron loses all of its kinetic energy, and a photon of the same energy is emitted. If the electron comes in close proximity of the nucleus, it is deflected by electrostatic forces. In the process, the electron loses some (but not all) of its energy, and a photon is emitted that carries this amount of energy. The greater the distance to the nucleus, the lower the energy loss of the electron and the lower the photon energy. Electrons can undergo multiple interactions with nuclei before they expend all of their energy. Since the kinetic energy   of the electrons only depends on the electrostatic potential (i.e., the anode voltage  ), the maximum photon energy   and its corresponding shortest wavelength   can be computed as

(2.1)

(2.2)

where   is the electron mass,   its charge,   its velocity upon impact with the anode,   is Planck's constant, and   the speed of light.

The X-ray radiation generated in a regular X-ray tube has therefore a continuous energy spectrum. A direct collision with the nucleus is rare, and photons with the highest possible energy occur in the spectrum with a low probability. The probability increases with increasing electron-nucleus distance and correspondingly lower photon energy.

Interactions of the high-energy electrons with shell electrons of the anode material play a significant role. If high-energy electrons collide with shell electrons, the atom of the anode material can be excited or ionized, usually by ejection of a k-shell electron. As the k-shell vacancy gets filled, energy is released--again as X-ray radiation. Since the binding energies of the target material are fixed, the X-ray energy that is emitted from shell electrons has a very narrow energy spectrum (Fig. 2.2). These narrow spikes in the energy spectrum are referred to as characteristic X-rays, because they are characteristic for the target material.

Fig. 2.1

X-ray generation by electron deceleration. High-energy electrons come in proximity of a nucleus of the anode material and are deflected in the Coulombic field of the nucleus. In the extreme case (1), the electron collides directly with the nucleus, thereby releasing all of its kinetic energy into one X-ray photon. A close "fly-by" (2) is much more probable, but the electron loses less energy, and the resulting X-ray photon has a lower energy and a correspondingly longer wavelength

Fig. 2.2

Typical X-ray spectrum of a tube with a tungsten anode operated at 100 kV. The maximum possible photon energy is 100 keV, but lower energies are more probable. Very low energies (25 keV and below) get absorbed by the anode material and by the glass wall of the tube, and do not occur in the spectrum. Two prominent peaks of characteristic X-rays can be seen at 69.5 keV (k-shell) and 57.4 keV (k l transition)

Example: In a tube with a tungsten anode, the anode voltage is 75 kV. The highest possible energy is 75 keV (1 eV   1.602   10  J). The shortest wavelength   pm (pm   picometers   10  m). In addition, the binding energy of the k-shell electrons of tungsten is 69.5 keV, and the binding energy of the l-shell electrons is 12.1 keV. If an electron is ejected from the k-shell, and the vacancy is filled from the l-shell, the energy difference of 57.4 keV is emitted as an X-ray photon. Note that the binding energy of higher shells is too low to cause X-ray radiation. Note also that the characteristic X-ray radiation can only occur if the kinetic energy of the incident electrons is high enough to allow at least the k l transition. For example, if the tube is operated at 50 kV, no characteristic X-rays from tungsten can be seen.

For comparison, visible light (green) has a wavelength of 520 nm and an associated energy of 2.4 eV. The lowest energy to affect the atomic shell is the k l excitation of hydrogen and requires 10.1 eV with an associated wavelength of 123 nm. Diagnostic X-rays lie approximately in the range from 30 to 120 keV with wavelengths in the picometer range.

### 2.1.1 The X-Ray Tube

The actual X-ray tube consists of a vacuum glass cylinder with the anode and cathode inside. Electrical contacts allow connecting of the cathode, its heating filament, and the anode to the power sources. The anode is usually made of copper for good heat conduction with a small target area of tungsten or rhodium. Tungsten melts at about 3700 K, and copper has a melting point of about 1360 K. Heat dissipation is important, because only a small amount of the electron beam's kinetic energy is actually emitted as X-rays. X-ray tubes have a fairly poor degree of efficiency  , which can be approximated by

(2.3)

where   is a proportionality constant with approximately  ,   is the anode voltage, and   is the atomic number of the anode material, for example,   74 for tungsten. Typically, about 1 % of the energy is emitted as X-rays, and 99 % is lost as heat. Assuming a 70 kV tube with a tube current of 5 mA, total energy dissipation is 350 W, comparable to a hair dryer on a medium setting. For this reason, large X-ray tubes designed for continuous operation use rotating anodes: the anode is a large disc, driven by a motor. The disc is beveled, and electrons hit only a small active region on the beveled rim. The rotating anode shows much lower local heating than a fixed anode.

The cathode is heated by a filament, and electrons are injected into the vacuum by thermionic emission. Electrons from the cathode are accelerated toward the anode, gathering kinetic energy in the process. When the electrons hit the target material of the anode, they lose their kinetic energy, and X-rays are emitted.

A simplified sketch of an X-ray tube is shown in Fig. 2.3. The anode target is usually angled to direct a large part of the X-ray radiation perpendicular to the electron beam.

Fig. 2.3

Schematic representation of an X-ray tube. The cathode is heated by the filament, and electrons are ejected into the vacuum by thermionic emission. These electrons are accelerated by the electrostatic field between anode and cathode. As the electrons hit the target material of the anode, they emit X-ray radiation by deceleration. The anode is usually a massive body of metal to facilitate heat transport from the target to the outside of the tube

The filament current allows to control the cathode temperature. The thermionic effect is highly temperature dependent, and the current flux   (current per area unit of emitting cathode material) is described by the Richardson equation,

(2.4)

where   is the Richardson constant,   is the activation energy (also called work function), k is Boltzmann's constant, and   is the absolute temperature.   and   are material constants of the cathode material. To achieve a reasonable tube current, a cathode temperature between 1800 and 2000 K is desirable (Fig. 2.4). Special cathode materials, such as cesium on tungsten or barium oxide, have a lower activation energy and require lower temperatures for the desired tube current. The activation energy   and Richardson constant   for some common cathode materials is listed in Table 2.1.

Fig. 2.4

Temperature-dependency of the cathode current flux by thermionic emission (Eq. 2.4). Typical tube currents are in the milliampere range, and a tungsten cathode needs to be heated to near its melting point. Some special materials, such as cesium or barium oxide, have a lower activation energy and require lower temperatures

Table 2.1

Richardson constant   and activation energy   of some cathode materials

Material |   |

---|---|---  
|

(eV) | (A / cm  K )

Tungsten | 4.53 | 60

Molybdenum  | 4.43 | 55

Cs on W | 1.36 | 3.2

BaO  | 0.99 | 1.18

Fig. 2.5

Qualitative changes of the emission spectra (characteristic X-rays omitted!) of an X-ray tube when the anode voltage is increased (a) and when the filament current is increased (b). An increase of the anode voltage (in this example from 100 to 120 and 140 kV) increases the maximum photon energy, but indirectly also increases the tube current and therefore the total photon flux. Conversely, an increase of the filament current (in this example by 20 and 50 %) increases the total photon flux, but not their energy distribution. Notably, the maximum energy remains the same

The X-ray tube has two main "knobs" which influence the emitted radiation (see also Fig. 2.5):

1.

The anode voltage: Increasing the anode voltage increases the maximum energy of the X-ray photons, and it also increases the total number of photons emitted. An empirical relationship between the anode voltage   and the number of photons emitted   is

(2.5)

where k is a tube-dependent proportionality constant,   is the atomic number of the target material, and   is a material-dependent exponent, typically  .

2.

The tube current: By modifying the filament current and with it the cathode temperature, the tube current can be controlled. Increasing the tube current by increasing the filament temperature increases X-ray photon flux, but does not change the highest energy of the emitted photons.

In practice, the X-ray tube is housed in a lead-shielded assembly to prevent X-ray radiation in any direction except the aperture of the X-ray housing. Often, collimators (movable lead blades) allow adjustment of the aperture. With a suitably positioned lamp and mirror, the area that will be exposed to X-rays can be highlighted beforehand with visible light. Lastly, a beam-hardening filter (sometimes removable) absorbs some of the low-energy radiation and reduces the patient dose. Figure 2.6 shows a sketch of a complete X-ray tube assembly with its housing, beam-hardening filter, collimators, and guide lamp.

Fig. 2.6

Schematic representation of an X-ray tube assembly with its enclosure and additional features. A beam-hardening filter removes low-energy X-rays from the beam. Subsequently, movable collimators allow control of the illuminated area. A visible-light lamp and a mirror provide visual information on the illuminated area. The entire enclosure is heavily lead-shielded to minimize radiation everywhere except at the target area of the patient

Fig. 2.7

Relevance of the focal spot size. a The anode angle   relates the focal spot (i.e., the diameter of the electron beam  ) to the projected focal spot size   through  . The focal spot size limits the size of small details that can be discerned in the image. b With an idealized point source, the edge of any radio-opaque object leads to a steep transition of the exposure at the detector. When the focal spot is large, a transition zone appears, in which the sharp edge appears blurred on the detector

A complete X-ray generator contains several more components. The key element is the high-voltage generator, built around a transformer and frequently a high-voltage cascade, such as a Cockroft-Walton multiplier. Through a resistor voltage divider, the anode voltage can be measured. For example, if the voltage divider has a ratio of 1:10,000, an anode voltage of 150 kV is converted down to 15 V, which is in the voltage range of conventional operational amplifiers. A closed-loop feedback control system is therefore feasible that tightly controls the anode voltage. Similarly, a current shunt resistor allows to measure the tube current and, through a feedback control system, to control the filament current. In addition, the total dose emitted by the X-ray tube can be measured and used to shut off the high voltage after the exposure is finished. Selection of the exposure time adds a third "knob", but tube current and exposure time have similar effects.

### 2.1.2 A Focus on Geometry

The anode is usually angled to direct a large part of the X-ray radiation perpendicular to the electron beam. Moreover, the anode angle allows to balance illumination strength of the tube with its ability to resolve small details. The cathode emits an electron beam of diameter   (Fig. 2.7). The projected focal spot, as seen from the detector, has a diameter  . Both the anode angle   and the electron beam diameter   are design parameters:

  * A thin electron beam with a small diameter   has a small focal spot with good detail resolution, but poor overall intensity, because the maximum tube current is limited.

  * An electron beam with a large diameter   and a steep angle   also has a small focal spot, but larger tube currents are possible. Because of the large angle, however, the illuminated field is limited.

  * An electron beam with a large diameter   and a small angle   has a large focal spot with poor detail resolution (details are blurred). However, larger tube currents are possible, and a wide field of illumination can be achieved.

A small field of illumination is sometimes desirable, for example, for pencil-beam CT tubes. In many cases, the emphasis lies on a small focal spot, for example for dental X-rays. Several designs exist where the size of the focal spot can be adjusted, for example, by providing a cathode with two emitters (Wehnelt cylinders): a wide field illumination with a large focal spot can be used for scout images, and by switching to the fine-focus cathode, a sharper image of a smaller zone of interest can be taken.

## 2.2 X-Ray Attenuation

We will now examine the source of X-ray contrast: the absorption of X-rays in matter of different density and composition. We will first examine the mechanisms of absorption on the atomic scale, and then create a macroscopic model of absorption. High-energy X-ray photons lose some or all of their energy in a collision with atoms along their path. Often, the photon is deflected from its path, i.e., scattered. In addition, the high-energy photons can elevate shell electrons to a higher energy level, causing either excitation or ionization:

  * Ionization occurs when an electron is ejected from the atom into the continuum. Since the electron is completely ejected, the atom is positively charged (an ion) until the electron vacancy is filled. Ionization requires energy (namely, the binding energy of the electron's shell), and filling the vacancy releases this energy again (photon).

  * Excitation occurs when an electron--most often a k-shell electron--is elevated to an outer shell. Excitation requires energy, namely, the binding energy difference between the two shells. When the vacancy in the inner shell is filled, the energy is released again (photon). Since the electron remains in one of the atomic shells, the excited atom does not carry a charge (i.e, does not become an ion).

### 2.2.1 Photon-Matter Interaction

An X-ray photon can interact with matter in any of the following four ways:

1.

Rayleigh scattering. This is an elastic collision of the photon with the atom that occurs predominantly at low photon energies. The collision causes the atoms to vibrate, and the photon loses a small portion of its energy. Scattering occurs at a small deflection angle. The rate of Rayleigh scattering rapidly diminishes with increasing X-ray energy, and Rayleigh scattering plays a negligible role in the diagnostic range from 50 keV upward.

2.

Compton scattering. Compton scattering is the dominant interaction in a wide range of photon energies. In this type of interaction, the X-ray photon ejects an electron (ionization), and the X-ray photon loses the amount of energy needed to ionize the atom. The photon can be deflected by a large angle. Filling of the shell vacancy generates additional X-ray photons that lead to the typical energy peaks of characteristic radiation. These photons are emitted in a random direction. Compton scattering is undesirable in the imaging process, because it causes ionization along the X-ray path, and because of the unpredictable large-angle deflection of the photon, which leads to background haze and therefore lowered contrast.

3.

Photoelectric effect. The photoelectric effect is the most important interaction for image formation. In this type of interaction, the energy of the X-ray photon is completely expended for ejecting and accelerating an electron (ionization) as sketched in Fig. 2.8. The kinetic energy of the electron is  . The ejected electron can have a high kinetic energy and cause additional ionization (Auger electron) and X-ray production through deceleration. Filling of the ion vacancy generates characteristic radiation. The photoelectric effect diminishes rapidly with higher energies, but dominates in the diagnostic range from 50 to 70 keV. The X-ray absorption rate of the photoelectric effect depends directly on the density of the matter. Moreover, the photon is not scattered. Therefore, the photoelectric effect is the most important contributor to X-ray contrast.

4.

Pair production. At very high energies (above 1.02 MeV) the photon can spontaneously transform into a positron-electron pair upon colliding with an atom. This process is a total energy-matter conversion, and the energy needed is the equivalent of two electron masses:  . Since the energy needed for pair production is far above the diagnostic energy range, pair production does not play a role in diagnostic imaging.

In Fig. 2.9, we can see the contribution of the individual scattering events to the total absorption. At low energies, the total attenuation coefficient is very high, and most of the low-energy radiation gets absorbed by the patient. At high energies, Compton scattering becomes dominant, leading to poor contrast and to haze. A good energy range for diagnostic imaging is in the range from 50 to 80 keV, and for some applications, such as CT, even up to 150keV.

Fig. 2.8

Illustration of the photoelectric effect. a A high-energy X-ray photon expends all of its energy by elevating an inner-shell electron into the continuum. The difference between the photon energy and the binding energy becomes the kinetic energy of the electron. Note that this electron can have a high enough kinetic energy to cause braking radiation X-rays. Furthermore, the inner-shell vacancy is filled (b), which releases characteristic -rays

Fig. 2.9

a Contribution of the individual absorption events to the total X-ray absorption in a tissue-equivalent material, and b total absorption as the sum of the four contributing absorption events. Rayleigh scattering and photoelectric effect dominate at small energies. At extremely high energies far above the diagnostic energy range, pair production becomes dominant. Compton scattering is present over a wide range of energies. The ideal energy range for diagnostic purposes (highlighted in gray) exists where absorption is low enough for the majority of the photons to pass through the object, but where the photoelectric effect still contributes strongly to the total absorption. The energy-dependence of the total absorption can clearly be seen

The ability of X-ray photons to create ions along their path--either through Compton scattering or through the photoelectric effect--is the cause of radiation damage and radiation burns. Molecules, particularly organic molecules, become more reactive when one or more of their atoms are ionized. These reactive species have a higher tendency to break apart or to react with nearby molecules. Cells can repair a certain amount of damage and therefore resist the influence of background radiation and low levels of radiation exposure. When radiation exposure exceeds the ability of the cell's repair mechanism, cells sustain irreparable damage and are replaced much like normal wound healing. If the damage exceeds the self-healing ability of the tissue, the tissue or organ may fail. Independently, ionization puts DNA elements at risk of breaking apart, which increases the probability of cancer development.

### 2.2.2 Macroscopic Attenuation and Lambert-Beer's Law

We know that photons get absorbed when they pass through matter. On a macroscopic scale, the absorption manifests itself as the linear attenuation coefficient  , usually given in cm  or the mass attenuation coefficient  , given in cm /g. The two coefficients are related by the material density  . The absorption behavior is governed by Lambert-Beer's law, which can be derived by considering a very thin slice of absorbing material (thickness  ). From   incident photons,   photons are absorbed.   is proportional to (a) the number of incident photons, (b) the thickness  , and (c) the linear absorption coefficient  :

(2.6)

If the slice is thick,   becomes a function of  , because each infinitesimal layer of material has its own incident number of photons,  . In other words, the number of absorbed photons   is proportional to the number of incident photons at that location,  , and the absorption coefficient  . We obtain a first-order differential equation for  :

(2.7)

To solve the differential equation, we rearrange Eq. 2.7:

(2.8)

Now, Eq. 2.8 can be integrated from   to the length of the material   under the boundary condition that the incident number of photons at   is  :

(2.9)

Solving Eq. 2.9 for   yields Lambert-Beer's law 1:

(2.10)

The attenuation coefficient is always a function of the photon energy, that is,  . The energy-dependency of the mass attenuation coefficient for three relevant tissues (muscle, adipose tissue, and bone) is shown in Fig. 2.10. In many cases, it is sufficient to approximate the tissue by some effective attenuation coefficient. However, for the quantitative determination of   by absorption measurement, beam hardening effects need to be taken into account. Beam hardening refers to a shift of the peak energy toward higher energies due to stronger absorption of X-ray photons at lower energies. In the example of Fig. 2.11 with aluminum filters of 2, 5, and 10 mm thickness, the overall photon flux is reduced by 21, 39, and 59 %, respectively. However, the photon flux of photons above 80 keV is only reduced by 9, 21, and 38 %, respectively. The shift of the energy peak toward higher energies can clearly be seen.

Beam hardening effects are of minor importance in planar X-ray imaging, but can cause major artifacts in computed tomography. For this reason, beam hardening filters are used that absorb some of the low-energy radiation (see Fig. 2.6). Beam hardening filters are thin films of metal that preferentially absorb the lower-energy photons.

Fig. 2.10

Mass attenuation coefficient   for muscle, adipose tissue, compact bone, and, for comparison purposes, aluminum [8]. X-ray imaging shows superior contrast between tissue and bone, but contrast between different types of tissue is relatively low

Fig. 2.11

Effect of beam hardening. Shown is the sample spectrum (characteristic X-rays omitted) of a tube at 140 keV (Fig. 2.5) and the filtered spectra after 2, 5, and 10 mm of aluminum. Arrows highlight the energy maximum.

Table 2.2

Linear attenuation coefficients (approximate) of some materials and biological tissues at 50 keV

Tissue/  | Linear attenuation

---|---

material | coefficient (cm )

Air | 0.00029

Water | 0.214

Blood | 0.241

Adipose tissue | 0.193

Muscle | 0.226

Brain | 0.237

Compact bone | 0.573

Table 2.2 lists the linear attenuation coefficients of some materials and biological tissues at 50 keV. The large difference between bone and other biological tissues is particularly prominent. The absorption coefficients of other tissues, e.g., blood, muscle tissue, or brain matter, are very similar, and X-ray imaging provides excellent bone-tissue contrast, but poor tissue-tissue contrast.

### 2.2.3 Lambert-Beer's Law in Inhomogeneous Materials

Equation 2.10 assumes a constant absorption coefficient along the path length  . What does Lambert-Beer's law look like in inhomogeneous materials? Figure 2.12 shows an X-ray beam attenuated by three consecutive blocks of material with different length   and different absorption coefficient   with  . If we assume the incident number of photons to be  , we can compute the non-absorbed photons   entering the second block:

(2.11)

For the second and third block, we apply Eq. 2.10 in a similar manner:

(2.12)

By combining the three attenuation equations into one, we can see that the exponential factors are multiplied, or the exponents added:

(2.13)

It does not matter if the materials are separated by air gaps or contiguous--the overall absorption remains the same. For the continuous case of attenuation by an inhomogeneous material along a path  , we can write the general form of Lambert-Beer's law as

(2.14)

where   is the attenuation coefficient at any point   of the path  .

Fig. 2.12

X-ray absorption in three blocks of material of different size and different absorption coefficients

We need to understand X-ray imaging as projection imaging. The X-ray image is two-dimensional, and information in one spatial dimension is lost. Let us assume that the object (patient) can be described by the absorption   as a function of the three spatial dimensions  ,  , and  . Let us further make the two simplifying assumptions that (1) the image plane is the  - -plane, and (2) that the X-ray illumination is homogeneous and approximately parallel to the  -axis. In this case, the image   that represents the detector exposure relates to the object through

(2.15)

In reality, the geometry is more complex, because the X-ray source emits rays in a cone-shaped configuration, and scattered photons create a background haze that reduces overall contrast. Moreover, the incident intensity is inhomogeneous within the cone. Lastly, the detectors, most notably film, often have a nonlinear characteristic. For these reasons, conventional X-ray imaging is widely qualitative, and it requires an experienced radiologist to identify the features of interest and make a diagnosis.

### 2.2.4 Dual-Energy X-Ray Absorptiometry

One area where projection X-ray imaging is used for highly accurate quantitative measurements is DEXA (dual-energy X-ray absorptiometry), usually used to measure bone density to diagnose osteopenia or osteoporosis. DEXA scanners are accurately calibrated, and the incident photon flux is either homogeneous or its spatial distribution known. DEXA operates under the assumption of a two-component model, i.e., that X-rays are attenuated along the path by either bone or soft tissue: When an X-ray beam travels through both bone and soft tissue, Eq. 2.10 can be extended to reflect X-ray attenuation by both tissues,

(2.16)

where   is the X-ray absorption coefficient for compact bone and   is the soft tissue absorption coefficient. It is assumed that the X-ray travels a total length   in compact bone and   in soft tissue, irrespective of the order of the tissues (such as, for example soft tissue--bone--soft tissue). Usually,   is not known and X-ray absorption by soft tissue causes a higher apparent bone density. The unknown absorbance can be eliminated by measuring the intensity at two different energies along the same path. The X-ray attenuation coefficients are energy-dependent (Fig. 2.10), and two different intensities   and   are obtained from high- and low-energy X-rays, respectively:

(2.17)

When the incident intensity is known, for example, from an X-ray path through air, the intensities in Eq. 2.17 can be converted into total absorbance   and  , respectively:

(2.18)

The unknown quantity   can now be removed from the absorbance image   by computing a weighed difference of both absorbances,

(2.19)

where   needs to be chosen as

(2.20)

Like conventional X-ray imaging, DEXA is a projection imaging method. Image analysis steps consist of the automated detection of the bone region and the computation of the averaged density. An example is shown in Fig. 2.13. DEXA is typically applied to the thoracic or lumbar spine, the femoral neck, or the calcaneus.

Fig. 2.13

DEXA scan of the lumbar spine. Lumbar vertebrae L1 through L4 haven been segmented by the software (a), and bone density is determined for the outlined regions. Since density is averaged for each vertebra, the low spatial resolution seen in the AP projection is acceptable for DEXA. The software also displays the average density in relation to a standard cohort (b). The three lines indicate mean bone density over age (central line) and the standard deviation of the cohort (upper and lower lines). Bone density for this patient was found to be slightly below the age-matched mean. Image   Springer Verlag, 2011, from Haidekker MA & Dougherty G, in: Medical Image Processing: Techniques and Applications, Springer 2011

The accuracy of the DEXA method is limited for two main reasons. First, the absorption coefficients are approximations, because the X-ray beam is polychromatic. Second, and more importantly, the premise of the DEXA model is a two-tissue system (bone and soft tissue). However, soft tissue may be composed of muscle and adipose tissue, and the absorption values   and   show variability between individuals. In spite of these errors, DEXA typically shows a measurement accuracy of 10 % or better compared to ash mass [9].

## 2.3 X-Ray Detectors

X-ray intensity can be measured with photographic film or with digital detectors. Film, due to its simplicity, its high dynamic range, and its high spatial resolution is still commonly used, although direct digital acquisition becomes more and more widespread. Both film and electronic detectors have in common a low quantum efficiency. Most X-ray photons would pass through film, for example, rather than causing the chemical reaction that leads to blackening of the film. Scintillation crystals exist that convert X-ray photons into--depending on the application--visible or UV photons. Scintillation crystals are therefore used in conversion layers to absorb X-rays with a high efficiency and produce those low-energy photons for which the detectors show a high sensitivity.

### 2.3.1 Film-Based Imaging

Photosensitive film uses silver halide salts (usually AgBr ) as the photosensitive compound. The silver halide is embedded in a thin, water-permeable layer (the emulsion). When exposed to visible or UV light, the salt breaks apart, and a latent image of elemental silver emerges. Two processing steps are necessary to create the final image. First, a developer (an aqueous, alkaline solution of several organic compounds) breaks apart more silver halide near the initial atoms of elemental silver. The elemental silver is responsible for the darkening (i.e., the high light absorption) of the exposed parts of the film. Second, a fixation solution (its central chemical is an inorganic thiosulfate) removes the remaining silver halide and prevents further darkening. Thorough rinsing is necessary to remove the development chemicals. The entire development process is usually performed by automated development machines--up to the level where the film is removed from the cassette by the machine, thus making a darkroom unnecessary (daylight processing).

Fig. 2.14

Sketch of an X-ray film cassette (a) and photosensitive film (b). The cassette itself is a sturdy metal box with a removable panel to allow easy insertion of the photosensitive film. Most importantly, the cassette features a conversion layer, often on a glass support, that absorbs X-rays and emits visible or UV light. Film itself is composed of an emulsion layer that contains the photosensitive silver, applied onto a transparent polymer carrier

For mechanical stability, the emulsion layer is supported by a transparent polymer film, the carrier (Fig. 2.14b). A thin, water-permeable layer mechanically protects the film on the emulsion side. Film has a very high dynamic range with light attenuation in the darkest regions of 1:1000 or higher. Some films have emulsion layers on both sides, effectively doubling their dynamic range.

Since film is sensitive towards light in the visible or UV range, the X-ray photons need to be converted. For this purpose, the film is placed in a cassette in close proximity to a conversion layer (Fig. 2.14a). Traditionally, tungsten compounds have been used, but newer materials based on rare earth metals show a higher efficiency (Table 2.3). Particularly, the high quantum yields of the rare earth materials allow a significant reduction of the patient dose. Calcium tungstate, for example, requires about three times the dose of terbium-doped gadolinium oxysulfide (Gd O S:Tb) to emit the same amount of light. The large difference in the absorption coefficient of lanthanum oxybromide (LaOBr:Tb) between 40 and 80 keV can be explained with the presence of a k-edge slightly below 40 keV.

Table 2.3

X-ray absorption coefficient   and quantum yield of some conversion layer materials

Conversion | Absorption coefficient | Quantum | Emission

---|---|---|---

material | (mm ) | yield (%) | range

|

at 40 keV | at 80 keV

|  |

CaWO  | 4.00 | 3.15 | 4 | Blue

LaOBr : Tb | 13.1 | 1.86 | 13 | Blue

Gd O S : Tb | 4.62 | 3.29 | 19 | Green

In a wide exposure range, film darkening follows a power-law relationship with the exposure. Since the human senses have logarithmic responses, film density is usually measured with the unitless logarithmic optical density OD, defined as

(2.21)

where   is the transmission, that is, the fraction   of the incident illumination   that is not absorbed by the film. We can approximately describe the film response to the X-ray exposure   as

(2.22)

where   is the film's contrast. However, in both the high exposure range and the low exposure range, the film deviates from Eq. 2.22, and a typical film sensitivity curve is shown in Fig. 2.15. The film carrier and the emulsion are not perfectly clear. Films used in diagnostic radiology transmit about 78 % of the incident light (optical density of 0.11) when unexposed. Minor blackening of the film remains invisible against the natural fogginess of the carrier/emulsion system. Radiologists refer to the region of underexposure in the sensitivity curve as the toe region of the film. Conversely, there is an exposure level where all available silver halide has been converted to elemental silver, and further exposure does not increase blackening. This saturation region is called the shoulder region.

Fig. 2.15

Sensitivity curve a and contrast curve b of typical photographic film used in diagnostic radiology. In the toe region, very weak blackening cannot be seen in the natural fog and base absorption of the film. In the shoulder region, all available silver has been converted, and increased exposure does not lead to further blackening. A linear region between toe and shoulder exists. The film contrast is the first derivative of the sensitivity

A wide variety of radiological films is available for any specific purpose. Films can be sensitized toward special conversion layers (for example, blue-or green-sensitive films), and films with different contrast are available. High-contrast films have a very narrow linear region and need to be accurately exposed. Films with lower contrast are good for general-purpose imaging and scout images. The region of optimal contrast (i.e., the region between toe and shoulder) is called the film's latitude. High-contrast films have a lower latitude than low-contrast films.

### 2.3.2 Fluoroscopes

The invention of the fluoroscope was the first major step in the direction of lower patient exposure and lower exposure times. Initially, a fluoroscope was a conversion layer (such as calcium tungstate) mounted in front of a light-shielded observation box. The radiologists used the fluoroscope to make the X-rays visible, but were exposed to the primary X-rays in the process. Development of image intensifiers remedied this situation.

Image intensifiers are based on a special photocathode composed of two conversion layers. The first layer is cesium iodide (CsI), which acts similar to the conversion layers in film cassettes, i.e., it emits visible light when hit by X-ray photons. However, cesium iodide has a very high quantum yield, producing about 3000 visible-light photons per X-ray photon (at 60 keV). Cesium iodide has the additional advantage of forming elongated crystals, and these crystals act as a form of natural anti-scatter grid. The second layer of the photocathode, often combinations of metals with antimony salts (Sb S ), emits electrons when exposed to the visible light coming from the CsI input layer. In vacuum, these electrons can be accelerated and directed onto a phosphor screen that is somewhat similar to the screen of a monochrome TV set, i.e., it emits visible light when hit by electrons. One example for an output phosphor material is ZnCdS:Ag, which emits green light when hit by kinetic electrons.

Figure 2.16 shows the principle of an image intensifier. A photocathode, preceded by a cesium iodide conversion layer, emits electrons when hit by an X-ray photon. These electrons are accelerated in an electrostatic field toward the anode. Typical acceleration voltages are 15-25 kV, similar to CRT TV tubes. A series of focusing electrodes ensures a highly defined path of the electrons such that each point on the input window is mapped to a point on the output window. For this reason, the output phosphor displays the image projected with X-rays onto the input conversion layer, but at a smaller scale and inverted. In addition, some geometric distortion occurs, and the output image is blurred to some extent. However these disadvantages are outweighed by the very high sensitivity of the image intensifier, which allows recording of the image in real-time, that is, at video frame rates. When the output phosphor is coupled to a video camera, the image can be monitored in a separate room.

Fig. 2.16

Schematic representation of an electronic image intensifier. A conversion layer strongly absorbs X-ray photons and emits electrons. The electrons are accelerated and focused onto a luminescent output phosphor. The output window therefore displays a scaled-down and inverted image by visible-light emission. The image can be monitored in real-time with a TV camera and monitor

Image intensifiers are fundamental for interventional radiology and image-guided surgery. In both situations, the patient is continuously monitored under low-dose X-ray illumination. X-ray source and image intensifier are mounted on a C-shaped arm, which allows flexible positioning with respect to the patient.

Moreover, fluoroscopy with image intensifiers allows special procedures, such as digital subtraction angiography, where an image before the injection of a contrast agent is subtracted from an image after contrast agent injection. The result is an image that displays only the contrast material-filled blood vessels and allows excellent visibility of the blood vessels. A change of the potentials on the focusing electrodes allows to reduce the field of view and therefore enter a magnification mode that makes even smaller details visible.

More recently, image intensifiers as sketched in Fig. 2.16 are more and more often replaced by semiconductor-based intensifiers, which can have an even higher quantum efficiency combined with lower spatial distortion, but semiconductor (flat-panel) intensifiers are still more expensive.

### 2.3.3 Semiconductor Detectors

Semiconductor detectors for digital radiology work analogously to film and the electronic fluoroscope: A conversion layer captures the X-rays with high quantum efficiency and gives off visible light, which in turn is captured by conventional visible light detectors, such as CCD (charge-coupled devices) and CMOS arrays. The basic light-sensitive element in a CCD detector is remotely related to a field-effect transistor with a metal-oxide gate. The main difference is the photosensitivity. Silicon p-n junctions are inherently sensitive to light, because a photon in the visible-light energy range can generate an electron-hole pair by elevating an electron from the valence band to the conduction band (photoelectric effect in semiconductors). In the special case of a CCD sensor, the channel under the gate is exposed to light. Electron-hole pairs are generated by light exposure, and the negative charges are held--and accumulated--under the influence of a positive gate potential. The process of exposure with charge accumulation and subsequent read-out is explained in Fig. 2.17. At the end of the channel, a charge amplifier and an A/D converter provide a digital signal that is proportional to the light exposure. CMOS sensors, which are more common than CCD sensors in low-end applications, follow a similar principle, but have added per-pixel processing circuitry at the expense of light sensitivity. For X-ray detection, the detector chip (CCD or CMOS) is sandwiched with a conversion layer. Unlike electronic image intensifiers, CCD-based semiconductor detectors are flat--even thinner than a film cassette. Unlike image intensifiers, spatial resolution can be very high. In fact, CCD chips with 10  m pixel size or smaller are common. With a suitable microfocus X-ray tube, this gives rise to X-ray microscopy. On the other hand, CCD-based X-ray detectors generally do not achieve the sensitivity of image intensifiers, and a lower dose requires longer exposure times and simultaneously adds a strong noise component to the image.

Fig. 2.17

Schematic of a charge-coupled device (CCD). The cross-section of a single element (pixel) is shown in a. In weakly doped p-Si, a n-channel is created by applying a positive voltage to a MOS gate. X-ray photons are captured by a conversion layer that is grown directly on top of the light-sensitive silicon wafer. The conversion layer emits visible light, which is captured by the silicon and creates an electron-hole pair. Electrons are captured and therefore accumulated in the positive gate field during the exposure, whereas holes (positive carriers) are shunted into the p  regions that separate the channels. b shows a view from the MOS gate side. Every third gate is connected. During exposure, only G  has a positive potential. During the read-out phase, a potential is applied to G , and the negative charges are attracted to the zone underneath G  and therefore move downward. Next, the positive potential is applied to G . Thus, the packet of accumulated charges is transported to the end of the CCD array, where it is read out. Each cycle G -G -G  allows to read out one row of pixels

Sensitivity can be increased by using the avalanche principle. In an avalanche diode, a low-doped intrinsic region is placed between the p- and n-Si zones (this configuration is called pin-diode). The intrinsic barrier allows the diode to be reverse-biased with high voltages of 100-200 V. Charges created in the junction zone by photoelectric absorption are accelerated by the high potential field and release additional charges along their path (thus, avalanche). Avalanche diodes are relatively large and require additional supporting circuitry. Although they can feature very high sensitivity, the spatial resolution is generally low, and avalanche diode imaging arrays are still a subject of active research.

### 2.3.4 Photomultiplier Tubes

The ultimate detection device when it comes to sensitivity is the photomultiplier tube (PMT). PMTs are high-voltage vacuum devices, somewhat like the electronic image intensifier. A PMT consists of several electron-multiplying dynodes. As shown in Fig. 2.18, an X-ray photon is absorbed by the conversion layer and converted into visible light. A visible-light photon that enters the PMT and hits the photocathode releases an electron--the photocathode is functionally similar to the photocathode of the image intensifier. This electron is accelerated in an electrostatic field toward the first dynode. In the process, the electron reaches a sufficiently high kinetic energy to release multiple electrons from the dynode it hits. These multiple electrons are now accelerated toward the second dynode, where each electron now releases multiple electrons, which in turn are accelerated towards the next dynodes. At the end of the dynode chain, the electrons are captured by the anode and create a measurable current spike per photon that hits the photocathode. Typical PMTs have 10 dynodes (multiplying stages), and each dynode releases between 3 and 5 electrons per received electron. Each photon can therefore cause a charge of 10 million electrons (1.5 pC) to be deposited on the anode, causing a pulse of several nanoamperes for a microsecond. The multiplication factor, that is, the average number of secondary electrons released per primary electron at any dynode is determined by the voltage difference between dynodes. Typically, a voltage divider creates the voltage gradient from the cathode to the anode, and by adjusting the cathode voltage, the overall sensitivity is controlled. PMTs can be operated in a lower-sensitivity continuous mode, where the anode current is proportional to the photon flux at the photocathode, or in the higher-sensitivity photon-counting mode. In photon-counting mode, the current pulse that is caused by each photon at the anode is measured and counted. PMTs operating in photon-counting mode can easily be saturated (pulse pile-up), and in this mode, a PMT should not be operated above a flux 10  photons per second, because a strong deviation from linearity can be expected. High-sensitivity PMTs can be damaged by overexposure to light.

A very detailed insight into principles and practice of photomultiplier tubes can be gained from R.W. Engstrom's Photomultiplier Handbook [10], which is freely available on the web.

Fig. 2.18

Schematic of a photomultiplier tube (PMT). The light-sensitive part is very similar to the electronic image intensifier (Fig. 2.16) as an X-ray photon is converted into visible light, which in turn hits the photocathode, where an electron is released into the vacuum. This electron is accelerated toward the first dynode, where it gathers enough energy to release 3-5 secondary electrons from the dynode upon impact. These secondary electrons are accelerated toward the next dynode where the process is repeated. After several multiplying stages (i.e., dynodes), a measurable electron shower hits the anode

PMTs are relatively large, with optical windows between 5 and 50 mm. Therefore, any detector that relies on PMTs has a very low spatial resolution. However, this is a price worth paying when extremely low radiation doses are required, typically in nuclear medicine. In Chap. , we will see that PMTs are fundamental elements in the  -sensitive Anger camera for SPECT (single-photon emission computed tomography) and for  -detection in PET (positron emission tomography).

## 2.4 Factors that Determine X-Ray Image Quality

We have already encountered two factors that degrade X-ray image quality, blur and haze. Noise is an additional factor, notably in digital detection systems. In this section, the most important factors that cause image degradation are summarized.

  * Geometric blur. Geometric blur occurs when details are imaged that are smaller than the X-ray beam diameter. The primary factor is the size of the focal spot, since the image is a convolution of the imaged object with the intensity distribution along the focal spot. Selectable focal spot sizes or special microfocus tubes help control geometric blur. Furthermore, keeping the object (patient) close to the detector also reduces geometric blur.

  * Detector blur. Detector blur is caused by the conversion layer. The conversion layer typically emits visible light with equal probability in all directions. With a thick conversion layer, cross-talk between neighboring pixels is possible (whereas a thin conversion layer has a lower quantum efficiency). Keeping the conversion layer as close to the detector element as possible reduces detector blur.

  * Motion blur. Motion blur occurs when the patient moves during exposure, for example by breathing. Generally, high detector sensitivity and high photon flux from the X-ray source allow shorter exposure times and reduce motion blur.

  * Haze. Haze is primarily caused by scattered X-ray photons (Compton scattering). An anti-scatter grid reduces the amount of off-angle photons. For thin layers of tissue (e.g., the extremities), a lower X-ray energy can increase the influence of photoelectric absorption and Rayleigh scattering and thus decrease haze.

  * Nonlinearity, over-and underexposure. Both over- and underexposure reduce image contrast. Overexposure applies equally to film and electronic detectors. Underexposure is more critical in electronic detectors due to the higher noise floor. Prior experience and--if necessary--a scout image can help selecting the correct exposure. The optical density of a film generally depends on the exposure in a nonlinear fashion. The use of calibration phantoms of known density allow per-image calibration.

Footnotes

1

In optics, the base-10 logarithm is frequently used (e.g., the molar extinction coefficient   of a fluid), whereas X-ray absorption coefficients   use base-  logarithms. This is a common source of confusion.
Mark A HaidekkerSpringerBriefs in PhysicsMedical Imaging Technology201310.1007/978-1-4614-7073-1_3(C) The Author(s) 2013

# 3. Computed Tomography

Mark A. Haidekker1

(1)

College of Engineering, University of Georgia, 597 D.W. Brooks Drive, Driftmier Engineering Center, Athens, 30602, USA

Mark A. Haidekker

Email: mhaidekk@uga.edu

Abstract

Computed tomography (CT), also known as computed axial tomography (CAT), is a volumetric imaging modality that is based on X-ray absorption. Unlike projection X-ray imaging (Chap. ), CT allows the reconstruction of a two- or three-dimensional absorber map. CT vastly exceeds projection X-ray imaging in soft tissue contrast, but the spatial resolution of a clinical whole-body CT scanner is significantly lower than that of plain X-ray imaging. Nonetheless, CT can reveal small tumors, structural detail in trabecular bone or the alveolar tissue in the lungs. CT was introduced in 1971, and it is the first imaging modality where the computer is essential in the image reconstruction: a series of X-ray projections undergoes a transformation that yields the cross-sectional image. Since the introduction of the first CT scanners, major progress has been made in contrast, image quality, spatial resolution, and acquisition time. Modern clinical CT scanners are very fast and can produce a 2D cross-sectional image in less than a second. Spatial resolution can be as low as 100  m in-plane, and specialized CT microscopes provide voxels of less than 10  m. On the other hand, clinical CT scanners are expensive, ranging in the millions of dollars. This translates into a relatively high cost per CT scan, which prevents its more widespread adoption.

## 3.1 CT Image Formation Principles

In a projection image, such as a standard X-ray projection image, the exact location of an area of interest (e.g., lesion, tumor), cannot be determined. For example, the two objects (b) in Fig. 3.1a and b would generate similar X-ray projection images. For this reason, radiologists often take two perpendicular projections (e.g., lateral and AP   anterior--posterior), see Fig. 3.1c.

Fig. 3.1

Ambiguity in an X-ray projection. X-rays emitted by the source (a) are attenuated by an object (b, for example, a tumor) and cast a shadow (c) on the detector. The projection image is very similar in cases a and b, although the position of the tumor is very different. c Some of the ambiguity can be removed by taking two perpendicular X-ray images, such as an AP (anterior--posterior) projection and a lateral projection

The goal of computed tomography (CT) is even more ambitious. The aim of CT is to obtain a spatially resolved map of absorption coefficients   in one slice of the patient's body. Such a map, if sampled at a finite resolution, is an image in our definition. The word tomography is a combination of the two Greek words for to cut and to graph: we obtain the view of a cross-sectional cut by using tomography without actually having to use a scalpel.

Fig. 3.2

A block, composed of four unknown materials, can be probed with two perpendicular projections. Each attenuated beam intensity,   through  , provides an equation toward solving the unknown attenuation coefficients   through

To understand the mathematical foundations that allow the reconstruction of a cross-sectional image from projections, let us look at a simplified example. A cubical object of 2 cm side length is composed of four equally-sized regions with different absorption coefficients   through   and the individual side length   cm (Fig. 3.2). We can obtain two different projections (lateral, AP) and determine the overall attenuation (attenuated beams  ,  ,  ,  ). Since each projection follows Lambert Beer's law, we obtain a linear equation system that we can solve for   through  ,1

(3.1)

An arbitrary object, composed of   by   different materials requires   independent equations, but two projections only provide   equations. The solution: We can take more projections at different angles. In fact, methods exist to solve a linear equation system of the type seen above. Since in the general case the number of equations and the number of unknowns do not match, numerical methods are employed to estimate the solution space of the linear equation system. These methods are referred to as ART (algebraic reconstruction techniques) [3] and are briefly covered in Chap. .

### 3.1.1 The Radon Transform and the Fourier Slice Theorem

The mathematical foundations for an efficient reconstruction algorithm were laid as early as 1917 by Austrian mathematician Radon [11, 12] who examined a form of integral transform that later became known as the Radon transform. It was almost 50 years later, however, that Radon's work was converted into a practical image reconstruction algorithm by Cormack [13, 14]. Cormack and Hounsfield, who counts as the actual inventor of the CT [7], shared the Nobel prize in 1979.

Let us define the straight line   as the set of all points   that obey the equation

(3.2)

where   is the angle of the line with the  -axis and   is the distance of the line from the origin.2 Furthermore, let us define the scanned object in the  -plane as  . We consider Lambert-Beer's law (Eq. 2.14) in its logarithmic form where a projection   is the X-ray absorption along its path, that is,  , where   are the incident photons and   are the measured photons on the other side of the object. The X-ray beam follows the straight-line path   with the parameters   and  . The projection   can therefore be expressed as

(3.3)

This projection has become known as the Radon transform of   for   and   and is denoted  . The geometry used in Eqs. 3.2 and 3.3 is illustrated in Fig. 3.3. The straight line   is the pencil-beam line from the X-ray source to the detector. X-rays are absorbed as they pass through the object, which has the spatial distribution of absorption coefficients  . By translating the source-detector pair (i.e., changing  ) and by rotating it (i.e., changing  ), different Radon transform data   can be collected.

Fig. 3.3

Illustration of the geometry in the Radon transform. An X-ray source emits a pencil-beam ray, and some of the X-ray photons get absorbed as they pass through the object with the absorption  , and a detector measures the intensity of the X-rays passing through the object. The pencil beam follows the straight-line path   with angle   and distance   (Eq. 3.2). For any   and  , the detector can now determine the Radon transform

In his seminal work, Radon has shown that the original function   can be exactly reconstructed from an infinite number of projections at every possible combination of   and   [11]. In other words, an inverse transform   exists for which  . The implication is that the projection data   can be used to compute the unknown cross-section  . The inverse transform is impractical, however, because the infinite number of projections cannot be obtained. Numerical methods need to be employed to account for the limited number of angular projections and the placement of the beam at discrete steps. It is interesting to note that the Radon transform can be elegantly expressed in the Fourier domain, where a direct relationship between the 2D Fourier transform of the sample,   and the 1D Fourier transform of the projections emerges. This relationship is known as the Fourier slice theorem [3]:

> The Fourier transform of a parallel projection of an image  , taken at an angle  , gives a 1D slice of the 2D Fourier transform   along a line through the origin   and subtending an angle   with the u-axis.

Fig. 3.4

Projecting the image   parallel to the  -axis yields one projection  , where each point of   is the line integral along a vertical line (arrow) through

The Fourier slice theorem can be derived in a very straightforward manner for any projection parallel to one axis. With reference to Fig. 3.4, a projection   parallel to the   axis can be defined as

(3.4)

and its one-dimensional continuous Fourier transform   is

(3.5)

The two-dimensional Fourier transform   of the entire image   is defined as:

(3.6)

For   we obtain the 1D slice of the Fourier transform on the  -axis:

(3.7)

Since the exponential term is constant with respect to the integral over  , the equation can be rearranged and immediately becomes Eq. 3.5. It can now be argued that any image can be rotated by an arbitrary angle  , and its Fourier transform is also rotated by  . Therefore, any projection taken at an angle   with the  -axis can be converted to a projection parallel to the  -axis by rotating both the image and its Fourier transform.

More rigorously, we can define a rotated   coordinate system:

(3.8)

In the   coordinate system, a projection along lines parallel to the  -axis is

(3.9)

and its Fourier transform with the frequency coordinate

(3.10)

By using Eq. 3.8, the Fourier-transform   of the projection can be transformed into the   coordinate system:

(3.11)

The expression   in the exponential term describes a straight line through the origin at an angle   with the  -axis. Equation 3.11 can now be rewritten as the Fourier transform   of   with the constraints   and  :

(3.12)

Equations 3.11 and 3.12 are the same, and we arrive at the equation for the Fourier slice theorem

(3.13)

Fig. 3.5

Scanning process with a parallel-beam scan geometry. The scanned object is denoted  . A pencil-beam source   emits an X-ray beam, and a detector   measures the X-ray intensity. a One projection   is acquired by moving the source-detector system along the  -axis perpendicular to the beam. The area covered by the scan is indicated by the gray gradient, and the projection profile   is drawn above the detector position. b For the next projection, the source-detector system is rotated by a small angle   (now the angle is  ), and the projection scan is repeated. The new projection   looks different. Over   of rotation, a large number of projections is collected

### 3.1.2 Practical Image Reconstruction

Image formation and image reconstruction can most easily be explained with a CT scanner in pencil-beam geometry. Pencil-beam scanners, also known as first-generation scanners, have a highly collimated X-ray source and a detector positioned on opposite sides of the sample. The source-detector system can be moved sideways (i.e., perpendicular to the beam direction) to cover the entire sample. Furthermore, the translation mechanism can be rotated with respect to the sample, but at discrete angular increments   (Fig. 3.5). One scan, therefore, provides us with one intensity profile   that is composed of discrete intensity samples, taken at regular intervals   along the  -axis, where   is perpendicular to the beam (cf. Eq. 3.9). The scans are taken at an angular position  . The first step in the reconstruction process is to convert the intensity values into absorption values. The shoulders of   left and right of the sample provide the unattenuated beam intensity  , and the discrete projection values are computed as

(3.14)

The   are the discrete representation of the projections  . The measured absorption values can be arranged in a two-dimensional matrix, and the value of   shown as a proportional intensity (Fig. 3.6). This image representation of   is referred to as sinogram, because inhomogeneities in the sample cause sinusoidal traces.

Fig. 3.6

The individual Radon transforms   from Fig. 3.5 can be intensity-coded and displayed as a two-dimensional image, where horizontal lines are scans along the  -axis, and the vertical axis is the scan angle  . Because off-center features create sinusoidal traces, the image representation of   is called sinogram. The position of the projections   and   from Fig 3.5 is indicated by dashed lines in the sinogram

The Fourier slice theorem can now be used to devise an algorithm for image reconstruction from projection data. Such an algorithm to generate a reconstruction of the object (i.e, the attenuation matrix  ) would rely on the collection of many projections  . Their one-dimensional Fourier transforms   are then entered into a two-dimensional frequency-domain matrix spaceholder that represents  . Since it is practically impossible to fill all matrix elements of the frequency-domain matrix, missing elements need to be interpolated. Once the matrix is completely filled and interpolated, the cross-sectional image is obtained by two-dimensional inverse Fourier transform of the matrix.

The fundamental problem of this type of reconstruction is that a lot of information exists for the center of the 2D Fourier transform, while the information becomes sparse further away from the origin, requiring the interpolation of more missing values. Unfortunately, this information "further out", where most of the interpolated pixels lie, contains the high-frequency components, i.e., the image detail. This interpolation process is the single most important disadvantage of reconstruction in frequency space. Although special interpolation algorithms in frequency space exist, the most common approach today is the filtered backprojection (FBP) algorithm, which does not use the frequency domain at all. For FBP, the profile is backprojected ("smeared") into the original object plane along the projection angle. The backprojections of all profiles are added to form the reconstruction of the object.

Kak and Slaney [3] provided an elegant derivation of the backprojection motivated by the Fourier slice theorem. Since projection data are sampled in a polar coordinate system (  with the radial frequency   and the sampling angle  ), the Fourier-domain spaceholder can be interpreted in polar coordinates as well, i.e.,  , and the inverse Fourier transform that yields the final image   from the two-dimensional Fourier data can be expressed in polar coordinates   and  :

(3.15)

where the following substitution has been made for the coordinates and the differential

(3.16)

A trick here is to split the outer integral into one integral from 0 to   and another from   to  , then shift the integration bounds of the second integral. Equation 3.15 can be rewritten as the sum of the two integrals,

(3.17)

We can make use of the symmetry of the 2D Fourier transform,  , and a sign change of the integration bounds of the inner integral to join the two integrals and obtain

(3.18)

In the Fourier slice theorem, the expression   is the one-dimensional Fourier transform of the projection  , namely,   (cf. Eq. 3.10), which allows us to obtain the reconstructed image   directly from the projections as

(3.19)

The expression inside the square brackets is a one-dimensional inverse Fourier transform of the projection, but with increasing frequencies weighted proportionally by  . We can define a filtered projection  ,

(3.20)

where we made use of the line equation   and generalized the function that emphasizes higher frequencies as  . Thus, we now simply write the filtered backprojection as

(3.21)

Why is Eq. 3.21 called backprojection? It clearly integrates all projections acquired over a semicircle of rotation. Let us look at the first projection where  . The contribution for   is  , and it contributes equally to all  . One could say, the projection is "smeared" along the  -axis. With the same rationale that we used to extend the Fourier slice theorem from a projection parallel to one axis to the general rotation, the projections at any angle   are "smeared" over the placeholder   perpendicular to  . This "smearing", or projection along one axis of a rotating coordinate system, is referred to as backprojecting.

The filter function   deserves some attention. If we use the unfiltered projection   in Eq. 3.21 (in other words, we set  ), the reconstructed image would be degraded by considerable blur. More precisely, its point-spread function is approximately   (see Sect. 1.3 for an introduction to the point-spread function). The consequence of this point-spread function is illustrated in Fig. 3.7 which clearly shows than even with a high number of projections the reconstruction appears blurred. By appropriately filtering the projection with the ideal filter  , the   point-spread function can be compensated for, and the reconstruction no longer appears blurred (Fig. 3.8).

Fig. 3.7

Image reconstruction by backprojection. a The projection of a cylindrical object is backprojected ("smeared") over an empty image placeholder. b Backprojection of two perpendicular projections. c With six projections, the original object begins to emerge. d Backprojection with 36 projections. The cylindrical object is clearly recognizable, but appears blurred

Fig. 3.8

Reconstruction process using filtered backprojection. The backprojections correspond to the second and fourth reconstruction in Fig. 3.7, but a filter has been applied. The blurred appearance has been corrected

The algorithm to use Eq. 3.21 for image reconstruction starts with the collection of projections   as described at the beginning of this section. We can visualize the data as a sinogram, and each line of the sinogram is one projection. With the projection data  , we now perform the following steps:

1.

Prepare an empty image   of   by   pixels and set all pixel values to zero. This is the placeholder for the reconstructed image.

2.

For each line   of the sinogram ( ),

a.

compute the filtered projection  . One option is to compute the discrete Fourier transform, multiply each value with its frequency, and perform the inverse Fourier transform. The alternative is to perform a discrete convolution with a filter kernel as described below .

b.

for all pixels   of  , compute   where  . Add   to  . Note that   may be a non-integer value, and interpolation between the neighboring sample points is needed. Note also that due to the symmetry of the sine and cosine functions,   and   do not run from 0 to  , as conventionally done in image processing, but rather from   to  .

3.

Scale all pixel values by  .

Step 2b with the scaling factor in Step 3 represents the discrete approximation of the backprojection integral (Eq. 3.21), that is,

(3.22)

where the scaling factor   is the discrete equivalent of the differential   in Eq. 3.21 for a sinogram with   projections.

Fig. 3.9

Two widely used deconvolution kernels for filtered backprojection. a shows the filter function  , and the filter coefficients   can be obtained from integer values of  . The resulting discrete convolution corresponds to a zero-mean finite-difference filter that approximates the first derivative. Their frequency response (b) shows that the Shepp-Logan kernel attenuates high frequencies compared to the kernel by Ramachandran and Lakshminarayanan and is therefore more robust against noise at the expense of some image sharpness

Special considerations can be taken into account when designing the filter for the filtered backprojection. The idealized filter that most accurately represents the scanned object has the frequency response   as shown above. Multiplying with the frequency response of a filter in the frequency domain corresponds to a convolution in the spatial domain. A discrete filter with the coefficients   can be designed that approximates the desired frequency response. In the context of CT reconstruction, these filter coefficients are referred to as thereconstruction kernel. The filter function in Eq. 3.23 has been proposed to approximate   [15]:

(3.23)

which leads to the kernel values for the discrete convolution  ,   for all odd   and   for all even  . We can therefore formulate the filtering step as the discrete convolution of the projection values   with the filtering kernel as

(3.24)

where   is the kernel support. Because the kernel coefficients drop off rapidly with increasing  , the convolution can be made relatively short, for example with  , which leads to efficient time-domain filtering.

The filter function (Eq. 3.23) and its frequency response are shown in Fig. 3.9. It can be seen that the filter strongly amplifies the high frequencies (edge-enhancing filter). At the same time, high-frequency noise components are also amplified. For this reason, filter kernels have been proposed that show some attenuation at higher frequencies. A widely used kernel was proposed by Shepp and Logan [16] with a frequency response of   for normalized frequencies  . Its filter coefficients are  . Both the filter function   and the frequency response of the Shepp-Logan kernel are shown in Fig. 3.9. In comparison with the kernel by Ramachandran and Lakshminarayanan in Eq. 3.23, it amplifies high frequencies less and is therefore a softer kernel with slightly less image sharpness, but better signal-to-noise ratio (SNR). Taking this idea further, Rieder [17] proposed a series of functions resembling Gaussian functions that the filter kernel in Eq. 3.23 gets multiplied with to attenuate its high-frequency amplification, thus allowing to more flexibly balance reconstruction error against noise sensitivity.

Fig. 3.10

CT reconstruction of a chest vertebra and its surrounding regions using different stock kernels. The vertebra, lung tissue (dark regions) as well as the circular aorta can clearly be identified. a Standard kernel, b bone kernel, c lung kernel. Each kernel amplifies specific structural aspects of the image

Moreover, manufacturers often use proprietary modifications of the reconstruction kernel to enhance specific aspects of the reconstructed image. For example, General Electric scanners come with built-in kernels named after structural details that they are designed to enhance (Fig. 3.10). Specialized kernels like the ones namedbone andlung emphasize higher spatial frequencies. On the other hand, these special kernels may also introduce artifacts, such as additional noise or pseudo-structure. A good example is the aorta as displayed in Fig. 3.10. Ideally, blood would show as a completely homogeneous region. However, both the bone and the lung kernel introduce some pseudo-texture that does not actually exist.

## 3.2 Engineering Aspects of CT Scanners

Pencil-beam scanners (also called first-generation scanners) generate a very thin beam of X-rays. A projection is generated by moving the source-detector system (source and detector on opposite sides of the patient) laterally alongside the patient ( -direction). For the next projection, the source-detector system is rotated around the patient (angle  ), and the next projection along   is obtained. This process is very slow due to the mechanical motion involved. Fan-beam scanners (also called second-generation scanners) reduce the time requirement by illuminating one entire slice of the patient (Fig. 3.11). The source emits a wedge-shaped beam which expands toward the patient and the detector. The detector is a one-dimensional array of sensors (line detector). The line represents the  -direction. Thus, a projection can be obtained without translational motion. Image reconstruction follows the same basic steps covered in the previous section, but the backprojection equation needs to be modified to account for the divergent beam geometry.

Fig. 3.11

Sketch of a fan-beam CT scanner. The X-ray source and the detector array are arranged on a gantry on opposite sides of the sample. The source-detector system can rotate along the gantry (angle  ). The rotating coordinate   along the detector array is equivalent to that of a pencil-beam system, but a geometric correction for the divergent beam geometry is required in the reconstruction process

Fan-beam geometry is frequently employed for CT microscopes. Prerequisite is an X-ray tube with an extremely fine focal spot (in the order of few  m). By placing the sample close to the tube and relatively far away from the detector, a magnification of   can be achieved (Fig. 3.12). This arrangement causes some image degradation by scattered X-rays. Therefore, thorough collimation in the z direction is needed. Z collimation (slice thickness in z direction) also determines resolution in the z direction. Unlike patient CT scanners, CT microscopes often use a stationary source-detector system and rotate the sample inside the beam.

Fig. 3.12

Geometry of a fan-beam CT microscope (top view). By placing the sample close to the X-ray source, a magnification factor of   can be achieved. A microfocus X-ray tube is required to reduce geometric blur

A further extension of the fan-beam principle is to allow the X-rays to expand in a cone, much like the cone of a flashlight. With a two-dimensional detector array, more than one thin z slice can be illuminated and imaged in one projection. Acquisition time, especially for volumetric scans, can be significantly reduced. However, for the cone-beam geometry, image reconstruction becomes dramatically more complex. The most frequently used approaches are known as the Feldkamp backprojection [18], which is a 3D extension of the filtered backprojection, and the reconstruction proposed by Grangeat [19], which directly inverts the Radon transform.

A different scanning principle is used by the helical scanner (also known as spiral CT). A helical scanner moves the patient tray continuously while performing the source-detector revolution. In order to reconstruct one slice, projection points are obtained by interpolation in   direction so that the projection is perpendicular to the   axis. Two interpolation modes can be selected, slim and wide reconstruction. For slim reconstruction, projections that lie   apart are chosen, while   separation is used for wide reconstruction. Axial resolution is better in slim reconstructions at the expense of SNR. Helical scanners provide two major advantages: any slice position can be chosen for reconstruction without increasing patient radiation exposure, and motion artifacts are reduced over conventional CT.

Table 3.1

Typical Hounsfield Units for different materials and tissues

Tissue | Relative attenuation (HU)

---|---

Fat |  200 to  50

Blood | 40 to 60

Liver | 20 to 50

Brain tissue | 30 (gray matter) to 40 (white matter)

Bone | 80 to 3000

Contrast agents | 3000 and above

## 3.3 Quantitative CT

Another important aspect of CT is the accurate reconstruction of the attenuation map   with respect to the X-ray absorption values  . The X-ray spectrum is polychromatic, and the absorption is a function of the photon energy (see Fig. 2.10). Conventional reconstruction approaches use the attenuation along the beam path (Eq. 3.14) and assume a "representative" absorption   for the Radon transform (Eq. 3.3), which will determine the image values returned in the reconstruction. The "representative"   may vary widely between scanners, within the same scanner when different tube parameters are used, and even when the tube ages. A straightforward way to make the absorption coefficients more comparable between different scanners is to express absorption relative to water and air. Expressing absorption relative to water was originally proposed by Hounsfield [7], and the normalized attenuation values are named after him as Hounsfield units (HU). By definition, water is the reference point with 0HU and air ( ) is defined as having   HU. Equation 3.25 can therefore be used to convert specific reconstructed values   into Hounsfield units:

(3.25)

Typical HU values for different tissues are given in Table 3.1. Although contrast between different types of soft tissue is markedly better than in conventional X-ray imaging, CT excels for imaging bone due to the high bone-tissue contrast and for the chest due to the high contrast between tissue and the air in the lungs. Contrast agents can be injected that increase the X-ray absorption of blood or of organs targeted by the contrast agents.

To improve accuracy, quantitative CT scans are often performed with several phantoms of known density in the field of view. Depending on the application, the phantoms approximate the density of known tissues or materials. A phantom could, for example, include a water-filled tube, another tube filled with an organic mixture that has a similar absorption coefficient as adipose tissue, and a resin with a known amount of calcium hydroxyapatite (bone mineral). In the cross-sectional image, average absorption coefficients of the phantom regions are measured. A linear regression with the known attenuation values of the phantom provides an accurate calibration curve.

## 3.4 Image Quality and Artifacts

Image quality is primarily determined by the detector size (spatial resolution) and the number of angular projections. Typical in-plane resolution of clinical CT scanners is 0.2-1 mm with slice thicknesses of 0.5-5 mm. Often, a higher slice thickness is chosen to reduce radiation dose and improve SNR at the expense of axial resolution.

The focal spot of the X-ray tube is another key determinant of image quality. A larger focal spot blurs the image (convolution of the ideal projection with the beam intensity profile). This is a major aspect in high-resolution CT and CT microscopes, where special tubes with 3-5  m focal spot size are employed.

CT units, very much like X-ray detectors, employ collimators. The interaction of X-rays with tissue creates randomly scattered photons (e.g., Compton scattering)--randomly scattered photons create image noise and haze. Collimators in front of the detector act as anti-scatter grids and eliminate X-ray photons that deviate from a straight source-detector path. In addition, collimators in front of the X-ray tube can be used to reduce the beam size, to limit beam thickness (axial slice resolution), and to create a small apparent focal spot.

The detectors are a critical element of the unit. Detectors need to be centered with respect to the source; otherwise, the image will be blurred. In addition, detectors need to be calibrated so that the output signal intensity is identical for all detectors with the same incident X-ray intensity. Uncalibrated detectors create ring artifacts in the reconstruction. In extreme cases, a detector element may fail giving a constant output signal. Detectors and their associated signal amplifiers are also the primary source of noise. Electronic noise-suppression filters and software noise suppression reduce the amount of image noise. Noise can also be reduced by the operator by choosing a larger slice thickness or lower resolution, provided that the loss of detail is acceptable.

Since X-ray sources cannot provide monochromatic beams, an artifact related to beam hardening is common. Beam hardening occurs when the X-rays pass through strongly absorbing materials. Softer (lower-energy) X-rays are absorbed more readily, and the energy peak shifts toward higher energies as the X-ray beam passes through tissue. In projections with strong beam hardening, absorption values are therefore underestimated. Beam hardening can be reduced by prehardening the beam. A thin metal plate (frequently made of molybdenum or tungsten) is placed in front of the X-ray source. As X-rays pass through the plate, softer X-rays are absorbed, and the prehardened beam is less subject to beam hardening inside the object. In extreme cases, the reconstruction algorithm needs to be adapted. Multi-pass algorithms have been proposed [20, 21] where a first reconstruction is generated using standard reconstruction methods. Based on this reconstruction, approximate beam hardening can be computed through a ray-tracing approach, allowing to compensate for beam hardening in the projections. Reconstruction from the corrected projections reduces the influence of beam hardening.

Partial volume effects occur whenever a pixel represents more than one kind of tissue. This is particularly relevant when a tissue boundary lies within a CT slice. Partial volume effects blur the intensity distinction between adjacent tissues. Higher resolution or sometimes repositioning the patient may reduce partial volume effects.

Motion blur occurs when the patient moves (e.g., breathes) during the scan of one slice. Motion blur cannot be corrected, but the risk of motion blur can be reduced with shorter acquisition times and with the use of the helical scanning principle. Often patients are asked to hold their breath during the scan, but detailed imaging of the heart clearly continues to pose problems.

Footnotes

1

Unfortunately, even this simple equation system can only be solved when one of the absorption coefficients is known, because the four equations are not linearly independent

2

Here, the Radon transform is presented in 2D, because reconstruction most often takes place in a single plane (slice). The Radon transform can easily be extended into  -dimensional space
Mark A HaidekkerSpringerBriefs in PhysicsMedical Imaging Technology201310.1007/978-1-4614-7073-1_4(C) The Author(s) 2013

# 4. Nuclear Imaging

Mark A. Haidekker1

(1)

College of Engineering, University of Georgia, 597 D.W. Brooks Drive, Driftmier Engineering Center, Athens, 30602, USA

Mark A. Haidekker

Email: mhaidekk@uga.edu

Abstract

Nuclear imaging is related to X-ray and CT imaging in that it uses radiation. However, unlike X-ray based imaging modalities, radioactive compounds are injected into the body as radiation sources. These radioactive compounds are typically linked to pharmacologically active substances ("radiopharmaceuticals") that accumulate at specific sites in the body, for example, in a tumor. With either projection techniques or volumetric computerized image reconstruction, the spatial distribution of the radiopharmaceutical can be determined. In this fashion, metabolic processes can be imaged and used for a diagnosis. Three-dimensional reconstructions are obtained in a fashion similar to CT, leading to a modality called single-photon emission computed tomography (SPECT). A parallel technology, positron emission tomography (PET), makes use of positron emitters that cause coincident pairs of gamma photons to be emitted. Its detection sensitivity and signal-to-noise ratio are better than SPECT. Both SPECT and PET have a significantly lower resolution than CT with voxel sizes not much smaller than 1 cm. Often, SPECT or PET images are superimposed with CT or MR images to provide a spatial reference. One limiting factor for the widespread use of nuclear imaging modalities is the cost. Furthermore, the radioactive labels are very short-lived with half-lives of mere hours, and most radiopharmaceuticals need to be produced on-site. This requires nuclear imaging centers to have some form of reactor for isotope generation.

## 4.1 Radiopharmaceuticals

Both computed tomography and magnetic resonance imaging provide predominantly structural information. One exception is functional MRI, where blood oxygenation levels, and therefore metabolic activity, can be determined. Functional information of a highly specific nature can be obtained by combining tomographic imaging with radioactively labeled pharmaceuticals (known as radiopharmaceuticals or radioactive tracers). Here, the radioactive compound acts as the radiation source (unlike the external X-ray source in CT), and tomographic techniques are used to reconstruct the spatially dependent concentration of the radioactive compound.

Table 4.1

Some radiopharmaceuticals, their radioactive label and their predominant use

Radio- pharmaceutical | Radioactive label | Half life | Typical application

---|---|---|---

Fluorodeoxyglucose |  F | 2 h | Glucose metabolism, particularly in the brain, because it crosses the blood-brain barrier

Sodium iodide |  I | 13 h | Thyroid

Pentetreotide |  In | 2.8 days | Neuroendocrine tumors (somatostatin analog)

Strontium chloride |  Sr | 50 days | Bone tumors

Apcitide |  Tc | 6 h | Acute thrombosis (binds to platelets)

Pentetate |  Tc | 6 h | Kidney imaging (renal perfusion)

Krypton |  Kr | 13 s | Lung ventilation imaging

Generally, two different types of radiolabels are used: Gamma emitters for single-photon emission computed tomography (SPECT) and positron emitters for positron emission tomography (PET). Examples for gamma emitters are the metastable isotopes of krypton ( Kr) and technetium ( Tc). Examples for positron emitters are  F,  O,  N,  C, and  Rb. In some cases, the radioactive substance can be used directly. The gas  Kr can be inhaled for immediate lung ventilation studies. More often, a specific physiologically relevant compound is labeled with a radioactive atom. One example is fluorodeoxyglucose, which is metabolized at sites of high glucose demand, for example, in the brain or in some tumors. The spatial distribution of fluorodeoxyglucose accurately reflects the glucose uptake in the body. By substituting the stable  F for the positron-emitter  F, local concentrations of glucose can be imaged with PET. Some representative examples of radiopharmaceuticals are provided in Table 4.1. The examples in Table 4.1 consistently show a short half-life of the radioactive decay. Fast radioactive decay is desirable, because it reduces the total patient exposure. For example,  F has a half-life of approximately 2 h. Therefore, only 1 % of the initially administered  F is left after 12 h, while 99% has decayed into the stable  O. In fact, the fraction of  F remaining is even lower, because the radiopharmaceutical typically gets excreted through the urine.

## 4.2 Production of Short-Lived Radioactive Tracers

The typical short half-life makes transportation of the radiopharmaceuticals impractical: they need to be produced on-site and on demand. Radiolabels require a nuclear reaction to convert a stable atom (more precisely, the nucleus) into an unstable or metastable isotope. The isotope is then bound to the target drug by a chemical reaction. Some isotopes can be harvested from nuclear fission reactors. The neutron-induced fission of  U creates a large number of isotopes with mass numbers ranging from 70 to 160. They can be separated by mass spectroscopy or by chemical reactions. Simultaneously, stable atoms can be bombarded with neutrons or protons to obtain isotopes. The three following examples illustrate commonly used pathways to generate isotopes:

  * Production of  I: Stable  Xe is converted into the isotope  Xe by neutron activation in a fission reactor. Isotope  Xe decays with a half-life of 17 h into  I.

  * Production of  F: The conversion of stable  O into  F is a typical example of a cyclotron reaction. A cyclotron is a particle accelerator in which charged particles, such as protons, can be accelerated to energies in the GeV range, enough to overcome Coulombic repulsion from other nuclei. Nuclei bombarded with protons usually transmute into positron emitters.

  * Production of  Tc: The popularity of  Tc may be attributable to its relatively easy production pathway. The precursor,  Mo is produced by uranium fission in large quantities.  Mo has a relatively long half-life of almost 3 days and continuously produces  Tc. Ammonium molybdenate can be adsorbed into an aluminum oxide column. Upon transmutation of  Mo into  Tc, the resulting pertechnetate radical ( Tc O ) reacts with the sodium in saline to form sodium pertechnetate, which can be easily eluted. The generator itself is a tabletop device that mainly contains a heavily shielded alumina column.

In a subsequent step, the radiopharmaceutical is created with conventional chemistry. For example, a two-step process creates fluorodeoxyglucose from mannose trifluoromethanesulfonate. Any "hot chemistry" reaction needs to be fast, because the radiolabel decays during the reaction. Large PET facilities are equipped with a cyclotron and an attached chemical lab adjacent to the PET scanner.

## 4.3 Detector Systems and the Anger Camera

Nuclear imaging requires the use of extremely sensitive detection devices, because higher detection sensitivity allows reducing the patient dose. Detectors are sensitive to high-energy gamma photons, since the relevant decay events produce gamma radiation. Most detectors are based on photomultiplier tubes (PMT), which are introduced in Sect. 2.3.4. For gamma detection, the PMT is combined with a conversion layer and additional collimators. Common materials for the conversion layer include thallium-activated sodium or cesium iodide (NaI(Tl), CsI(Tl)), cadmium tungstate (CdWO ), and bismuth germanium oxide (BGO, Bi Ge O ). All of these materials emit visible light, and the number of visible-light photons increases with the energy of the incident gamma photon. NaI(Tl) enjoys high popularity, because it shows a high sensitivity (about 2.5 times higher quantum yield than CdWO ) and a decay time of about 230 ns (about 20 times faster than CdWO ). Furthermore, NaI(Tl) forms elongated crystals that act as natural collimators, although additional collimation needs to be employed, because source collimation similar to the collimation of the X-ray tube is not possible. Detector collimation leads directly to the spatial resolution of a multidetector system. As sketched in Fig. 4.1, the sensitivity of a single PMT for a radiation source positioned off-axis from the collimator center decreases with the lateral distance  . The sensitivity function   widens with increasing distance   of the radiation source from the detector. Therefore, the sensitivity curve overlaps progressively more with that of the neighboring PMT as   increases. A full-width half-maximum criterion can be applied to obtain the half-maximum resolution   for cylindrical collimator holes [22],

(4.1)

where   is the half-thickness of the scintillation layer. For high spatial resolution, a small   and large   is desirable, but the maximum sensitivity   follows the exact opposite trend in that it decreases with decreasing   and increasing  . It is not necessary to provide a single hole per PMT (as indicated in Fig. 4.1). Rather, collimators with multiple collimator channels per PMT are common and allow to better balance sensitivity and resolution requirements. Accordingly, a number of standard collimators exists for specific purposes, such as LEAP (low energy, all-purpose), LEHR (low energy, high resolution), or LEHS (low energy, high sensitivity) collimators.

Fig. 4.1

Illustration of the spatial resolution of an array of collimated photomultiplier tubes (PMT). High-energy  -photons are converted to visible light by a scintillation crystal (i.e., the conversion layer). Off-angle photons are attenuated by a collimator layer. The collimators can be thought of as cylindrical openings in a lead layer of length   and diameter  . The relative sensitivity   as a function of the radial position   from the center of the collimator tube is approximately trapezoidal and overlaps more and more with the sensitivity curve of the neighboring PMT (indicated in gray) with increasing distance   from the detector array

Another important consideration is noise. In most imaging modalities, the noise component can be approximated as a Gaussian, zero-mean random variable. However, in nuclear imaging modalities, individual stochastic decay events (i.e., noise itself) are recorded. In a simplified fashion, one could say that signal and noise are one and the same. More precisely, the variance of the signal increases proportionally with its average. This type of noise is called Poisson noise. In addition, the detector counts both background radiation and gamma radiation that is scattered inside the patient (cf. Sect. 2.2.1). Because of the low radiation levels involved, the PMT is operated in its high-sensitivity photon-counting mode, and each pulse contains information about the number of visible light photons that arrived simultaneously. Thresholding of the PMT output pulse allows to suppress some of the lower-energy scattered photons. Integrating the PMT signal over some time improves the signal-to-noise ratio, but the integration time is limited due to motion artifacts, the need to image larger sections of the body, and the ongoing radioactive decay of the radiolabel.

If a large array of detectors, similar to the one sketched in Fig. 4.1, but extending in two dimensions, is held above the patient's body, a projection image is obtained in which the distribution of radiolabel concentration is resolved in two dimensions. Such an array of detectors has been proposed by H.O. Anger [22, 23]. Modern scintillation cameras, referred to as Anger cameras, are based on this design. Anger proposed to arrange PMT detectors in a hexagonal pattern. Since neighboring PMT cells have overlapping sensitivity functions (Fig. 4.1), the location   of a radiation source is approximated by weighted summation of the signals   provided by the individual detectors  ,

(4.2)

where   is the spatial location of the  th PMT. The location of the event   (analogous to the center of gravity) and the overall intensity   provide the image information of intensity as a function of spatial location. If the camera is moved during recording, the center coordinates of the moving camera are added to the center of gravity. Weighted summation was performed by an analog network in the original designs. However, modern designs use digital reconstruction techniques such as maximum likelihood estimation [24, 25]. In scintigraphy, this information is often denoted as (X, Y, Z) where X and Y refer to the centroid of the radiation source in a frame and Z to its intensity.

In a practical application, a large-field Anger camera can be mounted on a gantry that allows three-axis rotation and two-axis positioning (Fig. 4.2). By moving the patient underneath the camera, a whole-body scintigram can be obtained. The planar scintigram is the nuclear imaging analog of X-ray projection imaging.

Fig. 4.2

Sketch of a whole-body scintillation imaging device. The Anger camera is mounted flexibly on a gantry to allow positioning of the camera close to the patient body. In addition, the camera can be placed at an angle with respect to the patient table to allow anterior-posterior, oblique, or lateral projections. By moving the patient table through the gantry, a whole-body projection image can be taken

## 4.4 Single Photon Emission Computed Tomography

Single Photon Emission Computed Tomography (SPECT) is the logical extension of planar scintigraphy toward obtaining cross-sectional and three-dimensional images of physiological activity, thus complementing the structural images obtained with computed tomography or magnetic resonance imaging. At any one instant, an Anger camera takes a projection image of the radioactive decay along the camera's direction of view. If the camera can be rotated around the patient, as shown in Fig. 4.3, projections can be collected at different angles that lead to the same reconstruction principles used in CT. Compared to CT, however, SPECT has a much higher noise level and a lower spatial resolution.

Fig. 4.3

Sketch of a typical camera arrangement for SPECT. Multiple cameras are often used to capture as many radioactive events as possible, therefore to increase sensitivity and reduce scan time. Cameras are mounted on a gantry and can be rotated around the patient, thus capturing projections of the events along the camera's view direction (gray-shaded areas)

The Anger camera allows the localization of a decay event by its centroid and event magnitude. The image can therefore be interpreted as the projection of the decay rate along a path   orthogonal to the camera's local   coordinate system. For reasons of uniformity, we denote as   the axis of the camera that is parallel to the direction of the patient table (pointing out of the picture plane in Fig. 4.3). For the recorded number of decay events at the camera  , we can therefore use Eq. 4.3 as a starting point,

(4.3)

where   is the number of events occurring in the three-dimensional space defined by the path   and the axial position  . Equation 4.3 can be recognized as the Radon transform (see Sect. 3.1.1) with the path   as the set of all points   for which   holds. In a SPECT device, the camera is therefore rotated around the patient's body to obtain multiple projections at different angles  . Reconstruction takes place similar to computed tomography, and the camera's   direction provides multiple simultaneously acquired slices in the axial direction.

Equation  4.3 can be inverted with CT reconstruction methods, such as the filtered backprojection. However, the approximation suffers from two confounding factors. First, the number of recorded events depends on the strength of the source,   and on the distance   of the source from the camera. Furthermore, gamma radiation, like X-ray radiation, is attenuated inside the tissue. Equation 4.3 can be extended to include these terms:

(4.4)

Equation  4.4 has been simplified to a single axial slice (constant  ). The constant   reflects the relative sensitivity of the camera, and the path   is the part of   between the radiation source and the camera. No closed-term solution exists for the inverse problem of Eq. 4.4. The approximation in Eq. 4.3 is often sufficient, particularly when local concentrations of the radiopharmaceutical need to be detected, but the absolute activity level is of no critical concern. The filtered backprojection can be implemented in a robust fashion, but the filter function   would be chosen to attenuate high frequencies even more strongly than the filter functions discussed in Sect. 3.1.2.

Even more advantageous are iterative reconstruction methods, because those methods can be optimized to consider the Poisson noise. Iterative methods are based on a matrix formulation of the projection operation. Let us assume a two-dimensional slice with discretely distributed radiation sources that emit   photons. We further assume the slice to be composed of   by   pixels that are consecutively numbered, so that the index   runs from 0 to  . We can therefore take the   as elements in the emission vector  . We take   projections  , and the   are the components of the projection vector  . Each projection takes place along a straight line that follows the line equation   with the lateral position of the detector,  , and the angle of projection,  . Projections are consecutively numbered from 0 to  , and   is the product of the number of detectors (i.e., different possible  ) and the number of angular positions. We can now interpret the event count in the Anger camera as the sum of all   along a straight line along which the camera "looks". We can therefore write the projection as a line sum,

(4.5)

where   is called the system matrix, which reflects the contribution of the  th pixel to the  th projection. In the simplest case, we can set   for all   that are touched by the straight line   and   otherwise. In practice, the relative contribution of a pixel of finite area to a ray of finite thickness is considered as sketched in Fig. 4.4. The individual contribution factors   can be arranged in a matrix   with   by   elements. The projection operation can now be written as the seemingly simple matrix operation

(4.6)

At first glance it might appear as if the inversion of the sparse matrix   can provide the solution (i.e., the unknown pixel values  ) through  , but algebraic inversion is generally not possible. An iterative approach has first been proposed by Kaczmarz [26] and is known as algebraic reconstruction technique (ART).

Fig. 4.4

Illustration of the system matrix   in arithmetic reconstruction. A detector   records radiation from all locations along one ray   (diagonal gray region). The ray has the half-maximum width of the detector. The detector can be moved laterally (along the  -direction) and placed at different angles with respect to the  -axis to provide multiple projections  . Generally, the ray does not cover the entire area of one pixel. In this example, the contribution   of the radiation source in pixel   to the projection   is the hatched area. The fraction of the hatched area of the pixel area is

For iterative reconstruction, the result vector   is initially filled with some initial value. Often   is used, but other choices can accelerate convergence. At any stage of the reconstruction, projections   can be computed from the present solution vector  . Generally, there will be an error between the measured projection and the projection of the current estimate,  . This error is backprojected over the solution vector. By using the system matrix  , we can compute the individual elements of the error vector as

(4.7)

where   indicates the  th row vector of the system matrix  . The step to advance the solution from iteration   to   (by using the error vector from the  th iteration) is to compute for each pixel   1

(4.8)

Kaczmarz showed that the iteration converges. The iterative reconstruction process can be aborted either after a fixed number of iterations or after the error magnitude falls below a certain threshold.

The general reconstruction equation (Eq. 4.8) can be modified to reflect probabilities. Specifically, the Poisson nature of SPECT data leads to the question which image   has the highest probability to generate a set of projections   when the probability that a recorded count   was caused by a true activity   has a Poisson distribution. The probability maximization leads to the maximum likelihood expectation maximization (MLEM) algorithm [27], in which the correction step is multiplicative rather than additive:

(4.9)

The denominator of the second fraction represents the projection of the reconstructed image in the present iteration: let us denote this projection of the reconstructed data as  . The second fraction is the multiplicative error  , which leads to the correction of the  th pixel  . The downside of the MLEM method is its tendency to produce noisy reconstructed images as the iterations progress. A variant of the MLEM algorithm is the maximum a posteriori (MAP) method, which differs from MLEM primarily by the introduction of an energy function   that penalizes a high noise component [28]:

(4.10)

The adjustable parameter   determines the strength of the regularization term, and it can be seen that for   Eq. 4.10 becomes Eq. 4.9. There are a number of formulations for the energy term, including the squared deviation of a pixel   from the average of its neighborhood or the squared deviation from the neighborhood's median value. To provide one example, we could define

(4.11)

where   indicates the 3x3 neighborhood of pixel   in the conventional two-dimensional fashion.

Arithmetic reconstruction has only recently gained a foothold in SPECT reconstruction due to its computational expense. The filtered backprojection, for example, provides a reconstruction several orders of magnitude faster than ART or MLEM and remained the main-stream reconstruction method for a long time in spite of the superior image quality of iterative methods. However, the development of ordered-subset reconstruction and the availability of faster hardware (including accelerated processing on graphics hardware) are increasing the attractiveness of iterative reconstruction methods.

## 4.5 Positron Emission Tomography

Positron emission tomography (PET) differs from SPECT in one fundamental way: PET radionuclides are positron ( ) emitters. After the decay event, the positron travels a very short distance before colliding with an electron. The ensuing matter-antimatter annihilation event creates two  -photons with an energy of   each. The photons are traveling in exactly opposite directions. PET scanners use complete detector rings as sketched in Fig. 4.5. Individual detectors are PMTs with a conversion layer and a collimator, much like the individual elements of an Anger camera. As conversion materials, bismuth germanium oxide (BGO) is very common, but rare-earth materials, such as cerium-doped gadolinium oxyorthosilicate (GDO:Ce) and lutetium oxyorthosilicate (LSO:Ce) are gaining popularity due to their increased sensitivity and reduced luminescent decay time.

Fig. 4.5

Schematic representation of a PET detector ring. Multiple detectors are arrayed along the ring. Each detector assembly consists of a PMT, a scintillator crystal, and a collimator. A decay event inside the patient's body creates a pair of  -photons traveling in opposite directions that are recorded by the detector ring. The event is known to have taken place somewhere along the line of angle   with the  -axis and the distance   from the origin

Unlike SPECT, the almost-simultaneous detection of two  -photons can be expected, and single events can be rejected as noise. This method is referred to as coincidence detection. Furthermore, the line that connects the two PMTs that recorded the event serves as the direction of projection. As a result, PET has a dramatically better signal-to-noise ratio than SPECT. Coincidence detection requires ultrafast electronics. Any detector that records an event opens a gate for very short time window of length  . If a second detector records an event during this window, a coincidence event is recorded; otherwise, the initial event is rejected. Noise is recorded when two independent events (this includes   decay) occur within the detection window, or when a photon is scattered and changes its direction. Many PET scanners use   in the range from 5 to 15 ns, during which light travels between 1.5 and 4.5 m. In theory, shorter   could improve the coincidence detection with better noise rejection, but limits are imposed by the response time of the PMT and the decay time of the scintillator crystal. Coincidence detection also acts as a form of electronic collimation, and the collimators in PET scanners can be designed to be much smaller and weaker than those of SPECT scanners. As a result, the sensitivity of a PET scanner is several orders of magnitude higher than that of a SPECT scanner.

Image formation and reconstruction takes place by recording coincidence events over a certain period of time (integration). Each detector pair provides   and   of the connecting line (Fig. 4.5), and the counts are collected as   for each detector ring individually. This is the sinogram of the PET scan (Fig. 4.6a). For image reconstruction, the same methods are used as for SPECT: filtered backprojection or, preferably, MLEM/MAP-based iterative reconstruction (Fig. 4.6b). Here, another difference between SPECT and PET emerges as a consequence of the coincidence detection. in PET, local attenuation does not influence the signal, only the total attenuation along a coincidence line. Attenuation correction is therefore more straightforward by either using an existing structural image (such as a CT image), or by simply assuming homogeneous absorption inside the body.

Fig. 4.6

a PET sinogram of a slice with two radiation "hot spots". The sinogram can either allow angles   from 0to 360 with positive-only values of  , or, as in this case, limit   from 0to 180 and reflect larger angles with negative values of  . b Simple filtered backprojection-based reconstruction of the sinogram, showing spatially resolved activity. c PET reconstruction, thresholded, superimposed over a MR image. The "hot spots" are highlighted with arrows. In practice, the thresholded activity map would be false-colored and superimposed over the grayscale structural image

The reconstructed activity map is normally thresholded to eliminate background noise and then false-colored. The false-colored activity map is finally superimposed over a structural image from a CT or MRI scan (Fig. 4.6c).

## 4.6 Multi-Modality Imaging

Both SPECT and PET provide functional, but not structural, information. It is often desirable to know the location of radiopharmaceutical accumulation as exactly as possible to relate the activity to a specific organ or site. For this reason, a concurrent structural image is often acquired with CT or MRI. The reconstructed activity map can then be superimposed over the structural image. Specialized dual-modality scanners exist, for example, combined SPECT-CT scanners, but they are not widely used because of their extremely high costs. Combining SPECT or PET with magnetic resonance is even more complex because of the high magnetic fields involved, and combined PET/MRI scanners are currently in the research stage [29].

More commonly, a structural CT or MRI image is taken, followed by a separate SPECT or PET scan. The separate acquisition of images immediately leads to the question of image fusion or image registration. First, SPECT and CT reconstructions typically have much lower resolution than MRI or CT images. Whereas a standard CT image can provide in-plane pixel sizes of 0.1-0.5 mm, PET and SPECT pixels are more in the range of 1 cm. Pixel and voxel sizes can be brought to match by interpolation. However, the position of the patient inside the scanner is highly variable, and some form of transformation is needed to bring both images to congruence [30]. In the simplest form, rigid-body transforms (rotation and translation), guided by external markers, can achieve congruence. The rigid-body model is usually sufficient for the head and long bones. Image registration of soft tissue, on the other hand, may involve more advanced methods of nonlinear transforms, such as using elastic models to account for soft-tissue deformation.

Footnotes

1

The step described here is actually referred to as SART for simultaneous arithmetic reconstruction technique, in which the complete error vector, rather than an individual error value, is backprojected.
Mark A HaidekkerSpringerBriefs in PhysicsMedical Imaging Technology201310.1007/978-1-4614-7073-1_5(C) The Author(s) 2013

# 5. Magnetic Resonance Imaging

Mark A. Haidekker1

(1)

College of Engineering, University of Georgia, 597 D.W. Brooks Drive, Driftmier Engineering Center, Athens, 30602, USA

Mark A. Haidekker

Email: mhaidekk@uga.edu

Abstract

Magnetic resonance imaging (MRI) is a volumetric imaging modality that parallels, to a certain extent, computed tomography. However, the underlying physical principles are fundamentally different from CT. Where CT uses high-energy photons and the interaction of photons with electrons of the atomic shell for contrast generation, MRI is based on the orientation of protons inside a strong magnetic field. This orientation can be manipulated with resonant radiofrequency waves, and the return of the protons to their equilibrium state can be measured. The relaxation time constants are highly tissue-dependent, and MRI features superior soft tissue contrast, by far exceeding that of CT. On the other hand, MRI requires dramatically more time for image acquisition than CT, unless special high-speed protocols are used (which often suffer from poor image quality). In addition, modern MRI scanners require a superconductive magnet with liquid helium cooling infrastructure, extremely sensitive radiofrequency amplifiers, and a complete room shielded against electromagnetic interference. For this reason, MRI equipment is extremely expensive with costs of several million dollars for the scanner hardware and with accordingly high recurring costs for maintenance. However, MRI scanners provide images with a very high diagnostic value, and MRI can be used to monitor some physiological processes (e.g., water diffusion, blood oxygenation) and therefore partly overlaps with nuclear imaging modalities. Since MRI is a radiation-free modality, it is often used in clinical studies with volunteers.

## 5.1 Proton Spins in an External Magnetic Field

Protons and electrons are charged particles, and charged particles in motion create a magnetic field. Electrons moving in a straight wire, for example, create a magnetic field   that drops off linearly with the distance   from the wire,

(5.1)

where   is the current, that is, the number of unit charges per second. The magnetic field   has units of Amperes per meter (A/m). Better known is the magnetic induction   , which is related to the magnetic field through

(5.2)

where   is the magnetic permeability in vacuum ( ) and   is the relative permeability of the material. The magnetic induction has units of T ( ). Vacuum and air have  . Ferromagnetic materials have large relative permeabilities (examples: Steel,  , ferrite, i.e., nickel-manganese-zinc alloys,  , Mu-metal, i.e., alloys of nickel and iron with small amounts of copper and molybdenum,  ). Ferromagnetic materials can exhibit self-magnetism when the magnetic domains are oriented in the same direction. Paramagnetic materials have a relative magnetic permeability greater than one, but much lower than ferromagnetic materials. Paramagnetic materials are attracted by magnetic fields, but do not exhibit self-magnetism, because thermal influences randomize the orientation of the magnetic domains in the absence of an external magnetic field. Paramagnetic materials of importance for MRI are oxygen and manganese, and the contrast agent gadolinium. Lastly, diamagnetic materials have a negative permeability ( ) and are repulsed by a magnetic field. Since carbon and hydrogen are diamagnetic, most organic compounds also have diamagnetic properties.

In the nucleus, protons spin 1 and thus create a magnetic moment  , where   is the gyromagnetic ratio,   is the reduced Planck constant, and   is the proton's spin number with   for a proton. The gyromagnetic ratio relates the angular momentum to the magnetic moment and is a material constant. We use   in the context of angular rotation and   in the context of linear frequency. In MRI, hydrogen is the most relevant atom due to its abundance in organic tissue: One typical MRI voxel contains in the order of   hydrogen atoms. The hydrogen nucleus consists of only one proton. The magnetic moment of a proton is very weak (Table 5.1), and the random orientation of the spins causes the magnetic moments in a small tissue volume to cancel out.

Table 5.1

Some properties of neutrons and protons

Characteristic | Neutron | Proton

---|---|---

Mass   |   |

Charge   | 0 |

Magnetic moment   |   |

Gyromagnetic ratio   |   |

Fig. 5.1

a A current loop, such as a spinning proton, experiences forces that cause it to align with the direction of the external magnetic field  . b Depending on their spin orientation, the proton's magnetic moments   (indicated by arrows) align either parallel or antiparallel to the magnetic field. A small energy difference   in favor of the parallel orientation causes slightly more protons to assume the parallel orientation than the antiparallel orientation

In an external magnetic field  , the spins experience a torque that aligns them with the direction of the external field as illustrated in Fig. 5.1a: A current loop (such as the spinning proton) with a current   flowing along a differential segment   experiences a differential force

(5.3)

that turns the current loop until it aligns with the magnetic field. The analogy is limited, because the spin orientation is a quantum effect, and spin orientations parallel and antiparallel to the magnetic field are possible. The parallel orientation has a slightly lower energy level than the antiparallel orientation, and more spins are oriented parallel than antiparallel to the magnetic field. The difference is called spin excess. The spin excess   can be approximated by

(5.4)

where   is the total number of spins in the sample,   is the reduced Planck constant,   is Boltzmann's constant, and   is the absolute temperature.   is the Larmor frequency that is introduced below. Typically,   is a very small number in the order of less than ten spins per million. Since parallel and antiparallel spins cancel out, the net magnetic moment of a sample in an external field is dictated by the spin excess. We can multiply Eq. 5.4 with the magnetic moment of a proton and arrive at an approximation for the net magnetic moment   of a sample in a magnetic field  :

(5.5)

where   is the number of protons per unit volume and   is the gyromagnetic ratio. Note that--by convention--the external magnetic field   is always assumed to be oriented along the  -axis. The net magnetic moment   in Eq. 5.5 is therefore also parallel to the  -axis and usually referred to as the longitudinal magnetization. Following the convention, we will denote the longitudinal magnetization  . The net magnetization is a tiny quantity and cannot be measured directly, particularly when it is superimposed over the very strong magnetic field  .

Fig. 5.2

a Spinning top analogy of precession. The vector   indicates the distance of the top's center of mass from the pivot point. Gravity ( ) acts on the center of mass in the direction of the negative  -axis. The spin axis  , however, is at an angle   with respect to the  -axis, and the resulting differential change of the spin axis   lies in the  - -plane tangential to the precession circle. b The analogy can be applied to the magnetic moments   that precesses at an angle   with respect to the magnetic field   and displaces the magnetic moment by   tangential to the precession circle (direction of  )

The spins do not align perfectly parallel to the magnetic field. Rather, they tend to precess around the  -axis similar to a toy top (Fig. 5.2). The spins obey the equation of motion,

(5.6)

where   is the gyromagnetic ratio. In accordance with Eq. 5.6,   dictates the angular displacement of the magnetic moment   and therefore its precession speed. It can be shown that the precession frequency is proportional to both the gyromagnetic ratio and the magnetic field (Larmor equation):

(5.7)

where   is the precession frequency or Larmor frequency. For hydrogen,  . Correspondingly, the precession frequency   for hydrogen in a 1.5 T magnet is 63.87 MHz.

## 5.2 The Spin-Echo Experiment

We now define a new coordinate system ( ,  ,  ) in which the ( ,  )-plane rotates around the  -axis with the Larmor frequency (Fig. 5.3),

(5.8)

The spins show precession with respect to the laboratory coordinate system but appear fixed when observed from the rotating ( ,  ,  ) coordinate system. A radiofrequency (RF) signal can be constructed that is circularly polarized and whose magnetic component shows the time-dependent field strength

(5.9)

where   and   are unit vectors along the   and  -axis, respectively. If   is the Larmor frequency, the magnetic vector   is stationary within the rotating coordinate system and therefore keeps stationary relative to the rotating magnetic moments. It is said that such a RF signal is in resonance with the spins, thus giving rise to the resonance part in magnetic resonance imaging.

Fig. 5.3

a Multiple spins that are precessing around the  -axis create a net magnetization   in the  -direction. We do not see any magnetization in the  - -plane, because the projections of the spins into the  - -plane cancel out. b To better examine the transversal magnetization in the  - -plane, we define a rotating coordinate system ( ,  ,  ) where the  - -plane rotates around the  -axis with the Larmor frequency   with respect to the fixed laboratory coordinate system. Relative to the rotating coordinate system, the spins do not precess and are fixed

Introducing radiofrequency energy in resonance can be compared to the one-dimensional case of a swing that is pushed in resonance, thus accumulating the energy to move with higher and higher amplitude. In the case of the RF pulse, the rotating   field introduces a torque that attempts to tip the spins around the  -axis with its own Larmor frequency  . More precisely, the additional rotating RF field   forces an additional change of the spin direction in the rotating (primed) coordinate system

(5.10)

In other words, the RF pulse rotates the spin around the  -axis at a constant angular velocity. If the RF pulse is maintained only for a short duration  , the angular change   can be quantified as

(5.11)

We can see the RF pulse flipping the spin around the  -axis. From the perspective of the fixed laboratory system, the spin "spirals" downward (see Figs. 5.4, 5.5). A very frequently applied RF pulse is a   pulse, which flips the spin into the  - -plane. With a given RF field of, for example,  , we can determine the necessary duration of the RF field to achieve a   flip with Eq. 5.11:

(5.12)

which yields  . Note that we used   in radians per second rather than the linear gyromagnetic ratio   of 42.58 MHz.

In analogy to the swing that loses its accumulated resonant energy to friction and comes to a rest after some time, the spins, flipped under the influence of the RF electromagnetic pulse, lose their energy and return to the equilibrium state parallel to the  -axis. It is important to realize that the RF pulse, apart from flipping the spins, also induces phase coherence, that is, forces the spins to orient themselves closely parallel to the  -axis. Immediately after the cessation of the RF pulse, therefore, the spins rotate in the  - -plane in coherence, and the spins do not cancel each other out as they would in a random orientation.

When the spins are in phase coherence, their magnetization in the  - -plane adds up to a resulting net transversal magnetization  . The spin vector rotates with the Larmor frequency with respect to the fixed coordinate system but appears to be stationary with respect to the rotating (primed) coordinate system. Minor variations in the magnetic field (local field inhomogeneities) lead to small deviations of the precession frequency of different spins. The spins "fan out", a process called dephasing, and the resulting net transversal magnetization   diminishes (Fig. 5.6). Loss of phase coherence is governed by a first-order differential equation,

(5.13)

where   indicates a process with respect to the primed coordinate system.   is the decay constant, which depends on two components, a decay constant   that is tissue-dependent and a decay constant   that is dictated by field inhomogeneities such that

(5.14)

Fig. 5.4

Sketch of the total magnetization vector during RF-injection (a) and during recovery (b) with respect to the fixed coordinate system. In both cases, the apparent Larmor frequency has been reduced by about two orders of magnitude to better illustrate the spin rotation. During RF injection, the flip angle increases linearly with time (Eq. 5.11), and the total time for a   flip is assumed to be around 2 ms. Note that the spins can be flipped beyond  . Recovery follows two exponential functions with different time constants (Eqs. 5.16 and 5.17). Loss of transversal magnetization occurs rapidly, whereas the longitudinal recovery is relatively slow

Fig. 5.5

Illustration of the spin orientation (see Fig. 5.4), but with respect to the rotating coordinate system. a The RF pulse at exactly the Larmor frequency linearly turns the spins around the  -axis. After a RF duration of a few milliseconds, the spins have been rotated into the  - -plane. Note that the spins can be rotated even further if the RF signal is applied even longer. b After cessation of the RF pulse, spins lose phase coherence (leading to decaying transversal magnetization  ) and more slowly return to the lowest-energy configuration parallel to  . Consequently, longitudinal magnetization   recovers

Fig. 5.6

Loss of phase coherence. a After application of the RF pulse, spins rotate in phase in the  - -plane. With respect to the rotating primed coordinate system, they appear to be oriented closely around the  -axis. b After cessation of the RF pulse, the spins lose their energy rapidly to neighboring spins, and their phase orientation randomizes. From within the rotating coordinate system, it appears as if some (slower) spins move backward, while some (faster) spins move forward within the  - -plane. c The transversal magnetization   diminishes as the spins dephase and eventually reaches an equilibrium value of zero

The dephasing process is fast, with the decay constant   on the order of a few milliseconds. On a slower time scale, the spins lose more of their energy to the surrounding lattice, and as a consequence reorient themselves parallel to the  -axis. The longitudinal magnetization recovers and eventually reaches its equilibrium value as sketched in Fig. 5.3. Once again, the spin-lattice energy loss is governed by a first-order differential equation, and the time constant is  . The   time constant therefore depends on the density of neighboring lattice spins. Like  ,   is a tissue constant. The equation that governs the energy loss after cessation of the RF pulse is known as the Bloch equation:

(5.15)

where   is the equilibrium longitudinal magnetization. The process of flipping the spins under the influence of the RF signal and the equilibration process (loss of phase coherence with loss of transversal magnetization) combined with the recovery of the longitudinal magnetization is illustrated in Fig. 5.4 for the fixed laboratory coordinate system and in Fig. 5.5 for the rotating (primed) coordinate system.

It is important to realize that the transversal magnetization   rotates with the Larmor frequency. A rotating magnet gives off RF energy (induction!), and this RF signal can be measured with an antenna. In fact, quite often the same antenna is used to induce the RF spin-flip signal and to receive the RF echo signal. The induced RF signal follows an exponentially-decaying oscillation (Fig. 5.7) that is referred to as free induction decay (FID). This RF signal is key to all measurements in MRI.

Fig. 5.7

Free-induction decay (FID). This graph shows qualitatively the current that is induced in the RF antenna coil by the spins rotating in the  - -plane. As the spins lose their phase coherence, the net transversal magnetization decays and the FID current decays proportionally. For visualization purposes, the oscillation with   is not to scale

The envelope of the RF signal, i.e., the free-induction decay function is the solution of the  - -part of the Bloch equation (Eq. 5.13). It is an exponential decay function that describes the time-dependent loss of the transversal magnetization   from its maximum at  ,   (see Fig. 5.8):

(5.16)

Fig. 5.8

Decay of the transversal magnetization with  . This function is also the envelope of the free-induction decay

Fig. 5.9

Recovery of the longitudinal magnetization. After the cessation of the RF pulse, the spins are oriented in the  - -plane, leading to a longitudinal magnetization of  . As the spins lose their energy, they return to the equilibrium orientation parallel to the   field. During this process,   recovers

Similarly, the solution of the Bloch equation for the longitudinal magnetization   is an exponential recovery function with the time constant   as shown in Fig. 5.9:

(5.17)

After these derivations, we summarize the relaxation times:

  *  : Recovery time constant for the longitudinal magnetization (parallel to  ). Also referred to as spin-lattice relaxation, because the spins lose energy through thermal processes to the surrounding lattice.

  *  : Decay time constant for the transversal magnetization (rotating moment perpendicular to  ). Also referred to as spin-spin decay time, because dephasing occurs through energy loss from a spin to neighboring spins.

  *  : Time constant for the loss of transversal magnetization caused by local field inhomogeneities, for example, local paramagnetic structures or charges. Unlike   effects, field inhomogeneities can be canceled out by reversal of the spin direction. Unlike   and  ,   is not dependent on the tissue characteristics.

  *  : Decay constant of the free induction decay.   reflects loss of transversal magnetization through spin-spin relaxation and through local field inhomogeneities (Eq. 5.14).

As a rule of thumb,  . Furthermore, fluids have a very long   and   (e.g., cerebrospinal fluid), but the presence of proteins shortens the time constants (e.g., blood). Fat has short   and  , and muscle and other soft tissues lie in-between. Some typical values for   and   are given in Table 5.2; however, reported values for   and   span a wide range. The strong difference of the decay constants between soft tissues gives rise to the excellent tissue-tissue contrast of MRI.

Table 5.2

Decay constants   and   in some tissues at

Tissue |   (ms) |   (ms)

---|---|---

Fat | 260 | 80

Liver | 550 | 40

Muscle | 870 | 45

White matter | 780 | 90

Gray matter | 900 | 100

Cerebrospinal fluid | 2400 | 160

We are presently able to obtain   by direct measurement: Obtain the spin-echo, measure its decay function, and fit an exponential function into the signal envelope to obtain  . However, we are interested in the tissue properties   and  . We need to apply special tricks to obtain these constants. The need for such "tricks" gives rise to the pulse sequences.

## 5.3 The Spin-Echo Pulse Sequence

The term pulse sequence refers to the successive application of RF pulses and RF echo acquisition, and to the application of magnetic field gradients (covered in a later section). We need to recognize that the RF circuitry has to be switched from RF excitation to RF acquisition. This process can take a few milliseconds, in the process of which we lose relevant amplitude of the FID signal. It is more practical to wait a short time after the   pulse and allow the spins to slightly dephase, then apply a   pulse that flips the spins around the  -axis back into the  - -plane. After the   pulse, the spins appear mirrored along the  -axis, but their relative speed is unchanged. Therefore, the spins now approach phase coherence (Fig. 5.10). Refocusing the spins is referred to as spin-echo.

Fig. 5.10

Rephasing after application of an   pulse. a Initially, the spins are dephasing with a decay constant of  . Three representative spins are shown: One (solid) that precesses with the Larmor frequency, one (dotted) that lags behind, and one (dashed) that speeds ahead. b With the   rephasing pulse, the spins are flipped around the  -axis. Their relative speed is unchanged, but the faster speed of the dashed spin and the slower speed of the dotted spin causes them to converge towards phase coherence

We can now envision the following sequence of events that leads to the measurement of   in a small tissue sample:

1.

Apply a   pulse to flip the spins into the  - -plane. This pulse causes phase coherence, but the spins immediately begin to dephase.

2.

Wait several milliseconds (let's call this time  ) to allow some dephasing, then apply a   rephasing pulse. At this time, the spins begin to approach phase coherence.

3.

Wait another few milliseconds ( , again) and acquire the FID signal. Find the signal maximum and its envelope and obtain  . Let us denote the time from the start of the sequence to the echo acquisition   so that  .

### 5.3.1 Measurement of

Although   is a relevant parameter in some special applications, a more important tissue parameter is  , and its determination a separate goal. Interestingly, the field inhomogeneity effects ( ) cancel out as the spins reverse their direction, but the thermodynamic (spin-spin relaxation) effects do not. We can repeat the application of the   rephasing pulse several times, and each time, the FID peak is lower due to   losses only. More precisely, subsequent FID peaks decay with  , and the analysis of repeated echoes acquired with repeated rephasing pulses yields the tissue constant  . The full spin-echo sequence, therefore, starts with a   pulse followed by a   rephasing pulse at   and acquisition of the first FID amplitude at  . With the same time intervals,   rephasing and RF acquisition are repeated until a sufficient number of echoes have been acquired to obtain   by analysis of the peak decay (Fig. 5.11).

Fig. 5.11

Diagram of the spin-echo sequence. The sequence begins with a   flip pulse followed by a   rephasing pulse at  . After twice the time (that is, after  ), the spins regain phase coherence and the FID signal is acquired. The application of the   rephasing pulse and signal acquisition is repeated several times. After some time-- --the sequence can be repeated

### 5.3.2 Measurement of   Through Incomplete Recovery

We were able to measure the transversal magnetization, because the rotating spins emit a RF signal whose amplitude is proportional to  . The longitudinal magnetization cannot be directly accessed in the same fashion, and   cannot be determined directly.

It is possible, however, to access   indirectly by not allowing the longitudinal magnetization to fully recover after a   flip. If we choose a short repetition time   (Fig. 5.11), more specifically, a repetition time that is near the longitudinal relaxation constant  , the magnetization   does not have enough time to reach its equilibrium value. However, recovery is faster in tissues with shorter  . After identical repeat times, therefore, the tissue with the shorter   will have a larger longitudinal magnetization than the tissue with longer  . On the next application of the   pulse, the actual   is flipped, and the subsequent spin echo amplitude is therefore proportional to   at the time  . Clearly, the tissue with shorter   now provides a larger FID signal than the tissue with longer  .

When incomplete recovery is chosen through short  , decay effects with   still play a role. To minimize  -effects, a short   is chosen in addition to the short   to keep the transversal decay small.

### 5.3.3 Measurement of Proton Density

We can take the idea from the previous section one step further. If we allow full recovery of   by choosing a long  , but prevent  -effects from influencing our signal by choosing a short  , the main determinant of the FID amplitude is the number of aligned spins. In other words, the FID amplitude is proportional to the hydrogen content (i.e., the water content) of the tissue. This tissue parameter is referred to as proton density or PD.

### 5.3.4 The Significance of   and

We have seen in the previous sections that the spin-echo sequence allows us to determine  ,   or the proton density. This versatility makes the spin-echo sequence one of the most widely used sequences. In summary, the radiologist chooses the measurement times   and  , and the amplitude of the FID signal carries information on proton density,  , or   following Table 5.3. More specifically, the FID amplitude maximum   is governed by Eq. 5.18:

(5.18)

Table 5.3

How to obtain  ,  , or   (proton density) contrast with appropriate choices of   and

Desired | Choice | Choice

---|---|---

contrast | of   | of

  | short | short

  | long | long

  | short | long

We can see from Table 5.3 that a fourth option exists, that is, choosing a long   in combination with a short  . This is an undesirable combination, because it mixes   and   effects, and it allows   to decay unnecessarily.

How suitable choices for   and   relate to   and   is further illustrated in Figs. 5.12 and 5.13. With a short  , little decay of the transversal magnetization occurs. With a very long  , the transversal magnetization decays strongly, irrespective of the tissue. Two different tissues with different   yield good contrast if   is chosen near the tissue's   decay constants. Note the different behavior of   and   contrast, provided that the amplitude in Eq. 5.18 is used as image intensity: In a  -weighted image, the transversal magnetization decays faster when   is short, and tissues with short   appear darker in the image than tissues with long  . Conversely, tissues with a long   appear darker in a  -weighted image than tissues with a short  , because the longitudinal magnetization recovers faster in tissues with short  , and more transversal magnetization is available at the next   flip.

Fig. 5.12

Significance of   in obtaining   contrast. We assume that   is long enough for   to fully recover, that is,  . When   is long enough to allow   recovery,  -effects no longer play a role. The transversal magnetization   decays with  , and the tissue with the shorter   (dashed line) has a faster decaying signal. When a measurement of the FID amplitude is taken at  , the tissue with the shorter   has a lower signal strength and therefore a darker shade in the image than the tissue with longer  . Ideally,   is selected long enough to obtain a strong difference in   between the tissues, but short enough to avoid unnecessary signal decay with its associated poor SNR

Fig. 5.13

Significance of   and   in obtaining   contrast. The pulse sequence is repeated rapidly, and unlike in the situation in Fig. 5.12, the longitudinal magnetization   is not allowed to fully recover by choosing a short   (left). At  , a   flip pulse is applied (arrows), and the longitudinal magnetization present at   is converted into transversal magnetization. Thus, a tissue with a short   (dashed line) and consequently faster recovery of   has a larger transversal magnetization   immediately after the   flip. Now, the FID is measured with very short   to avoid  -related magnetization decays (right). In this case, the tissue with the shorter   and faster recovery of   has a larger signal and consequently brighter image value than the tissue with longer  . Ideally,   is selected near one of the  , and   is as short as technically possible

## 5.4 From NMR to MRI: The Gradient Fields

Up to this point, we discussed spin effects in a small, homogeneous sample. Magnetic resonance imaging requires that the spin-echo signal is spatially resolved and carries local information about inhomogeneous tissue. The FID signal amplitude   can then be displayed as a function of their spatial location, that is,  , and viewed as an image. Recall that RF excitation can only occur at resonance. An RF signal that is not in resonance does not cause the spins to flip or achieve phase coherence. The secret to resolving the signal spatially is to modulate the Larmor frequency along one spatial axis and exploit the spatially-dependent resonance frequency. Recall the Larmor equation,  . If we superimpose a spatially-varying magnetic field (the gradient field)  , the Larmor frequency becomes dependent on the location  :

(5.19)

Here,   is some vector. Usually,   coincides with one axis of the laboratory coordinate system. In the following sections, we will use   as the static primary magnetic field (assumed to be completely homogeneous) and   as the superimposed spatially dependent gradient field. Both magnetic fields are parallel to the  -axis, and we can interpret the magnetic fields as scalars.   is the field gradient,

(5.20)

where   is the three-dimensional gradient operator  . Usually, the gradient follows one axis  ,  , or   of the laboratory coordinate system (although gradients along multiple main axes could be superimposed to obtain an off-axis gradient). Moreover, the gradient is linear within the field of view (in other words,   is constant), and the combined field at a location   is calculated as

(5.21)

### 5.4.1 The Slice Encode Gradient

Consider the arrangement in Fig. 5.14. A gradient field   is superimposed over the primary magnetic field such that the Larmor frequency becomes dependent on the   coordinate:

(5.22)

Any NMR phenomenon can only take place in resonance when the RF signal matches exactly the Larmor frequency. When the  -gradient is applied, only spins in the section very close to the  - -plane can be manipulated when the RF signal has the same frequency as the original Larmor frequency when  . Consequently, any spin-echo that is received must originate from this thin slice, because any section above or below this slice has not achieved phase coherence and cannot emit an echo.

Fig. 5.14

Selective excitation of a slice parallel to the  - -plane. A field gradient is superimposed over the   field so that the field strength and therefore the Larmor frequency increases linearly in the  -direction. Since proton spin and phase can only be influenced by an RF signal in resonance, only a small slice of tissue experiences the   spin flip and the subsequent phase coherence. Any spin-echo that is measured is therefore known to originate from this thin slice of tissue. If no gradient field were applied ( ), the entire volume would be excited by the RF signal. If the gradient is applied, the original Larmor frequency excites the slice at   (dashed line). However, by slightly increasing the RF frequency, a slice in the positive   direction is excited (gray shaded area, arrow), and the choice of the RF frequency allows us to select the location of the excited slice with respect to the  -coordinate

Let us assume   for an example. We furthermore apply a gradient  . Also recall that the gyromagnetic ratio for hydrogen is  . The RF frequency for the slice at   is therefore 63.87 MHz. However, if we tune the RF generator to produce 63.891 MHz, the slice parallel to the  - -plane at   is in resonance, and the 90 and   pulses only affect this slice. By choosing a RF frequency that deviates from the original no-gradient Larmor frequency, we can choose the location of the slice to be excited. For this reason, a gradient that is activated during the RF excitation phase is called the slice select gradient or slice encode gradient. A volumetric MR image is obtained by acquiring one cross-sectional slice and then moving to a subsequent slice by repeating the acquisition with a different RF frequency. Staying with the example above, we can advance in 10 mm steps in the  -direction by increasing the injected RF frequency by 4258 Hz between slice acquisitions.

How thick is the slice that is excited by the RF signal? Deviations of a few ppm (parts per million) from the Larmor frequency are sufficient to no longer meet the resonance condition. However, the boundary of the excited slice is not abrupt. Rather, some phase coherence is achieved a small distance from the center of the plane. The RF signal can be designed to have a specified bandwidth, that is, its Fourier transform is approximately constant from   and then rapidly drops to zero. It can be shown that the sinc signal, that is,

(5.23)

has a nonzero Fourier transform between   and  , and rapidly drops to zero for higher frequencies ("boxcar" function, see Sect. 1.4). By multiplying the sinc-function with the unmodulated RF signal of frequency  , the center of the boxcar function is shifted to  , and the positive half of the Fourier transform of the modulated RF signal becomes approximately

(5.24)

Here,   is the bandwidth of the sinc-modulated RF signal (Fig. 5.15). The duration of the signal, more precisely, the number of sidelobes, determines the final bandwidth. In fact, the number of zero crossings,  , of the RF pulse is an important quantity in the design of a RF pulse. The zeros of   are all nonzero integer multiples of  . The number of zero crossings is related to the bandwidth through Eq. 5.25,

(5.25)

where   is identical to the bandwidth BW,   is the total duration of the RF pulse, and the square brackets [] indicate the floor function.2 The bandwidth can be seen as the frequency range in which frequency components exist. For this reason, the bandwidth directly and linearly influences the thickness of the slice excited by the sinc RF pulse.

Fig. 5.15

a Modulation profile (envelope) for a RF signal that approximates a sinc pulse or  . The theoretical sinc-pulse has infinite support, but a practical RF pulse is time-limited, in this example to 5 cycles. b Its Fourier transform approximates a "boxcar" function with a bandwidth   around the center frequency

From the above considerations and Fig. 5.16, both the gradient strength and the RF bandwidth can be used to choose the slice thickness. A narrow bandwidth improves the signal-to-noise ratio:

(5.26)

However, a narrow bandwidth causes other undesirable effects, such as chemical shifts.3 The choice of the bandwidth is therefore a balance between different artifacts.

The slice encode gradient is applied during RF excitation. Since the Larmor frequency slightly differs over the thickness of the slice, the spins are not in phase. Rather, spins accumulate phase in a way that bears much similarity to the phase encoding we need for the encoding in the third dimension (see Sect. 5.4.4 below). For this reason, a second gradient pulse is applied right after the first pulse of the slice encode gradient, but with opposite sign and with half the duration. This inverted gradient is referred to as the rephasing gradient, and it can be seen as "rewinding" the phase. After application of the rephasing gradient, all spins have zero phase.

Fig. 5.16

Both the bandwidth of the RF signal ( , see Fig. 5.15) and the gradient strength can influence the selected slice thickness. The diagonal line represents the gradient  . In Panel b, the gradient   is larger than in Panel a. The no-gradient Larmor frequency   selects a slice around  , indicated by the dashed line. The slice thickness TH can be influenced by the RF bandwidth  , and a larger bandwidth excites a thicker slice centered on  , where  . When   is larger (b), the slice thickness is smaller

### 5.4.2 Fourier-Encoding with the Gradient

The Larmor frequency changes instantly with the magnetic field strength. Once the slice select gradient has been turned off, all excited spins return to their original, non-gradient Larmor frequency  . Let us now introduce a second gradient   along a direction   in the  - -plane, whereby   subtends an angle   with the  -axis. The gradient, superimposed over the primary magnetic field, creates a direction-dependent Larmor frequency:

(5.27)

It is justifiable to interpret the measurable magnetization with the gradient   as a projection, because each frequency component contains the cumulative spin-echoes from thin "strips" orthogonal to the gradient direction. Let us denote the new axis that is perpendicular to   as  . Assuming that we excited only a thin slice by making use of the slice select gradient, these strips would be parallel to the  -direction, and we can write the cumulative transversal magnetization   (i.e., the projection onto the  -axis) as

(5.28)

where   is the transversal magnetization of an infinitesimal volume at the coordinates   and  . It is important to realize that the measured signal is a harmonic oscillation. In fact, the demodulated signal with respect to the rotating coordinate system is a time-dependent complex function  ,

(5.29)

where we introduce a normalized time   defined as  . The most interesting feature of Eq. 5.29 is its Fourier-transform character. In fact, the measured signal   and the projection   for any angle   form a Fourier transform pair so that the projection can be recovered from the signal by an inverse Fourier transform,

(5.30)

We can claim that the projection is Fourier-encoded in the measured signal by the magnetic gradient. The relationship to the Radon transform in computed tomography is interesting to note. In fact, the projections could be used to reconstruct the cross-sectional image by filtered backprojection, but in practice, a Fourier matrix, called k-space matrix with two orthogonal normalized time axes   and   can be filled by Cartesian sampling.

### 5.4.3 The Frequency Encode Gradient

In a simplification of the considerations from the previous section, we can instead introduce a second gradient, orthogonal to the slice select gradient, that is applied during the RF echo acquisition. Let us denote this gradient  , whereby the resulting magnetic field with the frequency encode gradient enabled becomes:

(5.31)

At this point, the frequency of the received RF signal depends on the  -position of the spins (Fig. 5.17). With the Fourier transform, we can now determine the signal strength as a function of the frequency, and consequently as a function of the  -position. We have localized the signal in the   direction by applying the gradient   during RF injection, and we have localized the signal in the   direction by applying the gradient   during echo acquisition.

### 5.4.4 The Phase Encode Gradient

With the slice select and frequency encode gradients, we have a one-dimensional signal from a two-dimensional slice of tissue, and one spatial direction is encoded in the frequency. Similar to Fourier-based reconstruction in CT, we need to fill an empty Fourier-domain spaceholder with data to obtain the cross-sectional image by inverse Fourier transform. With the definition of the normalized time   (Sect. 5.4.2), the Fourier-domain spaceholder is called k-space matrix.

Fig. 5.17

The Larmor frequency of the spins reacts instantaneously to changes in the magnetic field. If a gradient (for example, in the  -direction) is turned on during echo acquisition, the frequencies of the spins are proportional to their position along the  -axis. Quadrature demodulation and subsequent Fourier analysis yields the signal strength as a function of the  -coordinate

Fig. 5.18

Filling of the  -space matrix with repeated acquisitions at different gradient strengths  . Each gradient defines a position along   (arrow), and a subsequent frequency-encoded acquisition fills one column of the  -space matrix at   (gray shaded area). With repeated acquisitions that use different strengths for  , the entire  -space matrix can be filled

The solution to the defined filling of the  -space matrix is to make use of the gradient Fourier encoding introduced in Sect. 5.4.2. A gradient   changes the Larmor frequency along an arbitrary direction   in the  - -plane. If the gradient is applied for only a short time  , the spins accumulate a spatially-dependent phase  :

(5.32)

Equation 5.32 leads to the same type of Fourier encoding of the demodulated signal by the gradient through its phase. Since we have restricted RF excitation to the  - -plane and we are decoding the signal along the  -axis with a gradient in the  -direction, the phase encode gradient needs to be orthogonal to the two other gradients, that is, in our example in the  -direction. Whereas the frequency encode gradient provides us with a complete column of data in the  -space matrix, the phase encoding provides us with a single point along the   axis. We can therefore interpret the choice of one specific gradient   as positioning of the acquired row along   on a specific position   (Fig. 5.18). To completely fill the  -space matrix, repeated acquisitions need to be performed with different gradient strengths   (and its associated position  ) until all rows of the  -space matrix are filled and the desired cross-sectional image emerges as the inverse Fourier transform of the  -space matrix.

## 5.5 Putting Everything Together: Spatially-Resolved Spin-Echo Acquisition

We have now assembled all components for magnetic resonance imaging, and a summary of how the individual components fit into the bigger picture is helpful. We therefore revisit the spin-echo sequence, but with the gradients included (Fig. 5.19). The sequence follows the basic structure of the spin-echo experiment (Fig. 5.11) introduced earlier. In addition, a slice select gradient restricts RF excitation to a defined slice, and two orthogonal gradients allow Fourier encoding of the signal. Since one acquisition fills one column of the  -space matrix, the sequence needs to be repeated with a different phase encode gradient for each column of the  -space matrix. The significance of   and   to obtain  -,  \- or proton density-weighted images remains the same as in Sect. 5.3.

Fig. 5.19

Spin-echo sequence to obtain a cross-sectional image. The basic structure follows the spin-echo experiment (Fig. 5.11), but with the added spatial gradients. Any RF injection needs to be accompanied by the slice select gradient   to restrict spin excitation to a small plane. Positioning in  -space along   is performed with the phase encode gradient   that is applied between the rephasing pulse and the echo acquisition. A frequency encode gradient   is applied during echo acquisition to obtain the Fourier encoded information along the  -axis

The necessity to repeat the sequence multiple times gives rise to the question of exam duration. Two key determinants are   and the number of applications of   to fill the  -space matrix. If this number is denoted  , the total acquisition time for one slice is

(5.33)

where   is the number of repeated acquisitions with the same gradient. Averaging multiple acquisitions improves the signal-to-noise ratio by

(5.34)

For example, a  -weighted image sequence with a resolution of   and a relatively short   of 500 ms requires roughly 2 minutes per slice. A  -weighted image, which requires a longer   of 2 s (and assuming  ) would take more than half an hour per slice. Although the spin-echo sequence is very popular due to its versatility, faster sequences are available that significantly shorten the acquisition time, although often at the expense of image quality.

## 5.6 Other Imaging Sequences

Many alternative sequences exist, some of them with the primary purpose to reduce the acquisition time. As one simple example, a sequence could make use of the Hermitian symmetry of the Fourier space and merely fill half of the  -space matrix, thus reducing acquisition time by a factor of two, but at the expense of lower SNR. This sequence is known as Half-fourier Acquisition Single shot Turbo spin-echo ("HASTE" ).4 In another example, called fast spin-echo, FSE, repeated   pulses are used to rephase the spins and elicit a spin-echo after one initial   excitation pulse. By applying a phase encode gradient after each   rephasing pulse, multiple lines of the  -space matrix can be filled with one   excitation (and thus within  ). However, since the echo decays over time, a   excitation needs to be performed every few lines. Typically, FSE is used for  -weighted images, and the acquisition is shortened by a factor identical to the number of  -space lines filled between   pulses.

We will introduce a few key sequences in this section and explain their fundamental purpose. The goal of this section is more to provide an idea of what different sequences can achieve rather than provide a comprehensive list of available sequences.

### 5.6.1 Gradient-Recalled Echo Sequences

In Sect. 5.4.1, we mentioned that the slice encode gradient is followed by a negative gradient lobe of half the area, whose purpose is the rephasing of the spins. After a RF pulse with slice encode gradient and rephasing lobe, the spins are reset to have a   phase. Analogously, beginning the frequency encode gradient with a negative dephasing lobe, followed by the regular frequency encode gradient leads to phase coherence very similar to the spin-echo elicited by the   rephasing RF pulse. In this fashion, the FID echo signal can be obtained through the application of a gradient instead of the   rephasing pulse. Such a sequence is called gradient recalled echo or GRE sequence. A typical GRE sequence diagram is shown in Fig. 5.20.

GRE sequences have two advantages over the spin-echo sequence. First, because of the absence of the   rephasing pulse,   can be shortened, and second, it is not necessary to flip the spins by  . Recovery from a lower flip angle (see next section) is faster, and   can be shortened. Both factors allow for faster acquisition times. Depending on   and  , GRE sequences can provide  -,  -, and proton density-weighted images, although it is most often used to acquire  -weighted images because of its shorter   and consequently overall shorter acquisition time.

Fig. 5.20

Gradient recalled echo (GRE) sequence. In a GRE sequence, the initial RF pulse may flip the spins by an angle  , and no   rephasing pulse is applied. Instead, the FEG pulse is used to achieve phase coherence after an initial dephasing lobe. Acquisition takes place during the phase coherence period. Note that the shaded regions under the SSG and FEG pulses have the same area

Further speed improvement is possible by adding a spoiler gradient. We discussed before that field inhomogeneities shorten   and  . After echo acquisition, a strong gradient pulse (spoiler pulse) can be applied that "swirls" the protons and shortens the time to equilibrium. Further reduction of   is possible, notably in  -weighted sequences. GRE sequences are not typically used for true  -weighted images, because the gradient recalled echo does not accommodate spin inversion in a similar manner as the repeated   pulse in a spin-echo sequence does.

#### 5.6.1.1 Fast, Low-Angle Shot

The FLASH sequence is a good example of a high-speed GRE sequence. For a FLASH sequence, the initial   pulse is shortened to flip the spins halfway between their equilibrium position and the  - -plane. A transversal magnetization exists as the projection of the spins onto the  - -plane, and the projected transversal magnetization is smaller than it would be with a full   flip. Since phase coherence is achieved, a RF signal can be received, but the signal is attenuated by a factor of   over the conventional spin-echo sequence, where   is the flip angle. Corresponding with the lower signal, SNR is lower, and the images appear more noisy. However, a simple relationship between SNR and flip angle does not exist. Rather, a preferred flip angle exists that maximizes SNR for very short  , because the longitudinal magnetization is kept near its equilibrium. The flip angle that optimizes SNR is known as Ernst angle  , for which holds:

(5.35)

Since the spins are close to their equilibrium position,   can be shortened considerably. A gradient is applied to spoil the transverse magnetization, and very short   becomes possible. The FLASH sequence is often used to acquire 3D MR images with   weighting, when a multi-slice acquisition would take a very long time with the spin-echo sequence. Since   can be shortened to tens of milliseconds, a 3D volume scan of the head could be completed in a few minutes. With the very fast acquisition times possible with GRE sequences such as FLASH, motion artifacts are reduced, and it even becomes possible to image the uptake of contrast agents. In conjunction with gated acquisition (i.e., acquisition synchronized with the heartbeat), the heart can be imaged.

### 5.6.2 Inversion Recovery Sequence

The inversion recovery sequence is a sequence to achieve excellent   contrast. Its main disadvantage is that it requires a very long  . Unlike the spin-echo sequence, the inversion recovery sequence begins with a   flip, which flips the longitudinal magnetization from   to   (inversion). Recovery requires at some point--depending on the tissue's  --to go through  . After a recovery period  , a   pulse is applied that flips the current   into the  - -plane, and a transversal magnetization is obtained that is proportional to   at the time  . With a conventional rephasing pulse or gradient, the FID echo can be measured at the echo time  . Note that   is another variable time parameter, in addition to   and  . However,   is generally kept as short as possible to eliminate  -effects and   is kept long to allow sufficient recovery (unlike   in the  -weighted spin-echo sequence, where it is kept short to force incomplete recovery). Since recovery occurs from the highest possible energy level (the point farthest away from equilibrium), recovery times of 5 s or longer are not uncommon.

The demodulated signal intensity   can be approximated as (cf. Eq. 5.18):

(5.36)

and we can see from Eq. 5.36 that no terms with   occur. Note also that some choices of   and   may lead to negative values of  . Intuitively, this happens when   is so short that the spins are still flipped below the  - -plane. In fact, we can solve Eq. 5.36 for   and find for   that the signal disappears for  . this phenomenon of suppressing the signal from specific tissues gives rise to a set of tissue-attenuating sequences of which two are listed below.

#### 5.6.2.1 Short-Tau Inversion Recovery

With a short inversion recovery time  , adipose tissue with its short   is attenuated or even completely suppressed. Typical values for a STIR sequence are  -  and  . STIR images can sometimes emphasize tissue detail where fat content can lead to reduced contrast (this includes the brain), and it can reduce the distracting chemical shift that is associated with hydrogen embedded in fat.

#### 5.6.2.2 Fluid Attenuated Inversion Recovery

The opposite to STIR is FLAIR with a long   to suppress the signals from fluids, such as CSF. Since fluids have a high proton density, their signal can be overwhelming, and FLAIR images can bring out anatomical detail that are otherwise overshadowed by the fluid signal. A typical FLAIR sequence uses   and   or longer. The very long   is required for full   recovery. With such long  , FLAIR sequences become extremely time-consuming and are rarely performed.

### 5.6.3 Echo Planar Imaging

The purpose of the echo planar imaging (EPI) sequence is to achieve an extremely short acquisition time. In fact, it is possible to fill the entire  -space matrix within one  -period. The EPI sequence starts with a   spin-flip pulse followed by a strong combined PEG and FEG gradient to position the spins in one corner of the  -space matrix. A   rephasing pulse follows immediately. After the rephasing pulse, an oscillating frequency encode gradient alternates the direction in which the lines of the  -space matrix are filled per echo, whereby a simultaneous "blip" of PEG advances the readout by one line in the  -space matrix (Fig. 5.21). Therefore, the EPI sequence fills the  -space matrix in a zig-zag pattern. FID amplitude is uneven, and a maximum FID signal occurs at the effective echo time  , which is not an adjustable parameter in EPI. Due to  -effects, acquisition of the entire  -space matrix must occur within a period less than   (around 20-50 ms), which places a high demand on the gradient system and the RF acquisition rate. A gradient-recalled alternative exists, which does not need the   refocusing pulse.

Fig. 5.21

Spin-echo planar imaging (EPI) sequence. EPI begins similar to the spin-echo sequence, but there is a strong gradient between the   flip pulse and the   rephasing pulse, which positions the spins at the corner of the  -space matrix. With alternating FEG pulses and short PEG blips, the  -space matrix is filled in a zig-zag pattern within one  -period

Fig. 5.22

Overview of a complete MRI system. EMF-critical components (the primary magnet, gradient-, and RF subsystems, and, of course, the patient) are placed in a separate, shielded room. Control units are found in a different room, from which the MRI scanner is operated. The cooling system that provides liquid nitrogen and liquid helium is placed in a third, separate room

EPI images typically have a low resolution (  or  ) with very poor SNR. The EPI technique is particularly susceptible to artifacts, such as chemical shifts and local magnetic inhomogeneities, but it is one of the few techniques with real-time capabilities, being capable of delivering images with frame rates of 20 fps or higher. EPI has been used for monitoring physiological processes, such as the beating heart.

## 5.7 Technical Realization

The core of a MRI device contains three key components: The primary   magnet coil, the gradient subsystem, and the RF subsystem. In addition, a computer for timing and sequence control and for data processing is required. An overview of an entire MRI system is shown in Fig. 5.22, and some of the key components are discussed below.

### 5.7.1    Magnet

In early MRI devices, the   field was generated by permanent magnets, which were able to provide up to 0.2 T. However, a higher strength primary field has a number of advantages, predominantly shorter recovery times (shorter   and thus shorter  ), and a better SNR (cf. Eq. 5.4). Higher magnetic field strengths require electromagnets, but conventional coils are impractical due to thermal losses. For this reason, practically all primary magnets use superconducting coils.

A typical superconducting coil is a copper wire of about 2 mm diameter, which contains several NbTi filaments (strands) of about 0.1 mm diameter. This wire is then used to form an air coil (i.e., there is no supporting iron), and a typical MRI magnet coil can have a total filament length of several kilometers and carry currents of up to 500 A. The coil is then supercooled with liquid helium. One interesting aspect of the superconductive nature of the coil is the need to charge the coil, that is, introduce the current that then continues to flow without resistance. With a constant voltage   applied to the coil, the current builds up over time,

(5.37)

Once the target current is reached, the applied voltage is disconnected and the coil is shorted inside the supercooled system, thus allowing the current to continue to flow. A clinical 1.5 T magnet now has a total energy of   stored, enough to power a 60 W light bulb for 18.5 h. When a magnet needs to be de-energized, it gets discharged just the opposite way, i.e., by applying a negative voltage until the current reaches zero.

The helium acts not only as supercoolant, but also as a safety medium in case of coolant failure. If the coil temperature rises above the superconductive threshold temperature, the energy of about 4 MWs is dissipated instantaneously, but the helium absorbs the thermal energy. This process is known as quenching. In the process, a large amount of helium evaporates, which is costly to replace.

A final important aspect of the   coil is its homogeneity. Since magnetic gradients are used to spatially resolve the signal, any inhomogeneity leads to a spatial distortion. In fact, it is easy to calculate that a field inhomogeneity of only 50 ppm (parts per million, 0.005 %) in a 1.5 T clinical scanner with gradient system of 10 mT/m leads to a spatial distortion of 7.5 mm, which corresponds to tens of voxels or a shift between one and two axial slices in a typical 3D volume. Due to manufacturing tolerances, an out-of-the-box homogeneity of 500 ppm can be achieved. Any further improvement of the field homogeneity is performed on-site by measuring the field inside the bore (usually several hundred measurement points) and adding small shim magnets to offset the inhomogeneity. Additional shim coils allow extreme fine-tuning under software control.

### 5.7.2 Gradient Subsystem

The gradient subsystem contains the coils responsible for the  ,  , and   gradients and their driver circuits. The gradient coil system contains three independent coils with independent drivers. Interestingly, off-axis gradients can be applied when two coils are driven simultaneously. This allows, for example, to acquire angled slices.

The main challenge for the gradient subsystem is the fast response time required for fast gradient switching (for example, the rapid on-off sequence for the phase encode gradient). To achieve large gradients and fast gradient rates at the same time, both the coil's inductivity and the resistance are kept low. Gradient coils require currents of several hundred amperes with a transient response of hundreds of kA/s. Accordingly, extremely powerful gradient drivers deliver high-voltage pulses to build up the current rapidly, and coil-amplifier systems are carefully compensated to provide the optimal step response. The compensation needs to take into account the generation of eddy currents in conductive structures around the coils.

The rapid switching of gradient fields inside a strong   field causes the gradient coils to physically deform under the magnetic forces. This deformation causes the typical loud clicking, buzzing, or humming sound during the image acquisition.

### 5.7.3 RF Subsystem

The RF subsystem uses frequencies in the mid- to high MHz range, near the frequency range of FM radios. The resonance frequency for 1.5 T is 63.9 MHz, and for 3 T, 127.7 MHz, to provide two examples. Their corresponding wavelengths are 4.7 m and 2.35 m, respectively. In its simplest form, an RF coil is a cylindrical copper band with a small gap, like the letter  . The RF current, applied to the ends of the gap, causes a wave of current to travel along the coil. The resulting   magnetic field rotates with the wave. The same coil can be used to receive the echo signal, but the sending and receiving currents are several orders of magnitude different. For higher sensitivity applications, transmit and receive coils are often separated, and the smaller receive coil is placed more closely to the organ under examination (e.g., head coil or knee coil). Coils are combined with a capacitor to create a resonant LC circuit.

More sophisticated coils include phased-array coils with multiple wires or multiple coils in a circular arrangement, which receive phase-shifted versions of the same signal. The result is again a rotating magnetic field. Three representative design principles are shown in Fig. 5.23. A single-wire coil with a capacitor to provide LC resonance (Fig. 5.23a) can be used both for transmitting and receiving RF signals. A similar coil, made of thin wire with multiple windings, is frequently used as a receive coil for surface applications. A resonator can provide a full quadrature signal. In Fig. 5.23b, six discrete antenna wires in the   direction carry sinusoidal currents with a   phase shift with the resulting current density rotating with the angle  . Such a resonator is used for RF excitation only. The saddle coil (Fig. 5.23c) can be used for transmit and receive applications with small dimensions. The principle of the saddle coil is also used to generate the   and   gradient fields.

Fig. 5.23

Schematic of three possible realizations of an RF coil. a Single-turn solenoid with resonance capacitor. b Resonator cylinder (axial view) with discrete antenna wires. Each wire carries a sinusoidal current   through  , each of which lags   over the previous one. c Saddle coil, typically used for small-sized receive coils

The RF signal is generated by digital synthesis. However, an enormous precision is required for the frequency synthesis. With single-side-band (SSB) modulation, a low-frequency offset can be added to a carrier signal at the Larmor frequency  . An SSB modulator is a multiplier that takes the real and imaginary parts of the input signal   and multiplies it with the carrier frequency to obtain the transmit current  :

(5.38)

If the input signal is a low-frequency complex signal  , the resulting current is real-valued and contains both the low-frequency offset and its phase shift:

(5.39)

A transmit amplifier is required to amplify the current such that the resulting   field is strong enough to allow short flip pulses. For example,   is needed to achieve a   flip in 1 ms (Eq. 5.11). Depending on the coil design, the peak output power of the transmit amplifier may exceed 2 kW.

The receive signal is quadrature-demodulated, meaning, the real-valued induction current is multiplied with   to obtain the real component of the signal and with   to obtain the imaginary component of the signal. The multiplication provides terms with the sum and the difference of the frequencies. A low-pass filter removes components with  , and the remaining signal contains only the low-frequency component (the component modulated by the frequency encode gradient!) and the phase. It becomes more and more common to use fast digital signal processors for demodulation, but the initial amplifier and low-pass filter need to be carefully designed to minimize amplifier noise.

Minimization of noise is also one reason why the entire MRI scanner, notably with its RF components, is housed in an electromagnetically shielded room (Fig. 5.22) to reject environmental interference from 60 Hz power lines, radio stations, cell phone towers, or other sources of electromagnetic contamination.

Footnotes

1

For this reason, spin and proton are often used synonymously in MRI terminology.

2

The floor() function in the C language indicates the largest integer less than or equal to its argument.

3

Chemical shifts are very small shifts of the Larmor frequency depending on the environment, for example, whether the proton (i.e., hydrogen) is bound in a water or a lipid molecule. Chemical shifts are the basis of NMR analysis, but play a minor role in MRI. We will not further discuss chemical shifts in this book.

4

It seems to be a convention in MRI to often find cute or funny acronyms for the sequences. Enjoy.
Mark A HaidekkerSpringerBriefs in PhysicsMedical Imaging Technology201310.1007/978-1-4614-7073-1_6(C) The Author(s) 2013

# 6. Ultrasound Imaging

Mark A. Haidekker1

(1)

College of Engineering, University of Georgia, 597 D.W. Brooks Drive, Driftmier Engineering Center, Athens, 30602, USA

Mark A. Haidekker

Email: mhaidekk@uga.edu

Abstract

Ultrasound imaging makes use of the properties of sound waves in tissue. Pressure waves in the low megahertz range travel through tissue at the speed of sound, being refracted and partially reflected at interfaces. Ultrasound contrast is therefore related to echogenic inhomogeneities in tissue. The depth of an echogenic object can be determined by the travel time of the echo. By emitting focused sound waves in different directions, two-dimensional scans are possible. Ultrasound images are highly qualitative in nature due to the complex relationship between inhomogeneous tissue and the echoes, due to the differences in speed of sound in different tissues, and due to the high noise component that is a result of the weak signal and high amplification. Ultrasound images show good soft tissue contrast, but fail in the presence of bone and air. Although ultrasound images can be generated with purely analog circuitry, modern ultrasound devices use computerized image processing for image formation, enhancement, and visualization. Ultrasound imaging is very popular because of its low-cost instrumentation and easy application. However, an ultrasound exam requires the presence of an experienced operator to adjust various parameters for optimum contrast, and ultrasound images usually require an experienced radiologist to interpret the image.

## 6.1 Sound Propagation in Biological Tissue

Historically, ultrasound imaging emerged shortly after World War II from sonar (sound navigation and ranging) with somewhat similar technology. Sonar makes use of the excellent sound propagation properties of water, whereby a short acoustic pulse is introduced into the water. Any object in the path of the sound pulse causes a reflected echo, which can be picked up. Sound travels in water at approximately 1500 m/s, and the round-trip time of the echo gives an indication of the distance of the echogenic object. The first documented medical application was in 1942 [31], but major improvements of the instrumentation are likely attributable to Donald and MacVicar in the 1950s [32, 33]. At this time, ultrasound imaging was purely based on analog electronics, and the image was visualized on a CRT screen. As such, ultrasound imaging is the only other imaging modality (apart from x-ray imaging) that does not require computerized data processing for image formation.

Fig. 6.1

Propagation of a compressive sound wave in elastic tissue. The sound wave is introduced by a piezoelectric element (transducer), and the wave propagates inside the tissue (wide arrow) with the speed of sound  , which is a tissue constant. The wavelength   depends on the frequency   as

Like sonar, ultrasound imaging is based on the introduction of a compressive wave into the tissue (Fig. 6.1). Even the earliest instruments made use of the piezoelectric effect to generate the compressive wave. Wave propagation can be described by the partial differential equation

(6.1)

where   is the relative deviation of the tissue density   from its normal (uncompressed) density  ,   is the direction of sound propagation, and   is the speed of sound. The one-dimensional nature of the sound wave is an approximation. Tissue viscoelasticity and diffraction broaden the sound beam, but the one-dimensional treatment is sufficient in the context of this chapter to examine the sources of contrast and methods of image formation. The solution of the one-dimensional wave equation is the complex harmonic oscillation

(6.2)

where   is the wavelength (Fig. 6.1) and   is the frequency of the sound wave. A compressive wave implies the presence of a corresponding pressure, and the pressure depends on the amount of compression   and the bulk modulus   through  . The bulk modulus is inversely proportional to the compressibility of a material and can be seen as the material stiffness or resistance against deformation. For low ultrasound energies, linear elasticity is assumed (i.e., Hooke's law).

Two important tissue constants are the speed of sound   and the acoustic impedance  , which depend on   and   through

(6.3)

The acoustic impedance can best be explained as the ability of the tissue to conduct sound. In analogy to a voltage, which causes a current to flow, the sound pressure causes a local motion of the infinitesimal tissue elements with a velocity  . The velocity of a local particle is not the same as the speed of sound, but the two are related through  . To continue the analogy, Ohm's law relates voltage (pressure) to current (local velocity), and we find that  . The same analogy leads to the definition of the power of the sound field as  . Power is dissipated as the sound wave travels through tissue, and the power decreases from the incident power   with traveled distance   according to Lambert-Beer's law,

(6.4)

where the absorption coefficient   depends on the tissue and the ultrasound frequency. It is practical to express sound attenuation   in decibels and rewrite Eq. 6.4 as

(6.5)

where   is the attenuation coefficient at 1 MHz,   is the frequency, and   the distance traveled. As a rough approximation, sound power is decreased by 1 dB per cm and MHz in soft tissue. Some representative values of the attenuation coefficient, speed of sound, and the acoustic impedance are found in Table 6.1. The sound power lost by tissue attenuation is converted into heat, and at very higher power levels causes direct shear damage. Diagnostic ultrasound typically introduces less than 0.1 W/cm , and this level is thought to cause no harmful bioeffects. At higher power levels, the total exposure time would be limited to keep harmful effects to a minimum.

Table 6.1

Some representative material and tissue constants for sound wave propagation

Material | Speed of sound | Acoustic impedance | Sound attenuation | Half-value layer

---|---|---|---|---  
|

  (m/s) |   (kg/m  s) |   (dB/MHz cm) | at 5 MHz (cm)

Air | 330 | 430 | - | -

Water | 1492 |   |   0 | -

Adipose tissue | 1470 |   | 0.5 | 2.4

Liver | 1540 |   | 0.7 | 1.7

Muscle | 1568 |   | 2.0 | 0.6

Brain tissue | 1530 |   | 1.0 | 1.2

Compact bone | 3600 |   | 10 or more | 0.12

PZT | 4000 |

|  |

PZT is a commonly used transducer material and is discussed in the next section

Fig. 6.2

Ultrasound propagation in inhomogeneous materials. a At a smooth interface, the sound wave is split into a reflected and a refracted component. The angle of incidence   with respect to the surface normal N is the same as the angle of reflection  . The angle   of the refracted wave obeys Snell's law. b An irregular interface causes the reflected part to spread out in a diffuse pattern

Ultrasound waves, like light waves, undergo reflection and refraction at interfaces with different acoustic impedance. At a smooth surface, the sound wave is split into a reflected and a refracted component that obey the law of reflection and Snell's law, respectively (Fig. 6.2):

(6.6)

The acoustic power   of the incident wave is split between the reflected and refracted part, and expressions for the reflectivity   and transmittivity   can be derived:

(6.7)

In biological tissue, the occurrence of an irregular surface is more likely that causes diffuse reflection (Fig. 6.2). In addition, inhomogeneities that are smaller than the wavelength of the sound wave cause Rayleigh scattering. Reflected or scattered sound waves that return to the transducer can be recorded as an echo and used for image formation. However, the energy loss through wave reflection is even more significant than energy loss through regular tissue attenuation, and the echo signal can be very weak. To get an idea of the amount of energy loss, let us assume an incident sound wave normal to a muscle-fat interface. By using the values from Table 6.1 and the reflectivity in Eq. 6.7, we can calculate that only about 0.1 % of the incident power is reflected back at the transducer.

Another similarity between light waves and sound waves is the beam expansion by diffraction. Without covering the theory of sound propagation in three dimensions in detail, the beam expands approximately linear with the traveled distance. The beam diameter   can be described by the Fraunhofer approximation,

(6.8)

where   is the diameter of the transducer, which is assumed to be circular. The region where the Fraunhofer approximation holds begins at a distance of approximately   from the transducer and is called the far-field region. At the opposite end, when  , the sound pressure pattern becomes very complex. The beam diameter converges and forms a waist, and interference causes the formation of side lobes. This region is called the near-field and is generally avoided for ultrasound echo generation. For a general approximation, it is sufficient to assume a constant near-field beam diameter  .

## 6.2 Ultrasound Image Formation

### 6.2.1 Ultrasound Generation and Echo Detection

The key element in ultrasound imaging is the sound transducer. The transducer is made of a piezoelectric material. Most frequently, lead zirconate titanate (PZT) is used, which is a ceramic combination of PbZrO  and PbTiO  molecules. Piezoelectric materials consist of strong dipoles that change the shape of the crystalline structure when exposed to an electrostatic field. Conversely, when the crystalline structure is exposed to mechanical stress, the crystal exhibits an electrostatic potential across its sides [34, 35]. Therefore, the transducer can serve both as pulse generator and microphone for the sound echoes. Typical ultrasound scanners use several hundred volts to create a surface displacement in the  m range. A retuning echo causes a potential in the nanovolt to microvolt range.

Transducers are operated in resonance, with part of the wave energy reflected back into the PZT material. A standing wave forms at the fundamental resonant frequency  ,

(6.9)

where   is the speed of sound in PZT and   is the thickness of the PZT layer. The resonance frequency is therefore determined by the thickness of the PZT transducer element. A 5 MHz transducer, for example, can be obtained with a 0.8 mm PZT layer. The resonant oscillation is excited with a short voltage pulse of microsecond duration.

Fig. 6.3

Sketch of a simple single-element ultrasound transducer. A thin layer of PZT is the active element. Toward the tissue, an impedance-matching layer is responsible for minimizing the loss between the high-impedance PZT and the relatively low-impedance tissue. A backing material ensures that the wave is emitted through the acoustical window, and the damper attenuates the resonant vibrations and therefore controls the pulse duration

Fig. 6.4

Photo of a single-point dual-focus transducer element. Shown is the front view (a) and back view (b) of a movable element that carries two transducers. Both transducers are slightly curved to produce a converging sound wave, and the smaller transducer has a higher curvature and therefore a focal point nearer to the transducer

A sketch of a single-element ultrasound transducer is shown in Fig. 6.3. The PZT layer, together with the backing material and the impedance-matching layer, forms the actual transducer element. Absorber material (i.e., the acoustic damper) in the ultrasound probe determines the attenuation of the resonant oscillation and therefore the duration of the ultrasound pulse. Furthermore, the attenuation determines the bandwidth of the pulse. Strong attenuation causes a broader bandwidth than weaker attenuation, and consequently a shorter pulse. A broad bandwidth (short pulse) increases the axial resolution at the expense of SNR. A practical example of a transducer element is shown in Fig. 6.4.

At its acoustical window, an impedance-matching layer is placed between the PZT crystal and the tissue. For single-layer designs, the impedance-matching layer is a polymer with the approximate acoustic impedance  , where   is the acoustic impedance of PZT and   that of tissue. The additional application of ultrasound gel is necessary to remove any air between the transducer and the tissue. The acoustic impedance of air is extremely low (Table 6.1), and more than 99.9 % of the sound wave would be reflected at any tissue-air interface.

The resonance frequency of the transducer depends on the clinical application. The wavelength of the sound wave provides the theoretical limits for the axial resolution ( ) and also for the lateral resolution through the numerical aperture, where under ideal conditions ( ) the lateral resolution  . Higher frequencies therefore provide better spatial resolution, but the attenuation coefficient also increases, and maximum depth is reduced. Table 6.2 lists commonly used ultrasound frequencies with their approximate depth and resolution limits, and some applications.

Table 6.2

Ultrasound frequency, depth and resolution limits, and typical clinical applications

Ultrasound | Maximum | Axial | Lateral | Typical

---|---|---|---|---

frequency | depth | resolution | resolution | application

(MHz) | (mm) |   (mm) |   (mm)

|

3 | 150 | 0.6 | 2.0 | General purpose;

|  |  |  |

fetus, heart, liver

5 | 100 | 0.35 | 1.2 | Kidney, heart, brain

10 | 50 | 0.2 | 0.6 | Muscle, tendons,

|  |  |  |

endoscopic applications (prostate)

15 | 33 | 0.15 | 0.4 | Intraoperative applications

|  |  |  |

blood vessels

  |   |   |   | Research applications;

|  |  |  |

vasculature, skin

Fig. 6.5

Schematic representation of an instrument to generate an A-mode scan in pulse echo operation. A master timing generator (gate generator) repeatedly switches the transducer to transmit mode and applies a pulse waveform through a pulse generator, thus causing a short sound pulse to travel into the tissue. The transducer is immediately put into receive mode, and echoes are recoded and plotted along the time axis. By multiplying the time axis with  , it can be scaled to provide the approximate depth  . A ramp generator provides a signal that is proportional to  . The echo amplitude   is corrected for sound attenuation by increasing the gain exponentially with time (time gain control). In this example, two echogenic objects   and   are sketched at depths   and  , respectively, and their echoes are recorded as the amplitude   over the time axis

### 6.2.2 A-Mode Scans

The simplest form of an ultrasound scan is to emit a short sound pulse and wait for the arrival of an echo. The round-trip travel time is converted into the depth   of the echogenic object, and the echo amplitude is drawn in a two-dimensional coordinate system over the depth  . This scan is referred to as A-mode scan or amplitude-mode scan. The strongest echoes are generated along the beam path, and the beam diameter determines the lateral resolution. A schematic representation of an apparatus to generate a sound burst and record the echoes is shown in Fig. 6.5. Typically, the same transducer is used for sound wave generation and for recording the reflected waves. A master timing generator is responsible for switching the transducer between transmit and receive modes, for causing a signal generator to generate the sound signal, and for starting the time-resolved echo acquisition. This mode of operation is also referred to as pulse echo acquisition, because the transmitter is used only for short periods of time to emit a sound pulse. The round-trip time   of an echo is directly related to its depth   through  . The ultrasound scanner cannot know what type of tissue is traveled by the sound wave, and some representative value of  , usually 1540 m/s, is assumed when quantitative measurements are made.

The master timing generator operates with a pulse repetition frequency of typically 2-4 kHz, and the pulse repetition frequency determines the maximum depth of the scan. For an average speed of sound   m/s, a 4 kHz pulse repetition frequency translates into a maximum round-trip time of 250  s and a maximum depth of 19 cm. With increasing depth, the sound wave is attenuated. The receive amplifier therefore increases its gain exponentially with increasing time, and the gain is reset upon the next pulse. This correction is called time gain control. If we use the rule-of-thumb attenuation of 1 dB per cm and MHz and a 5 MHz probe, the time gain control unit would add 10 dB gain at 2 cm depth (26  s round-trip time), and 50 dB at 10 cm (130  s round-trip time).

In multidimensional scans (B-mode and M-mode scans), the pulse repetition frequency determines the total image acquisition time. However, very high pulse repetition frequencies can lead to aliasing when an echo of a strongly echogenic object arrives later than the next pulse is emitted. For example, the echo from a reflecting interface at 12 cm depth arrives after approximately 156  s. The pulse repetition time for an 8 kHz pulse repetition frequency is 125  s, and the echo would arrive 31  s after the subsequent pulse. The ultrasound scanner would draw this echo as a weak pulse from an object approximately 2.4 cm deep.

Fig. 6.6

Focused ultrasound source mounted on a mechanical pivot to provide a B-mode scan. a Sketch of the scan head. The transducer   emits a focused beam through the acoustical window  . The transducer assembly is mounted on a pivot and driven by a motor  , thus allowing to position the beam along a fan-shaped plane. The entire assembly is immersed in index-matching oil that allows ultrasound transmission through the acoustic window. b Image of a scan head that is built after this principle. From the acoustic window, the sound is guided along a soft silicone pad with the surface that actually comes in contact with the tissue. If the silicone pad is removed (c), the oil-immersed transducer and its mounting mechanism become visible. The transducer element itself is similar to the one shown in Fig. 6.4

### 6.2.3 B-Mode Scans

The amplitude of an A-mode scan can be encoded as intensity (brightness) in one line of an image. Lateral movement of the transducer would then provide the second dimension, and a two-dimensional image is presented. This type of scan is termed B-mode scan for its brightness encoding. In many scanner designs, the transducer movement is indeed mechanical, as shown in Fig. 6.6. Two dominant designs exist, the first where the head performs a wobbling motion (Fig. 6.6), and the second, where multiple transducers are arranged on the outer surface of a rotating cylinder.

Fig. 6.7

Linear transducer array for high-resolution ultrasound imaging. a The matching layer covers a large number of individually controllable transducer elements (indicated as white circles, which are not to scale). Each element sends out an ultrasound wave with its respective lateral position  . By pulsing the elements in sequence, the individual A-mode scans can be arranged into a two-dimensional image. b The B-mode image shows a cross-section through a cotton boll immersed in water

In modern systems, mechanical actuation has been replaced by phased arrays of transducers. A phased array refers to a large number of small transducers, usually arranged in a linear fashion, that can be driven independently. In its simplest form, we can envision the transducer array as a large number of spot transducers arranged in a line as shown in Fig. 6.7. For example, the array could consist of 128 transducer elements spaced about 0.5 mm apart. Each transducer emits a wave normal to the array at its respective position, and pulsing the transducers from left to right provides echoes along parallel lines (Fig. 6.7b) to cover a slice of 64 mm width. Although individual elements are pulsed for sound wave generation, the elements are usually combined (that is, their signals added up) for echo reception. By combining multiple transducers for echo reception, the probability of capturing an off-axis maximum is increased, and the overall signal-to-noise ratio improved.

The image represents the echogenic strength of the tissue along a rectangular slice that extends in the  -direction into the tissue. The radiologist can reposition the probe or change the angle of incidence to cover different planes in the tissue. The need for continuous manual interaction is one of the most fundamental differences between ultrasound imaging and other imaging modalities.

The example of the linear array is one possible application of phased arrays. Another important aspect is the ability of phased arrays to shape and direct the sound beam. The idea is to generate a curved sound front. The transducer elements are closely spaced, and are considered to generate a spherical wave. For example, a larger curved transducer (such as the point transducer in Fig. 6.4) can be replaced by a phased array where the outer transducers are pulsed earlier than the inner ones (Fig. 6.8a). More precisely, a spherical wave front can be obtained with a delay   between the beginning of the pulse at   and the pulsing of an element at position  ,

(6.10)

where   is the overall length of the linear array and   is the position of the focus along the  -axis. With the appropriate asymmetrical time delays, the center of the spherical sound front can be shifted away from the  -axis, thus allowing the resulting sound front to enter the tissue at an angle   with respect to the  -axis (Fig. 6.8b). This angle can be changed between each A-mode scan, and the resulting echoes are collected from a wedge-shaped plane. In a similar fashion, a receive focus can be achieved by applying a short position-dependent delay to the echo signals of each transducer element before the summation of their signals.

Fig. 6.8

Illustration of the beamforming capabilities of a phased array. a Focus generation. By pulsing the outer transducer elements earlier than the center elements, a curved sound front is achieved that converges towards a focus. The location of the focus can be determined by the phase difference between the pulsed elements. b Beam steering. Asymmetric pulsing of the transducer elements leads to a sound front that is not symmetrical and therefore travels at an angle with respect to the   axis

### 6.2.4 M-Mode Scans

The M-mode scan (M stands for motion) is achieved by arranging intensity-encoded A-mode lines as a function of time. Each line of the M-mode scan therefore represents the depth of an echogenic object along the  -axis, and its motion along the  -axis can be imaged. M-mode scans are often obtained to observe the motion of the heart valves, and the evolution of the M-mode scan along the time axis can be correlated to the ECG of the patient.

### 6.2.5 Volumetric Scans and 3D Ultrasound

Ultrasound scans can be extended into three dimensions by mechanical actuation of the transducer along a third axis or with two-dimensional phased arrays that allow beam steering in two dimensions. This technique has become very popular for diagnosing fetal congenital diseases [36, 37]. The amniotic fluid has a very low echogenicity and is followed by a strong echo from the fetus. Image processing and surface reconstruction allow a very photo-like rendering of the fetus.

Often, reference is made to 4D ultrasound [38] . The fourth dimension is time, and 4D ultrasound imaging refers to an acquisition process fast enough to capture motion. When we consider a typical 4 kHz pulse repetition rate, a B-mode scan with 512 A-mode lines has a frame rate of 8s . If a third dimension is added, for example, with 128 B-mode scans, the 3D image acquisition time is approximately 16 s. This time is too long to allow the usual manual operation of the scanner. Initially, image acquisition time was reduced by reducing the number of pulsed elements [39], with consequently reduced resolution and image quality. With narrower angular coverage and faster transducers, significant speed gains can be realized without sacrificing image quality [40]. New approaches, such as single-wave coherent imaging and shear wave imaging promise even faster imaging speeds [41].

## 6.3 Doppler Ultrasound

One of the most powerful features of ultrasound imaging is the ability to directly measure blood flow by exploiting the Doppler effect. The frequency of a sound wave reflected by a moving object differs from the incident frequency by the Doppler frequency  ,

(6.11)

where   is the frequency of the incident wave,   the speed of sound,   the speed of the moving particle that creates the echo (such as a blood cell), and   the angle between the reflected sound wave and the path of the particle. The Doppler frequency can either be positive (the particle moves toward the source, and  ) or negative (the particle moves away from the source, and  ). Equation 6.11 is an approximation with the assumption that  . It is possible to use a two-transducer system to continuously transmit the ultrasound wave and receive the echo. Demodulation, that is, multiplying the echo signal with the carrier frequency and subsequent low-pass filtering, provides an oscillation with the Doppler frequency   that can be made audible or displayed as a frequency spectrum. If we assume, for example, a 3.5 MHz probe and blood velocity of 15 cm/s with the probe at a 45angle to the direction of blood flow, the Doppler signal would have a frequency of roughly 500 Hz (Eq. 6.11). This frequency is well within the audible range.

More commonly used is the pulsed Doppler operation, because the Doppler velocity measurement can be combined with the ability of a regular pulsed-operation ultrasound system to provide a spatially resolved signal. Devices operating in pulsed Doppler mode exhibit three differences from regular B-mode scanners:

  * The acoustic damper in the transducer element is usually weaker, creating a more sustained wave. With the longer transmit pulse, some axial resolution is sacrificed, but a better signal detection is possible

  * The maximum A-mode depth is reduced and the pulse repetition frequency raised. The reason for choosing a higher pulse repetition frequency is discussed below.

  * The signal used for Doppler detection is gated, that is, only signals arriving during a short time window between two subsequent pulses are used for Doppler detection. The time window corresponds to a depth section and is selectable by the operator.

In pulsed Doppler operation, the phase shift between incident wave and echo is determined. The phase shift depends both on the travel time of the echo (i.e., the depth of the object) and the velocity. With short pulse repetition times, the object can be assumed to be stationary. A small blood volume traveling at 15 cm/s, for example, moves only 37  m between two pulses at a pulse repetition frequency of 4 kHz. This is a fraction of the lateral resolution. If the flow velocity is assumed to be constant in the brief period between pulse and echo, the Doppler frequency   can be recovered from the measured phase shift   through  . With suitable quadrature demodulators, the sign of   can be recovered, allowing to determine the flow direction, and the quadrature signal can be analyzed by computing the Fourier spectrum or by correlating signals of two subsequent pulse echoes. Since the echo phase is sampled only once per pulse repetition period, the resulting velocity signal is time-discrete, and the maximum Doppler frequency that can be recovered is exactly one half of the pulse repetition frequency (Nyquist limit). By using this relationship between   and the pulse repetition frequency  , the maximum flow velocity can be computed with Eq. 6.11:

(6.12)

Any higher flow velocity is subject to aliasing, that is, the phase detector incorrectly indicates a much lower velocity. A higher pulse repetition frequency (and consequently a lower scan depth) allows to measure proportionally higher velocities. The incident angle is also critical in Doppler ultrasound measurements. For example, a probe held at a 45angle can cause an error of  6 % when the probe angle is varied as little as  3. The error rises disproportionately with higher incidence angles. Those are undesirable, though, because the Doppler frequency is lowered. However, sometimes the radiologist needs to find a compromise between a high incidence angle that is needed to access certain blood vessels and measurement accuracy.

Since the results of the Doppler measurement are spatially resolved, flow velocity can be superimposed over the B-mode scan. This mode is sometimes termed duplex ultrasound scanning. Velocity is color coded and superimposed over the grayscale B-mode scan. By convention, flow in the direction of the scan head is colored in red shades, and flow in the opposite direction in blue shades. Pathological flow, for example a reversed flow direction in a bifurcation or stenosis, becomes immediately evident, even if the absolute flow velocity is subject to measurement error. 
Mark A HaidekkerSpringerBriefs in PhysicsMedical Imaging Technology201310.1007/978-1-4614-7073-1_7(C) The Author(s) 2013

# 7. Trends in Medical Imaging Technology

Mark A. Haidekker1

(1)

College of Engineering, University of Georgia, 597 D.W. Brooks Drive, Driftmier Engineering Center, Athens, 30602, USA

Mark A. Haidekker

Email: mhaidekk@uga.edu

Abstract

Medical imaging technologies have, to a varying extent, experienced significant recent progress. From the introduction of a new imaging modality to its adoption to routine clinical practice, many years of development and testing are needed. Recently, the focus of medical imaging research shifted toward detail optimization and the development of new disease-specific protocols, although several striking new developments need to be highlighted. For example, the adaptation of phase and darkfield contrast, well-known from microscope imaging, to X-ray imaging, provides a new and astonishing level of contrast in X-ray imaging and CT. Another example is the development of new, ultra-portable ultrasound devices, which make ultrasound even more attractive as a fast and low-cost imaging modality. Laser-based optical imaging that uses visible or near-infrared light deserves special attention as some methods have been adopted in clinical practice. Last but not least, improved image processing--both in terms of new algorithms and of improved computing power--have continually improved image quality and opened new avenues of image processing, with many new functions available to aid the radiologist in providing a diagnosis.

The enormous technological progress seen in the last decades of the twentieth century gave rise to what could be called pioneering days of tomographic modalities. Invention of the transistor, the integrated circuit, and eventually the microprocessor were prerequisites for the development of modern imaging methods. During the same period of time, many of the fundamental image processing algorithms were introduced, such as digital filters or automated segmentation. Medical imaging technology benefited from these developments, and in the course of about two decades, the image quality delivered by CT, MRI, and ultrasound devices increased in great strides.

In recent years, progress has become more detail-oriented, with major efforts dedicated to optimizing imaging protocols for specific organs or diseases. Two examples can serve to illustrate this trend:

  * Spiral CT (introduced in 1990 [42]): Conventional CT scanners complete one revolution to acquire a slice before advancing to the next axial slice. Spiral (or helical) CT differs in that the patient table is advanced during the rotation of the source-detector gantry. The reconstruction maintains the same slice by interpolating between two successive rotations. The main advantage is that slices at arbitrary axial positions, even overlapping slices, can be reconstructed from the same raw data set. The development of the helical scanning principle was accompanied with improved cooling of the X-ray tube and improved detector efficiency, as well as improved gantry mechanics (the slip-ring gantry), which overall allowed one revolution of the gantry to be completed in one second. With such fast acquisition rates, motion artifacts were reduced, and complete volumetric scans could be completed in one breath hold.

  * Open MRI (introduced in 1992 [43]): The open MRI scanner uses a C-shaped magnet with the primary   field following a vertical orientation. In comparison, conventional MR scanners use a toroidal coil with a horizontal field orientation. The main advantage of the open MRI geometry is easier access to the patient during imaging, which facilitates, for example, interventional imaging. Open MRI enjoys additional popularity, because it does not expose the patient to the somewhat claustrophobic environment of the conventional magnet bore.

In the following sections, some recent developments are highlighted. In short, the trend of improving existing modalities continues. Some of the progress is made by combining multiple imaging modalities, and by obtaining functional information with slightly altered imaging protocols. Optical imaging, that is, imaging with visible or near-infrared light, is a relatively young modality that is still in the development stages. In all cases, advances are being helped by progress in computerized image processing and image understanding.

## 7.1 Progress in Established Imaging Modalities

### 7.1.1 X-ray and CT

In recent years, X-ray imaging has reached a technological plateau. The trend away from film and toward digital X-ray imaging continues. Improved detectors with higher sensitivity allow to further reduce exposure time and the patient radiation dose. After concerns were raised that increased use of X-rays in diagnostic and interventional procedures could lead to elevated cancer risk (see, e.g., [44]), a shift away from X-ray imaging toward ultrasound and MRI has been observed, leading to a further reduction of the radiation exposure in patients. Computed tomography, however, remains an attractive modality because of its very high per-slice acquisition rates, notably with the development of dual-source CT scanners [45]. Modern dual-source CT scanners are capable of 0.3 s or less per rotation with an axial speed of 0.4 m/s. With such high acquisition speeds, motion artifacts cease to be a concern. In addition, the heart can be scanned in 3D during one heart beat. Lastly, modern CT scanners give rise to sub-mSv scans (i.e., scans with a total exposure of less than 1 mSv, which approaches the level of a typical chest X-ray of 0.1 mSv). Today, radiation exposure from CT scans is less of a concern than 20 years ago.

Transmission-based X-ray imaging has recently been complemented by phase-contrast and darkfield methods that are known from light microscopy. Phase contrast microscopy makes use of wave interference: The illumination light wave is split in two parts, a reference wave and a wave that experiences a phase delay in the object (in media of different refractive index, the apparent path length is changed). In a similar way, X-rays experience a phase change in weakly absorbing materials [46]. With suitable diffraction gratings, the phase (more precisely, its first derivative perpendicular to the grating slits) can be converted into intensity and thus recorded by the detector [47]. The same principle can be used to record scattered X-rays, leading to the X-ray analog of darkfield imaging [48].

Phase contrast X-ray imaging provides the projection of the refractive index along the beam path, analogous to conventional X-ray imaging that provides the projection of the absorber density. Therefore, phase contrast-enhanced radiography advertises itself for CT reconstruction methods [49, 50]. These methods promise not only markedly enhanced perception of contrast, but actually a different type of information retrieved from the scanned object, namely, its refractive index. Particularly in conjunction with CT reconstruction methods, tissue-tissue contrast could be dramatically enhanced, thus eliminating one weakness of X-ray based CT imaging. However, the method is still under development, and it will probably take several years before phase contrast CT can be found in medical diagnostic centers.

### 7.1.2 Magnetic Resonance Imaging

Magnetic resonance imaging experiences progress from the use of stronger magnets and improved amplifiers. Both lead to higher spatial resolution and improved SNR, or, with constant SNR, to shorter acquisition times [51]. A significant milestone was the introduction of a blood-oxygen level dependent (BOLD) sequence [52]. The BOLD sequence makes use of differences in   relaxation times between oxyhemoglobin and deoxyhemoglobin and allows to measure blood flow and blood oxygenation levels. This technique has given rise to functional MRI. Although functional MRI enjoys most of its popularity in studies to localize brain activity, clinical applications exist, including Alzheimer's disease and the measurement of coronary blood flow. Due to its low SNR and lower spatial resolution, BOLD functional images are often superimposed over structural MRI images, similar to PET and SPECT images.

Diffusion tensor imaging is another method to take MRI in the direction of functional imaging. The Bloch equation (Eq. 5.15) assumes stationary protons; an additional term that depends on diffusion modulates the change of magnetization  . Diffusion tensor imaging allows to recover the three-dimensional vector field of water diffusion [53]. Clinical applications include brain imaging in dementia, including Alzheimer's disease [54].

Another "abnormal" MR imaging technique makes use of reduced   as a consequence of microscopic susceptibility changes caused by bone [55]. Normally, MR is not widely popular for bone imaging due to the low proton density and the short   and   relaxation times in bone. Special sequences, such as FLASE (fast, low-angle spin echo), make use of   contrast in bones, while achieving voxel sizes between 0.1 and 0.2 mm [56]. Although bone strength assessment widely relies on ultrasound (for a fast, less accurate measurement) and X-ray imaging (for a higher-accuracy assessment), micro-MRI imaging of bone promises to evolve into one pillar of trabecular structure assessment [57]. These examples highlight the present trend in MRI to extract more information from the tissue with the same underlying physical principle, but with the sophisticated application of new sequences.

### 7.1.3 Ultrasound Imaging

Ultrasound imaging technology has also reached a certain plateau. Ultrasound imaging remains the modality of choice for rapid, low-cost diagnostic procedures without ionizing radiation. The size of ultrasound scanners has been reduced dramatically over the last two decades, and recently hand-held ultrasound scanners with full diagnostic capabilities were introduced [58]. It can be expected that the popularity of ultrasound imaging will further increase with the spread of ultra-portable devices.

Ultrasound contrast agents have been introduced that consist of gas-filled microbubbles. These bubbles literally burst in the incident sound field and create a strong signal. These microbubbles can be functionalized to bind to specific sites, such as tumors or inflammatory processes [59]. With such contrast agents, ultrasound, too, takes the step toward functional imaging, with the additional potential for targeted drug delivery [60] as the microbubbles can be loaded with drugs, set to burst at the insonicated target site.

### 7.1.4 PET and Multi-Modality Imaging

PET offers one potential improvement that SPECT cannot offer: more precise localization of a decay event with time-of-flight measurements, and a resulting improvement of SNR. Ultrafast electronics and fast-decay scintillation crystals are a prerequisite. For example, to achieve a time-of-flight resolution of 45 mm, the detector needs a resolution of 150 ps, which is technologically feasible [61]. Moreover, experiments with semiconductor detectors are under way. The proposed detector element is an avalanche photodiode, which has a much higher sensitivity than CMOS or CCD elements, but a lower sensitivity than a PMT. Avalanche photodiodes are much smaller than PMTs and promise dramatically improved spatial resolution.

Combined SPECT/CT devices and PET/CT devices have been introduced more than 10 years ago [62], a technology that advertises itself, because the detectors for gamma and X-ray radiation are similar. These scanners remove the need for image fusion related to moving a patient between different scanners. Technologically more challenging is the combination of PET with MRI, and the first PET/MRI multimodality scanners became available only in the last few years [63]. Once again, the development of semiconductor detectors was crucial, because the strong magnetic field of the MRI device makes the use of PMTs impractical.

### 7.1.5 Molecular Imaging

In the context of radionuclide imaging and multimodality imaging, brief mention of molecular imaging is appropriate. Molecular imaging can be defined as imaging of functional (i.e., physiological) processes at the cellular and subcellular scale. Traditionally, fluorescent markers in conjunction with light microscopy were used for research at this level. The concept of molecular imaging can be extended to high-resolution functional imaging with tomographic methods. SPECT and PET can be used with radiolabeled antibodies or other markers that specifically interact with targeted proteins (see, for example [64] for a review of molecular imaging in cardiovascular applications). Another emerging field links molecular imaging with stem cells: Stem cells are loaded with paramagnetic nanoparticles, which allows them to be traced with MRI [65].

In Sect. 7.1.3, we briefly introduced functionalized microbubbles for functional ultrasound imaging. Another related imaging technique is photoacoustic imaging and photoacoustic tomography [66]. In the case of photoacoustic imaging, the sound wave is generated inside the tissue by absorption of a high-energy pulse of light. Absorption can take place either in intrinsic chromophores (e.g., hemoglobin) or in externally supplied dyes. B-mode scan techniques can be used for spatially resolving the sound source, or array detectors allow tomography-like reconstructions. One of the limits of photoacoustic imaging is light scattering, which limits the depth of the incident light pulse.

## 7.2 Optical Tomography

Optical tomographic imaging modalities are those that use visible or near-infrared light for image formation. They deserve a separate section, because optical imaging currently finds its way into medical practice. Optical imaging is attractive, because it does not use ionizing radiation, has short acquisition times, offers a spatial resolution much higher than ultrasound imaging, and potentially can be achieved with low-cost instrumentation.

Visible or near-infrared light experiences strong scattering in biological tissue, and common optical imaging modalities are limited to a depth of few millimeters. With the exception of optical transillumination tomography, optical imaging cannot rely on unscattered photons, and reconstruction methods such as those in CT cannot be used.

Optical coherence tomography (OCT) can be considered the optical equivalent of ultrasound. The light source is a special type of laser with an unusually broad bandwidth. Normally, lasers are considered to be monochromatic, that is, their emission spectrum is very narrow. Conversely, OCT lasers (known as superluminescent diodes, SLD) have a broad bandwidth, more precisely, a Gaussian wavelength distribution with a bandwidth   of 20-100 nm. Broadband light loses its coherent properties rapidly, and a superluminescent diode has an approximate coherent length of  .

Optical coherence tomography instrumentation is based on the Michelson interferometer. The sketch of a basic OCT system shown in Fig. 7.1 is based on free-space optics, although OCT devices are normally based on fiber optics. In either case, the light from the SLD is split into a sample and a reference path. Light is scattered back from the tissue and recombined with the reflected reference beam. Only light scattered from the tissue section where the reference and sample beams have the same length are recorded by the photodetector. By moving the reference mirror, the depth of the detected light is changed. Moving the reference mirror therefore produces a scan of the scattered light amplitude  , which is the optical equivalent of an ultrasound A-mode scan.

Fig. 7.1

Schematic of an optical coherence tomography device. The SLD emits a collimated beam of light with short coherent length. A beamsplitter (i.e., a semi-transparent mirror) splits the beam into the sample path   and the reference path  . Scattered light from the tissue can only be recorded at a depth where the reference arm and sample arm of the beam have the same length. Scattered light is sent back to the beamsplitter, where it is recombined with the reflected reference beam and directed onto a photodiode (PD). Moving the reference mirror ( ) moves the position of the coherent signal and produces the optical equivalent of an A-mode scan

Due to the short coherent length of the SLD, the axial resolution is high, often in the range of 5-15 m. Depending on the focusing optics (not shown in Fig. 7.1), similar resolution can be achieved in the lateral direction. A scan mirror in the sample beam path can deflect the beam and sweep it much like an ultrasound beam (cf. Fig. 6.6) to produce a B-mode scan. The source of contrast is the amount of light scattered by the tissue back along the sample path.

Even more elegant is Fourier-domain OCT. It can be shown that the wavelength of the light (more precisely, its frequency) contains the Fourier-encoded depth information. In other words, frequency   and depth   are related through the Fourier transform. To make use of this principle, the reference arm in Fig. 7.1 is movable only for coarse depth adjustment, and the detector is replaced by a spectrometer. As a consequence, the scattered signal   is resolved by the frequency. Inverse Fourier transform of   yields the scattered intensity  . The advantage of Fourier-domain OCT is its ability to obtain a complete A-mode scan in one measurement. Its main disadvantage is the lower SNR that is a consequence of the scattered light being distributed over many detectors in the spectrometer.

OCT has found its way into clinical practice, primarily used by ophthalmologists to examine the retina [67] and less frequently the cornea [68]. In dermatology, OCT scans help diagnose skin cancer and inflammatory processes [69]. The flexibility of fiber optics also allows OCT to be used in the gastrointestinal tract [70].

Diffuse optical tomography (DOT) uses scattered photons to reconstruct the scattering coefficient in the sample. A typical DOT imaging device uses a cylindrical sample holder filled with index-matching fluid. Along two closely-spaced circles are arrayed a ring of point sources (optical fibers connected to a laser) and point detectors (optical fibers connected to photodetectors). For a known medium and known geometry, the wave propagation equation can be solved, and the intensity at the detectors predicted. However, unlike tomographic methods that are based on straight-line geometries (CT), no closed-form solution for the inverse problem exists, and iterative methods need to be employed to reconstruct the geometry from measured light intensities [71]. The method can be modified to measure the local concentration of fluorescent emitters [72]. With fluorescently labeled drugs or physiologically active compounds, DOT holds the promise to image physiological processes analogous to SPECT and PET. The main disadvantage of the method is its poor spatial resolution. A commercial DOT-based screening device for breast cancer has been introduced recently, and DOT is in the process of being adapted for clinical practice [73, 74].

Optical transillumination tomography is the optical equivalent to X-ray CT. The single most difficult challenge for optical transillumination tomography is any form of refractive index change along the path, which invalidates the straight-line assumption of the Radon transform. In addition, the strong scattering properties of tissue require a very high dynamic range photodetector. Optical transillumination tomography has been proposed for several decades, but has not yet found applications in clinical practice [75]. Some attempts have been made to correct for refractive index mismatch [76], but those were limited to well-defined geometries, such as tissue-engineered blood vessels. However, in this context, optical transillumination tomography can offer unusually high acquisition speed [77]. A different approach is to use in vitro preparations that reduce the index of refraction mismatch with the application of special chemicals, and at the same time reduce the scattering coefficient [78]. This method, also known as optical projection tomography, shows very promising results [79]. However, as an in vivo imaging method, major obstacles need to be overcome before this imaging modality reaches practicability.

## 7.3 Advanced Image Processing

At the conclusion of this chapter, it is also necessary to mention trends and progress in computerized image processing. In the previous chapters, we have seen to what extent image formation depends on the ability of computers to collect and transform data. New image processing methods and higher computing power both work together to provide improved image quality. Most of the backend image processing takes place after the image has been generated: image enhancement, detection (segmentation) of objects of interest, or the measurement of, for example, density or size of a feature of interest. However, as part of the image formation process, improved algorithms play an important role more "behind the curtains". One example was introduced in Chap. , where the deconvolution kernel in the filtered backprojection determines the balance between detail and artifactual texture.

Several more examples are introduced below to serve as illustration how progress in computerized image processing influences the entire image formation chain.

After its discovery, the wavelet transform has rapidly found its place in image processing [30]. Donoho and Johnstone proposed powerful wavelet-based noise-reduction methods [80], and many attempts have been made to use wavelet-based denoising most notably in MRI [81], multimodality PET images [82], and ultrasound [83], although ultimately other filter approaches may prove superior [84]. The phenomenon of noise (especially the multiplicative noise in PET and ultrasound) is still subject of active research. Noise reduction is a key element for further progress, because it would allow reconstruction of a tomographic image with less data, which in turn translates into reduced image acquisition times and--in the case of X-ray- or radionuclide-based imaging--reduced radiation exposure.

With a similar goal, that is, image formation from fewer measurements, compressed sensing may become the next frontier in tomographic imaging. Compressed sensing was introduced by Candes et al. [85] and is based on the observation that sparse signals can be reconstructed far above the Shannon sampling limit. The principle is taken to the extreme in the model of a single-pixel camera, which takes a limited number of exposures (samples) to form an image of the photographed object. New reconstruction algorithms for CT [86] and MRI [87] have already been introduced.

Another recent development is the more and more prominent role of computer graphics hardware in image processing. The popularity of 3D gaming has made available massively parallel processing engines (known as GPU or graphics processing units) 88]. In light of their raw processing power, GPUs are extraordinarily cheap. With GPU support, many vector operations can be accelerated by several orders of magnitude [89]. A typical example is arithmetic reconstruction (ART, cf. Sect. [4.4), where GPU acceleration makes a 1000- to 2000-fold reduction of reconstruction time feasible. Hardware-accelerated versions of the time-consuming cone-beam reconstruction process in CT have also become available [90]. Similarly, hardware-accelerated rigid- or deformable-body registration for multimodality imaging has become available. Clearly, GPU acceleration enables the use of more sophisticated algorithms that would be prohibitively time-consuming on conventional CPUs.

Fig. 7.2

Processing steps in modern computer-aided radiology. Conventionally, the image from the scanner is printed on film or displayed on a digital workstation (curved arrow). In computer-aided radiology, the computer performs several steps, starting with image preprocessing and enhancement. The result of this step is valuable in itself and can be provided to the radiologist (dashed arrow). For the computer to aid in decision-making, the object of interest needs to be isolated (segmentation), and one or more descriptive metrics extracted (feature extraction). These metrics, or a decision based on them (i.e, classification), are then provided to the radiologist

The philosopher's stone of automated image processing is the automated, computerized diagnosis of a disease or anomaly from a given image. This goal is out of reach for any foreseeable future. However, the computer is still capable of aiding the radiologist in various ways, which are often summarized under the term computer-aided diagnosis (see e.g., [91, 92] for reviews). The image processing steps in computer-aided diagnosis are highlighted in Fig. 7.2. In conventional radiology, the image is directly displayed or printed on film. Computerized image enhancement, such as noise reduction or adaptive contrast enhancement, is a step that is often included in the digital workstation and is in itself a help for the radiologist. The computer can further support the decision process by (a) extracting the object of interest (segmentation), (b) deriving quantitative descriptors (feature extraction), and (c) proposing a classification (e.g., healthy versus diseased). The final decision lies with the examining radiologist. However, the computer is more and more able to provide objective data and objective measurements to support such a decision. With the development of new methods and the increasing computing power at our disposal, this trend can be expected to continue. Specifically, we can expect ongoing integration of the steps highlighted in Fig. 7.2 into the scanner's image formation software. As such, the radiologist will be more and more able to rely on specific exam modules, for example, bone density or lung emphysema, that complement the pure image acquisition. As a consequence, the radiologist's decision becomes more objective, can be reached in shorter time, and high-level diagnostic services therefore become more broadly available.
Mark A HaidekkerSpringerBriefs in PhysicsMedical Imaging Technology201310.1007/978-1-4614-7073-1(C) The Author(s) 2013

References

1.

Bushberg JT, Seibert JA, Leidholdt EM, Boone JM. The essential physics of medical imaging. Philadelphia: Lippincott Williams & Wilkins; 2002.

2.

Kalender WA. Computed tomography: fundamentals, system technology, image quality, applications. Erlangen: Publicis; 2011.

3.

Kak AC, Slaney M. Principles of computerized tomographic imaging. New York: IEEE Press; 1998. http://www.slaney.org/pct/pct-toc.html . Accessed Aug 2012.

4.

Herman GT. Fundamentals of computerized tomography: image reconstruction from projections. Berlin: Springer; 2009.

5.

Haacke EM, Brown RW, Thompson MR, Venkatesan R. Magnetic resonance--physical principles and sequence design. New York: Wiley; 1999.

6.

Hedrick WR, Hykes DL, Starchman DE. Ultrasound physics and instrumentation. St. Louis: Elsevier Mosby; 2005.

7.

Hounsfield GN. Computerized transverse axial scanning (tomography): Part 1. Description of system. Br J Radiol. 1973;46(552):1016-22.

8.

Chantler CT, Olsen K, Dragoset RA, Chang J, Kishore AR, Kotochigova SA, et al. Detailed tabulation of atomic form factors, photoelectric absorption and scattering cross section, and mass attenuation coefficients for   -92 from   -10 eV to   -1.0 MeV; 2005. NIST Standard Reference Database 66. http://www.nist.gov/pml/data/ffast/index.cfm . Accessed July 2012.

9.

Ho CP, Kim RW, Schaffler MB, Sartoris DJ. Accuracy of dual-energy radiographic absorptiometry of the lumbar spine: cadaver study. Radiology. 1990;176(1):171.

10.

Engstrom RW. Photomultiplier handbook. Lancaster: RCA Corp.; 1980. Available on-line at http://psec.uchicago.edu/links/Photomultiplier_Handbook.pdf . Accessed April 2013.

11.

Radon J. ber die Bestimmung von Funktionen durch ihre Integralwerte langs gewisser Mannigfaltigkeiten. Ber Sachs Akad Wiss. 1917;69:262-77.

12.

Radon J. On the determination of functions from integral values along certain manifolds (translated by P.C. Parks). IEEE Trans Med Imaging. 1986;5(4):170-6.

13.

Cormack AM. Representation of a function by its line integrals, with some radiological applications. J Appl Phys. 1963;34(9):2722-7.

14.

Cormack AM. Representation of a function by its line integrals, with some radiological applications. II. J Appl Phys. 1964;35(10):2908-13.

15.

Ramachandran GN, Lakshminarayanan AV. Three-dimensional reconstruction from radiographs and electron micrographs: application of convolutions instead of Fourier transforms. Proc Natl Acad Sci U S A. 1971;68(9):2236-40.

16.

Shepp LA, Logan BF. The Fourier reconstruction of a head section. IEEE Trans Nucl Sci. 1974;21(3):21-43.

17.

Rieder A. Principles of reconstruction filter design in 2D-computerized tomography. Contempo Math. 2001;278:207-26.

18.

Feldkamp LA, Davis LC, Kress JW. Practical cone-beam algorithm. J Opt Soc Am A. 1984;1(6):612-9.

19.

Grangeat P. Mathematical framework of cone beam 3D reconstruction via the first derivative of the Radon transform. In: Herman G, Luis AK, Natterer F, editors. Mathematical methods in tomography. Berlin: Springer; 1991. p. 66-97.

20.

Kyriakou Y, Meyer E, Prell D, Kachelriess M. Empirical beam hardening correction (EBHC) for CT. Med Phys. 2010;37:5179-87.

21.

Van Gompel G, Van Slambrouck K, Defrise M, Batenburg KJ, de Mey J, Sijbers J, et al. Iterative correction of beam hardening artifacts in CT. Med Phys. 2011;38 (Suppl.1):S36.

22.

Anger HO. Scintillation camera with multichannel collimators. J Nucl Med. 1964;5(7):515-31.

23.

Anger HO. Scintillation camera. Rev Sci Instrum. 1958;29(1):27-33.

24.

Milster TD, Aarsvold JN, Barrett HH, Landesman AL, Mar LS, Patton DD, et al. A full-field modular gamma camera. J Nucl Med. 1990;31(5):632.

25.

Hunter WCJ. Modeling stochastic processes in gamma-ray imaging detectors and evaluation of a multi-anode PMT scintillation camera for use with maximum-likelihood estimation methods; 2007.

26.

Kaczmarz S. Angenaherte Auflosung von Systemen linearer Gleichungen [Approximate Solution of Linear Equation Systems]. Bull Int Acad Polon Sci Lett. 1937;A35:355-7.

27.

Lange K, Carson R. EM reconstruction algorithms for emission and transmission tomography. J Comput Assist Tomogr. 1984;8(2):306.

28.

Green PJ. Bayesian reconstructions from emission tomography data using a modified EM algorithm. IEEE Trans Med Imaging. 1990;9(1):84-93.

29.

Judenhofer MS, Wehrl HF, Newport DF, Catana C, Siegel SB, Becker M, et al. Simultaneous PET-MRI: a new approach for functional and morphological imaging. Nat Med. 2008;14(4):459-65.

30.

Haidekker MA. Image registration. In: Haidekker MA, Advanced biomedical image analysis. New York: Wiley; 2011.

31.

Dussik KT. ber die Moglichkeit, hochfrequente mechanische Schwingungen als diagnostisches Hilfsmittel zu verwerten [On the possibility of using high-frequency mechanical waves as a diagnostic aid]. Z Neurol Psychiat. 1942;174(1):153-68.

32.

Donald I. Sonar-the story of an experiment. Ultrasound Med Biol. 1974;1(2):109-17.

33.

Donald I. Apologia: how and why medical sonar developed. Ann R Coll Surg Engl. 1974;54(3):132-40.

34.

Ikeda TO. Fundamentals of piezoelectricity. Oxford: Oxford University Press; 1990.

35.

Hunt JW, Arditi M, Foster FS. Ultrasound transducers for pulse-echo medical imaging. IEEE Trans Biomed Eng. 1983;8:453-81.

36.

Duckelmann AM, Kalache KD. Three-dimensional ultrasound in evaluating the fetus. Prenat Diagn. 2010;30(7):631-8.

37.

Rizzo G, Pietrolucci M, Aiello E, Mammarella S, Bosi C, Arduini D. The role of three-dimensional ultrasound in the diagnosis of fetal congenital anomalies: a review. Minerva Ginecol. 2011;63(5):401.

38.

Li G, Citrin D, Camphausen K, Mueller B, Burman C, Mychalczak B, et al. Advances in 4D medical imaging and 4D radiation therapy. Technol Cancer Res Treat. 2008;7(1):67.

39.

Sheikh KH, Smith SW, Ramm Ov, Kisslo J. Real-time, three-dimensional echocardiography: feasibility and initial use. Echocardiography. 1991;8(1):119-25.

40.

Hung J, Lang R, Flachskampf F, Shernan SK, McCulloch ML, Adams DB, et al. 3D echocardiography: a review of the current status and future directions. J Am Soc Echocardiogr. 2007;20(3):213-33.

41.

Szabo TL. Diagnostic ultrasound imaging: inside out. Burlington: Academic Press; 2004.

42.

Kalender WA, Vock P, Polacin A, Soucek M. [Spiral-CT: a new technique for volumetric scans. I. Basic principles and methodology]. Rontgenpraxis; Zeitschrift fur Radiologische Technik. 1990;43(9):323.

43.

Laskaris ET, Ackermann R, Dorri B, Gross D, Herd K, Minas C. A cryogen-free open superconducting magnet for interventional MRI applications. IEEE Trans Appl Supercond. 1995;5(2):163-8.

44.

Ron E. Cancer risks from medical radiation. Health Phys. 2003;85(1):47.

45.

Kalender WA, Quick HH. Recent advances in medical physics. Eur Radiol. 2011;21:501-4.

46.

Davis TJ, Gao D, Gureyev TE, Stevenson AW, Wilkins SW. Phase-contrast imaging of weakly absorbing materials using hard X-rays. Nature. 1995;373(6515):595-8.

47.

Pfeiffer F, Weitkamp T, Bunk O, David C. Phase retrieval and differential phase-contrast imaging with low-brilliance X-ray sources. Nat Phys. 2006;2(4):258-61.

48.

Pfeiffer F, Bech M, Bunk O, Kraft P, Eikenberry EF, Bronnimann C, et al. Hard-X-ray dark-field imaging using a grating interferometer. Nat. Mater. 2008;7(2):134-7.

49.

Donath T, Pfeiffer F, Bunk O, Grunzweig C, Hempel E, Popescu S, et al. Toward clinical X-ray phase-contrast CT: demonstration of enhanced soft-tissue contrast in human specimen. Invest Radiol. 2010;45(7):445.

50.

Bech M, Jensen TH, Bunk O, Donath T, David C, Weitkamp T, et al. Advanced contrast modalities for X-ray radiology: phase-contrast and dark-field imaging using a grating interferometer. Zeitschrift fur Medizinische Physik. 2010;20(1):7-16.

51.

Webb A. Increasing the sensitivity of magnetic resonance spectroscopy and imaging. Anal Chem. 2012;84(1):9-16.

52.

Ogawa S, Lee TM, Kay AR, Tank DW. Brain magnetic resonance imaging with contrast dependent on blood oxygenation. Proc Natl Acad Sci U S A. 1990;87(24):9868.

53.

Cercignani M, Horsfield MA. The physical basis of diffusion-weighted MRI. J Neurol Sci. 2001;186:S11-4.

54.

Le Bihan D, Mangin JF, Poupon C, Clark CA, Pappata S, Molko N, et al. Diffusion tensor imaging: concepts and applications. J Magn Reson Imaging. 2001;13(4):534-46.

55.

Wehrli FW, Saha PK, Gomberg BR, Song HK, Snyder PJ, Benito M, et al. Role of magnetic resonance for assessing structure and function of trabecular bone. Top Magn Reson Imaging. 2002;13(5):335.

56.

Magland JF, Wald MJ, Wehrli FW. Spin-echo micro-MRI of trabecular bone using improved 3D fast large-angle spin-echo (FLASE). Magn Reson Med. 2009;61(5):1114-21.

57.

Haidekker MA, Dougherty G. In: Dougherty G, editor. Medical imaging in the diagnosis of osteoporosis and estimation of the individual bone fracture risk. Berlin: Springer; 2011. p. 193-225.

58.

Prinz C, Voigt JU. Diagnostic accuracy of a hand-held ultrasound scanner in routine patients referred for echocardiography. J Am Soc Echocardiogr. 2011;24(2):111-6.

59.

Kiessling F, Fokong S, Koczera P, Lederle W, Lammers T. Ultrasound microbubbles for molecular diagnosis, therapy, and theranostics. J Nucl Med. 2012;53(3):345-8.

60.

Ferrara KW, Borden MA, Zhang H. Lipid-shelled vehicles: engineering for ultrasound molecular imaging and drug delivery. Acc Chem Res. 2009;42(7):881-92.

61.

Pichler BJ, Wehrl HF, Judenhofer MS. Latest advances in molecular imaging instrumentation. J Nucl Med. 2008;49(Suppl 2):5S-23S.

62.

Beyer T, Townsend DW, Brun T, Kinahan PE, Charron M, Roddy R, et al. A combined PET/CT scanner for clinical oncology. J Nucl Med. 2000;41(8):1369-79.

63.

Sauter AW, Wehrl HF, Kolb A, Judenhofer MS, Pichler BJ. Combined PET/MRI: one step further in multimodality imaging. Trends Mol Med. 2010;16(11):508-15.

64.

Majmudar MD, Nahrendorf M. Cardiovascular molecular imaging: the road ahead. J Nucl Med. 2012;53(5):673-6.

65.

Cromer Berman SM, Walczak P, Bulte JWM. Tracking stem cells using magnetic nanoparticles. Wiley Interdiscip Rev: Nanomed Nanobiotechnol. 2011;3:343-55.

66.

Wang LV, Hu S. Photoacoustic tomography: in vivo imaging from organelles to organs. Science. 2012;335(6075):1458-62.

67.

Geitzenauer W, Hitzenberger CK, Schmidt-Erfurth UM. Retinal optical coherence tomography: past, present and future perspectives. Br J Ophthalmol. 2011;95(2):171-7.

68.

Maeda N. Optical coherence tomography for corneal diseases. Eye Contact Lens. 2010;36(5):254.

69.

Mogensen M, Thrane L, Jrgensen TM, Andersen PE, Jemec GBE. OCT imaging of skin cancer and other dermatological diseases. J Biophoton. 2009;2:442-51.

70.

Osiac E, Saftoiu A, Gheonea DI, Mandrila I, Angelescu R. Optical coherence tomography and Doppler optical coherence tomography in the gastrointestinal tract. World J Gastroenterol. 2011;17(1):15-20.

71.

Ripoll J, Nieto-Vesperinas M, Weissleder R, Ntziachristos V. Fast analytical approximation for arbitrary geometries in diffuse optical tomography. Opt Lett. 2002;27(7):527-9.

72.

Ntziachristos V, Bremer C, Graves EE, Ripoll J, Weissleder R. In vivo tomographic imaging of near-infrared fluorescent probes. Mol Imaging. 2002;1(2):82-8.

73.

Van De Ven S, Elias S, Wiethoff A, Van Der Voort M, Leproux A, Nielsen T, et al. Diffuse optical tomography of the breast: initial validation in benign cysts. Mol Imaging Biol. 2009;11(2):64-70.

74.

Choe R, Konecky SD, Corlu A, Lee K, Durduran T, Busch DR, et al. Differentiation of benign and malignant breast tumors by in-vivo three-dimensional parallel-plate diffuse optical tomography. J Biomed Opt. 2009;14:024020.

75.

Gladish JC, Yao G, Heureux NL, Haidekker MA. Optical transillumination tomography for imaging of tissue-engineered blood vessels. Ann Biomed Eng. 2005;33(3):323-7.

76.

Haidekker MA. Optical transillumination tomography with tolerance against refraction mismatch. Comput Methods Programs Biomed. 2005;80(3):225-35.

77.

Huang HM, Xia J, Haidekker MA. Fast optical transillumination tomography with large-size projection acquisition. Ann Biomed Eng. 2008;36(10):1699-707.

78.

Sharpe J, Ahlgren U, Perry P, Hill B, Ross A, Hecksher-Srensen J, et al. Optical projection tomography as a tool for 3D microscopy and gene expression studies. Science. 2002;296(5567):541-5.

79.

Swoger J, Sharpe J, Haidekker MA. Optical projection and transillumination tomography for multidimensional mesoscopic imaging. In: Morgan S, Rose FR, Matcher S, editors. Optical techniques in regenerative medicine. New York: Taylor & Francis; 2013.

80.

Donoho DL, Johnstone JM. Ideal spatial adaptation by wavelet shrinkage. Biometrika. 1994;81(3):425-55.

81.

Zaroubi S, Goelman G. Complex denoising of MR data via wavelet analysis: application for functional MRI. Magn Reson Imaging. 2000;18(1):59-68.

82.

Turkheimer FE, Boussion N, Anderson AN, Pavese N, Piccini P, Visvikis D. PET image denoising using a synergistic multiresolution analysis of structural (MRI/CT) and functional datasets. J Nucl Med. 2008;49(4):657-66.

83.

Bhuiyan MIH, Omair Ahmad M, Swamy MNS, New spatially adaptive wavelet-based method for the despeckling of medical ultrasound images. In: Circuits and Systems, 2007. ISCAS 2007. IEEE international symposium on. IEEE; 2007. p. 2347-50.

84.

Finn S, Glavin M, Jones E. Echocardiographic speckle reduction comparison. IEEE Trans Ultrason Ferroelectr Freq Control. 2011;58(1):82-101.

85.

Candes EJ, Romberg J, Tao T. Robust uncertainty principles: exact signal reconstruction from highly incomplete frequency information. IEEE Trans Inf Theory. 2006;52(2):489-509.

86.

Li X, Luo S. A compressed sensing-based iterative algorithm for CT reconstruction and its possible application to phase contrast imaging. Biomed Eng Online. 2011;10(1):1-14.

87.

Mistretta CA. Undersampled radial MR acquisition and highly constrained back projection (HYPR) reconstruction: potential medical imaging applications in the post-Nyquist era. J Magn Reson Imaging. 2009;29(3):501-16.

88.

Etter DM, Hermann RJ. The rise of games and high performance computing for modeling and simulation. Washington, DC: The National Academies Press; 2010.

89.

Payne JL, Sinnott-Armstrong NA, Moore JH. Exploiting graphics processing units for computational biology and bioinformatics. Interdiscip Sci: Comput Life Sci. 2010;2(3):213-20.

90.

Zhao X, Hu J, Zhang P. GPU-based 3D cone-beam CT image reconstruction for large data volume. J Biomed Imaging. 2009;2009:8.

91.

Erickson BJ, Bartholmai B. Computer-aided detection and diagnosis at the start of the third millennium. J Dig Imaging. 2002;15(2):59-68.

92.

Kim TY, Son J, Kim KG. The recent progress in quantitative medical image analysis for computer aided diagnosis systems. Healthcare Inf Research. 2011;17(3):143.

Index

A

Acoustic impedance

Algebraic reconstruction

Aliasing

Alzheimer's disease

Amniotic fluid

A-mode scan

Anger camera

Annihilation event

Anode

rotating

Aorta

Arithmetic reconstruction

Atomic shell

Avalanche photodiode

B

Bandwidth

Beam hardening

BGO (bismuth germanium oxide)

Bit depth

Bloch equation

Blur

B-mode scan

Bone

density

Boxcar function

Brain

Braking radiation

Bremsstrahlung

C

Calcaneus

Calcium tungstate

Camera

Cathode

Cerebrospinal fluid

Characteristic X-rays

Chest

CMOS image sensor

Coincidence detection

Collimators

Compressed sensing

Compton scattering

Computed tomography

Computer-aided diagnosis

Cone-beam geometry

Contrast

Conversion layer

Convolution

Cornea

Coronary blood flow

CT microscope

Cyclotron

D

Darkfield X-ray imaging

Dephasing

DEXA

Diamagnetism

Diffuse optical tomography

Diffusion tensor imaging

Digital image

Digital image workstation

Digitization noise

Doppler ultrasound

Dual-energy X-ray absorptiometry

Dual-modality scanners

Dual-source CT

Duplex ultrasound

Dynode

E

Echo planar imaging

Echo time

Electromagnetic interference

Emulsion

Energy spectrum

Excitation

F

Fan-beam geometry

Far-field

Fat

Femoral neck

Fetus

Filament

Film

latitude

sensitivity

shoulder region

toe region

Film cassette

Filtered backprojection

Fluoroscope

Focal spot

Fourier slice theorem

Fourier transform

Free induction decay

Frequency encode gradient

Frequency response

Full width at half-maximum

Functional MRI

G

Gadolinium

Gamma radiation

Gantry

Gated acquisition

Gradient coils

Gradient field

Gradient-recalled echo

Graphics processing units

Gray matter

Gyromagnetic ratio

H

Haze

Heart

Hooke's law

Hounsfield units

I

Image-guided surgery

Image intensifier

Image processing

Image registration

Image sensor

Impedance-matching layer

Interpolation

Interventional radiology

Inversion recovery sequence

Ionization

Isotopes

Iterative reconstruction

K

Kernel

Kidney

Kinetic energy

k-space matrix

L

Lambert-Beer's law

Larmor equation

Larmor frequency

Laser

Linear system

Line sum

Liver

Longitudinal magnetization

Lumbar spine

Lung

M

Magnetic gradient

Magnetic induction

Magnetic moment

Magnetic permeability

Magnetic resonance imaging

Maximum a posteriori

Maximum likelihood expectation maximization

Michelson interferometer

M-mode scan

Molecular imaging

Multi-modality imaging

Muscle

N

Near-field

Neutron activation

Noise

Nyquist frequency

O

Open MRI

Ophthalmology

Optical coherence tomography

Optical density

Optical projection tomography

Optical transillumination tomography

P

Pair production

Paramagnetism

Partial-volume effect

Pencil-beam geometry

Phase coherence

Phase-contrast X-ray imaging

Phased array

Phase encode gradient

Phosphor screen

Photoacoustinc imaging

Photocathode

Photoelectric effect

Photomultiplier tube

Piezoelectricity

Point-spread function

Poisson noise

Positron emission tomography

Precession

Pressure

Projection

Proton density

Pseudo-structure

Pulse-echo acquisition

Pulse-repetition frequency

Pulsed Doppler ultrasound

Pulse sequence

PZT

Q

Quantitative CT

R

Radiation damage

Radioactive decay

Radiofrequency

Radionuclide imaging

Radiopharmaceuticals

Radon transform

Rayleigh scattering

Receive focus

Reconstruction kernel

Reflection

Refraction

Registration

Repetition time

Rephasing

Resonance

Retina

RF coils

Richardson equation

Ring artifacts

S

Scintigraphy

Self-magnetism

Semiconductor detector

Sensitivity

Shannon sampling theorem

Signal

Signal-to-noise ratio

sinc function

Single photon emission computed tomography

Sinogram

Skin

Slice encode gradient

Slice thickness

Snell's law

Soft tissue

Sonar

Spatial frequency

Spatial resolution

Speed of sound

Spin

Spin echo

Spin-echo sequence

Spin excess

Spiral CT

Spoiler gradient

Stenosis

Structural image

Superconductor

Superluminescent diode

System

T

Thermionic emission

Thorax

Thrombosis

Thyroid

Time gain control

Transducer

Transmission

Transmit focus

Transversal magnetization

Tumors

U

Ultrasound gel

Ultrasound imaging

W

Wavelength

Wavelet transform

Wave propagation

White matter

X

X-ray absorption

X-ray attenuation coefficients

X-ray tube

# Source Metadata

- Domain: physics
- Context ID: 8c17dccdcecf806604208a748c4f906f
- Document ID: 2709dfdc049d8247f61b33511f9130e3
- Approx. Length: 63430 characters